{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/whoami-Lory271/thesis-project/blob/main/LSTM_Linear_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Symbol legend\n",
        "\n",
        "* B: batch size\n",
        "* L: lookback window (aka input_size)\n"
      ],
      "metadata": {
        "id": "7s9odzFFQWyj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installations and imports\n"
      ],
      "metadata": {
        "id": "w7opc0NsjlNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-lightning==2.0.1.post0 --quiet\n",
        "!pip install einops==0.6.1 --quiet\n",
        "!pip install ipdb --quiet\n",
        "!pip install wandb --quiet\n",
        "# !pip install objsize --quiet"
      ],
      "metadata": {
        "id": "ehQC2AKyci-4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e515387-0307-45e8-9753-b7a40a03f2a9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m718.6/718.6 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m763.4/763.4 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m842.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.8/188.8 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.8/218.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import logging\n",
        "# https://github.com/gotcha/ipdb\n",
        "import ipdb\n",
        "import copy\n",
        "from google.colab import drive\n",
        "from tqdm.notebook import tqdm\n",
        "import torch\n",
        "from torch import optim\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import TensorDataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import defaultdict\n",
        "import functools\n",
        "# https://theaisummer.com/einsum-attention/\n",
        "import einops\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import Ridge\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning.callbacks.progress import TQDMProgressBar\n",
        "from pytorch_lightning.callbacks import EarlyStopping\n",
        "from pytorch_lightning.profilers import PyTorchProfiler\n",
        "from pytorch_lightning.tuner import Tuner\n",
        "\n",
        "import wandb\n",
        "import sys\n",
        "import pickle\n",
        "import time\n",
        "import os\n",
        "import torch.fft as fft\n",
        "import argparse"
      ],
      "metadata": {
        "id": "WuaX4Ts_jqmd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "!cp /content/drive/MyDrive/Tesi/code/.netrc /root/\n",
        "wandb.login()\n",
        "# 024a3906e525e6d2640af94c364128bb3d33e44b"
      ],
      "metadata": {
        "id": "4F86VEC4VtL8",
        "outputId": "2d53202b-4b6a-4b3b-d683-e1ca8b084c35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdesantis-1849114\u001b[0m (\u001b[33mdesantis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/dataset\n",
        "!cp \"/content/drive/MyDrive/Tesi/code/datasets/NLinear/ETTh1.csv\" \"/content/dataset/\"\n",
        "\n",
        "!cp \"/content/drive/MyDrive/Tesi/code/datasets/NLinear/ETTh2.csv\" \"/content/dataset/\"\n",
        "\n",
        "!cp \"/content/drive/MyDrive/Tesi/code/datasets/NLinear/ETTm1.csv\" \"/content/dataset/\"\n",
        "\n",
        "!cp \"/content/drive/MyDrive/Tesi/code/datasets/NLinear/ETTm2.csv\" \"/content/dataset/\"\n",
        "\n",
        "!cp \"/content/drive/MyDrive/Tesi/code/datasets/NLinear/electricity.csv\" \"/content/dataset/\""
      ],
      "metadata": {
        "id": "YKC7BhtXbkD9"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Functions"
      ],
      "metadata": {
        "id": "J2UVU6VqgizJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# setup logger function\n",
        "def setup_log(self, level):\n",
        "    log = logging.getLogger(self.__class__.__name__)\n",
        "    log.setLevel(level)\n",
        "    return log"
      ],
      "metadata": {
        "id": "gkBTe3nko46m"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write log on a file\n",
        "def write_log(a):\n",
        "    with open(\"log.txt\", 'w') as file:\n",
        "        for row in a:\n",
        "            file.write(str(row))\n",
        "        log.debug(\"object logged\")"
      ],
      "metadata": {
        "id": "i8LE_3TogiZn"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# random seed functions\n",
        "def seed_everything(seed):\n",
        "    pl.seed_everything(seed, workers=True)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "mzBQAeoIi8l2"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pickle helper\n",
        "def pkl_save(name, var):\n",
        "    os.makedirs(os.path.dirname(name), exist_ok=True)\n",
        "    with open(name, 'wb') as f:\n",
        "        pickle.dump(var, f)\n",
        "\n",
        "def pkl_load(name):\n",
        "    with open(name, 'rb') as f:\n",
        "        return pickle.load(f)"
      ],
      "metadata": {
        "id": "NxBH76S1gdKm"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Constants"
      ],
      "metadata": {
        "id": "94xSbfaKkOmq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# logger\n",
        "LOG_LEVEL = logging.DEBUG\n",
        "\n",
        "# datasets name\n",
        "ELECTRICITY = \"electricity\"\n",
        "M5 = \"M5\"\n",
        "ETTh1 = \"ETTh1\"\n",
        "ETTh2 = \"ETTh2\"\n",
        "ETTm1 = \"ETTm1\"\n",
        "ETTm2 = \"ETTm2\"\n",
        "WEATHER = \"WTH\"\n",
        "\n",
        "# models\n",
        "SLinear = \"SLinear\"\n",
        "MODEL = SLinear\n",
        "DATASET = ETTh1\n",
        "MODEL_ID = MODEL+'_'+DATASET\n",
        "\n",
        "# device\n",
        "\n",
        "DEVICE = torch.device('cpu') if not torch.cuda.is_available() else torch.device('cuda')\n",
        "\n",
        "#hyperparameters\n",
        "\n",
        "LR = 0.005\n",
        "\n",
        "# Train\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10\n",
        "UNIVARIATE = True\n",
        "SEQ_LEN = 336\n",
        "\n",
        "# Eval\n",
        "EVALUATE = True\n",
        "\n",
        "\n",
        "# training\n",
        "TRAIN = True\n",
        "DETERMINISTIC = True\n",
        "LOAD_MODEL = False\n",
        "RESUME_TRAINING = False\n",
        "MEMORY_PROFILING = False\n",
        "LOAD_ENCODE = False\n",
        "\n",
        "# wandb\n",
        "\n",
        "SETTINGS_STRING = \"univariate\" if UNIVARIATE else \"multivariate\"\n",
        "RUN_ID = \"0z980736\"\n",
        "RESUME_RUN = False\n",
        "RUN_ID = wandb.util.generate_id() if not RESUME_RUN else RUN_ID\n",
        "print(f\"current RUN_ID is {RUN_ID}\")\n",
        "\n",
        "\n",
        "#paths\n",
        "MODEL_SETTINGS_FOLDER = \"/univariate\" if UNIVARIATE else \"/multivariate\"\n",
        "ROOT_FOLDER = \"/content/drive/MyDrive/Tesi/code\"\n",
        "MODEL_FOLDER = ROOT_FOLDER + \"/models\"\n",
        "CHECKPOINT_FOLDER = ROOT_FOLDER + \"/checkpoints\" + \"/\" + MODEL + \"/\" + DATASET + MODEL_SETTINGS_FOLDER\n",
        "# LOGS_FOLDER = ROOT_FOLDER + \"/logs\"\n",
        "ENCODING_FOLDER = ROOT_FOLDER + \"/encoding/\" + MODEL + \"/\" + DATASET + MODEL_SETTINGS_FOLDER\n",
        "FORECASTING_RESULT = ROOT_FOLDER + \"/forecasting_result/\" + MODEL + \"/\" + DATASET + MODEL_SETTINGS_FOLDER\n",
        "\n",
        "datasets_name = {\n",
        "    ELECTRICITY: \"electricity.csv\",\n",
        "    ETTh1: \"ETTh1.csv\",\n",
        "    ETTh2: \"ETTh2.csv\",\n",
        "    ETTm1: \"ETTm1.csv\",\n",
        "    ETTm2: \"ETTm2.csv\"\n",
        "}\n",
        "\n",
        "datasets_pred_lens = {\n",
        "    ELECTRICITY: [24, 48, 168, 336, 720], # [24, 48, 168, 336, 720]\n",
        "    ETTh1: [24, 48, 168, 336, 720],\n",
        "    ETTh2: [24, 48, 168, 336, 720],\n",
        "    WEATHER: [24, 48, 168, 336, 720],\n",
        "    M5: [28],\n",
        "    ETTm1: [24, 48, 96, 288, 672],\n",
        "    ETTm2: [24, 48, 96, 288, 672]\n",
        "}\n",
        "\n",
        "enc_in = {\n",
        "    ELECTRICITY: 321,\n",
        "    ETTh1: 7,\n",
        "    ETTh2: 7,\n",
        "    ETTm1: 7,\n",
        "    ETTm2: 7\n",
        "}\n",
        "\n",
        "config = dict(\n",
        "    model_id = MODEL_ID,\n",
        "    iters = ITERS,\n",
        "    epochs = 0 if EPOCHS is None else EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    learning_rate = LR,\n",
        "    dataset=DATASET,\n",
        "    architecture=MODEL,\n",
        "    run_id = RUN_ID)"
      ],
      "metadata": {
        "id": "ySxfaOHQkQ_B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aac20522-ae47-4b78-f279-f16f403f896b"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "current RUN_ID is 03x1jszx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WanDB"
      ],
      "metadata": {
        "id": "WXYPOAZ8O2zL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# start a new experiment\n",
        "run = wandb.init(project=config['model_id'], config = config, id = config[\"run_id\"], resume = 'allow')\n",
        "ENCODING_FOLDER += \"/\" + run.name\n",
        "FORECASTING_RESULT += \"/\" + run.name\n",
        "CHECKPOINT_FOLDER += \"/\" + run.name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "sC_id8hdO5i_",
        "outputId": "2899a6af-8e5c-4a85-90cf-6eafdb69d2ea"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230909_144932-03x1jszx</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/desantis/SLinear_ETTh1/runs/03x1jszx' target=\"_blank\">smart-sun-1</a></strong> to <a href='https://wandb.ai/desantis/SLinear_ETTh1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/desantis/SLinear_ETTh1' target=\"_blank\">https://wandb.ai/desantis/SLinear_ETTh1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/desantis/SLinear_ETTh1/runs/03x1jszx' target=\"_blank\">https://wandb.ai/desantis/SLinear_ETTh1/runs/03x1jszx</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Argument Parser"
      ],
      "metadata": {
        "id": "A7-lHvJRzOty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser(description='Autoformer & Transformer family for Time Series Forecasting')\n",
        "\n",
        "# basic config\n",
        "parser.add_argument('--is_training', type=int, required=True, default=1, help='status')\n",
        "parser.add_argument('--train_only', type=bool, required=False, default=False, help='perform training on full input dataset without validation and testing')\n",
        "parser.add_argument('--model_id', type=str, required=True, default='test', help='model id')\n",
        "parser.add_argument('--model', type=str, required=True, default='Autoformer',\n",
        "                    help='model name, options: [Autoformer, Informer, Transformer]')\n",
        "\n",
        "# data loader\n",
        "parser.add_argument('--data', type=str, required=True, default='ETTm1', help='dataset type')\n",
        "parser.add_argument('--root_path', type=str, default='./data/ETT/', help='root path of the data file')\n",
        "parser.add_argument('--data_path', type=str, default='ETTh1.csv', help='data file')\n",
        "parser.add_argument('--features', type=str, default='M',\n",
        "                    help='forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate')\n",
        "parser.add_argument('--target', type=str, default='OT', help='target feature in S or MS task')\n",
        "parser.add_argument('--freq', type=str, default='h',\n",
        "                    help='freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h')\n",
        "parser.add_argument('--checkpoints', type=str, default='./checkpoints/', help='location of model checkpoints')\n",
        "\n",
        "# forecasting task\n",
        "parser.add_argument('--seq_len', type=int, default=96, help='input sequence length')\n",
        "parser.add_argument('--label_len', type=int, default=48, help='start token length')\n",
        "parser.add_argument('--pred_len', type=int, default=96, help='prediction sequence length')\n",
        "parser.add_argument('--seed', type=int, default=0, help='random seed')\n",
        "\n",
        "\n",
        "# DLinear\n",
        "parser.add_argument('--individual', action='store_true', default=False, help='DLinear: a linear layer for each variate(channel) individually')\n",
        "# Formers\n",
        "parser.add_argument('--embed_type', type=int, default=0, help='0: default 1: value embedding + temporal embedding + positional embedding 2: value embedding + temporal embedding 3: value embedding + positional embedding 4: value embedding')\n",
        "parser.add_argument('--enc_in', type=int, default=7, help='encoder input size') # DLinear with --individual, use this hyperparameter as the number of channels\n",
        "parser.add_argument('--dec_in', type=int, default=7, help='decoder input size')\n",
        "parser.add_argument('--c_out', type=int, default=7, help='output size')\n",
        "parser.add_argument('--d_model', type=int, default=512, help='dimension of model')\n",
        "parser.add_argument('--n_heads', type=int, default=8, help='num of heads')\n",
        "parser.add_argument('--e_layers', type=int, default=2, help='num of encoder layers')\n",
        "parser.add_argument('--d_layers', type=int, default=1, help='num of decoder layers')\n",
        "parser.add_argument('--d_ff', type=int, default=2048, help='dimension of fcn')\n",
        "parser.add_argument('--moving_avg', type=int, default=25, help='window size of moving average')\n",
        "parser.add_argument('--factor', type=int, default=1, help='attn factor')\n",
        "parser.add_argument('--distil', action='store_false',\n",
        "                    help='whether to use distilling in encoder, using this argument means not using distilling',\n",
        "                    default=True)\n",
        "parser.add_argument('--dropout', type=float, default=0.05, help='dropout')\n",
        "parser.add_argument('--embed', type=str, default='timeF',\n",
        "                    help='time features encoding, options:[timeF, fixed, learned]')\n",
        "parser.add_argument('--activation', type=str, default='gelu', help='activation')\n",
        "parser.add_argument('--output_attention', action='store_true', help='whether to output attention in ecoder')\n",
        "parser.add_argument('--do_predict', action='store_true', help='whether to predict unseen future data')\n",
        "\n",
        "# optimization\n",
        "parser.add_argument('--num_workers', type=int, default=10, help='data loader num workers')\n",
        "parser.add_argument('--train_epochs', type=int, default=10, help='train epochs')\n",
        "parser.add_argument('--batch_size', type=int, default=32, help='batch size of train input data')\n",
        "parser.add_argument('--patience', type=int, default=3, help='early stopping patience')\n",
        "parser.add_argument('--learning_rate', type=float, default=0.0001, help='optimizer learning rate')\n",
        "parser.add_argument('--des', type=str, default='test', help='exp description')\n",
        "parser.add_argument('--loss', type=str, default='mse', help='loss function')\n",
        "parser.add_argument('--lradj', type=str, default='type1', help='adjust learning rate')\n",
        "parser.add_argument('--use_amp', action='store_true', help='use automatic mixed precision training', default=False)\n",
        "\n",
        "# GPU\n",
        "parser.add_argument('--use_gpu', type=bool, default=True, help='use gpu')\n",
        "parser.add_argument('--gpu', type=int, default=0, help='gpu')\n",
        "parser.add_argument('--use_multi_gpu', action='store_true', help='use multiple gpus', default=False)\n",
        "parser.add_argument('--devices', type=str, default='0,1,2,3', help='device ids of multile gpus')\n",
        "parser.add_argument('--test_flop', action='store_true', default=False, help='See utils/tools for usage')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7o0eClOzR6d",
        "outputId": "e66ff13e-1b1c-4b21-8c31-8e4a1b44c549"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreTrueAction(option_strings=['--test_flop'], dest='test_flop', nargs=0, const=True, default=False, type=None, choices=None, required=False, help='See utils/tools for usage', metavar=None)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logger"
      ],
      "metadata": {
        "id": "8j5-8dn5ppGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create logger\n",
        "log = logging.getLogger('APP')\n",
        "log.setLevel(LOG_LEVEL)\n",
        "logging.basicConfig(level=LOG_LEVEL)\n",
        "\n",
        "# # create console handler and set level to debug\n",
        "# ch = logging.StreamHandler()\n",
        "# ch.setLevel(logging.INFO)\n",
        "\n",
        "# # create formatter\n",
        "# formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# # add formatter to ch\n",
        "# ch.setFormatter(formatter)\n",
        "\n",
        "# # add ch to logger\n",
        "# logger.addHandler(ch)"
      ],
      "metadata": {
        "id": "iRLWiTu4mlx9"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset and Datamodule"
      ],
      "metadata": {
        "id": "ma655OWbiZ0h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Datasets"
      ],
      "metadata": {
        "id": "0dOIrBBc1ewD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset_ETT_hour(Dataset):\n",
        "    def __init__(self, root_path, flag='train', size=None,\n",
        "                 features='S', data_path='ETTh1.csv',\n",
        "                 target='OT', scale=True, timeenc=0, freq='h', train_only=False):\n",
        "        # size [seq_len, label_len, pred_len]\n",
        "        # info\n",
        "        if size == None:\n",
        "            self.seq_len = 24 * 4 * 4\n",
        "            self.label_len = 24 * 4\n",
        "            self.pred_len = 24 * 4\n",
        "        else:\n",
        "            self.seq_len = size[0]\n",
        "            self.label_len = size[1]\n",
        "            self.pred_len = size[2]\n",
        "        # init\n",
        "        assert flag in ['train', 'test', 'val']\n",
        "        type_map = {'train': 0, 'val': 1, 'test': 2}\n",
        "        self.set_type = type_map[flag]\n",
        "\n",
        "        self.features = features\n",
        "        self.target = target\n",
        "        self.scale = scale\n",
        "        self.timeenc = timeenc\n",
        "        self.freq = freq\n",
        "\n",
        "        self.root_path = root_path\n",
        "        self.data_path = data_path\n",
        "        self.__read_data__()\n",
        "\n",
        "    def __read_data__(self):\n",
        "        self.scaler = StandardScaler()\n",
        "        df_raw = pd.read_csv(os.path.join(self.root_path,\n",
        "                                          self.data_path))\n",
        "\n",
        "        border1s = [0, 12 * 30 * 24 - self.seq_len, 12 * 30 * 24 + 4 * 30 * 24 - self.seq_len]\n",
        "        border2s = [12 * 30 * 24, 12 * 30 * 24 + 4 * 30 * 24, 12 * 30 * 24 + 8 * 30 * 24]\n",
        "        border1 = border1s[self.set_type]\n",
        "        border2 = border2s[self.set_type]\n",
        "\n",
        "        if self.features == 'M' or self.features == 'MS':\n",
        "            cols_data = df_raw.columns[1:]\n",
        "            df_data = df_raw[cols_data]\n",
        "        elif self.features == 'S':\n",
        "            df_data = df_raw[[self.target]]\n",
        "\n",
        "        if self.scale:\n",
        "            train_data = df_data[border1s[0]:border2s[0]]\n",
        "            self.scaler.fit(train_data.values)\n",
        "            data = self.scaler.transform(df_data.values)\n",
        "        else:\n",
        "            data = df_data.values\n",
        "\n",
        "        df_stamp = df_raw[['date']][border1:border2]\n",
        "        df_stamp['date'] = pd.to_datetime(df_stamp.date)\n",
        "        if self.timeenc == 0:\n",
        "            df_stamp['month'] = df_stamp.date.apply(lambda row: row.month, 1)\n",
        "            df_stamp['day'] = df_stamp.date.apply(lambda row: row.day, 1)\n",
        "            df_stamp['weekday'] = df_stamp.date.apply(lambda row: row.weekday(), 1)\n",
        "            df_stamp['hour'] = df_stamp.date.apply(lambda row: row.hour, 1)\n",
        "            data_stamp = df_stamp.drop(['date'], 1).values\n",
        "        elif self.timeenc == 1:\n",
        "            data_stamp = time_features(pd.to_datetime(df_stamp['date'].values), freq=self.freq)\n",
        "            data_stamp = data_stamp.transpose(1, 0)\n",
        "\n",
        "        self.data_x = data[border1:border2]\n",
        "        self.data_y = data[border1:border2]\n",
        "        self.data_stamp = data_stamp\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        s_begin = index\n",
        "        s_end = s_begin + self.seq_len\n",
        "        r_begin = s_end - self.label_len\n",
        "        r_end = r_begin + self.label_len + self.pred_len\n",
        "\n",
        "        seq_x = self.data_x[s_begin:s_end]\n",
        "        seq_y = self.data_y[r_begin:r_end]\n",
        "        seq_x_mark = self.data_stamp[s_begin:s_end]\n",
        "        seq_y_mark = self.data_stamp[r_begin:r_end]\n",
        "\n",
        "        return seq_x, seq_y, seq_x_mark, seq_y_mark\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_x) - self.seq_len - self.pred_len + 1\n",
        "\n",
        "    def inverse_transform(self, data):\n",
        "        return self.scaler.inverse_transform(data)\n",
        "\n",
        "\n",
        "class Dataset_ETT_minute(Dataset):\n",
        "    def __init__(self, root_path, flag='train', size=None,\n",
        "                 features='S', data_path='ETTm1.csv',\n",
        "                 target='OT', scale=True, timeenc=0, freq='t', train_only=False):\n",
        "        # size [seq_len, label_len, pred_len]\n",
        "        # info\n",
        "        if size == None:\n",
        "            self.seq_len = 24 * 4 * 4\n",
        "            self.label_len = 24 * 4\n",
        "            self.pred_len = 24 * 4\n",
        "        else:\n",
        "            self.seq_len = size[0]\n",
        "            self.label_len = size[1]\n",
        "            self.pred_len = size[2]\n",
        "        # init\n",
        "        assert flag in ['train', 'test', 'val']\n",
        "        type_map = {'train': 0, 'val': 1, 'test': 2}\n",
        "        self.set_type = type_map[flag]\n",
        "\n",
        "        self.features = features\n",
        "        self.target = target\n",
        "        self.scale = scale\n",
        "        self.timeenc = timeenc\n",
        "        self.freq = freq\n",
        "\n",
        "        self.root_path = root_path\n",
        "        self.data_path = data_path\n",
        "        self.__read_data__()\n",
        "\n",
        "    def __read_data__(self):\n",
        "        self.scaler = StandardScaler()\n",
        "        df_raw = pd.read_csv(os.path.join(self.root_path,\n",
        "                                          self.data_path))\n",
        "\n",
        "        border1s = [0, 12 * 30 * 24 * 4 - self.seq_len, 12 * 30 * 24 * 4 + 4 * 30 * 24 * 4 - self.seq_len]\n",
        "        border2s = [12 * 30 * 24 * 4, 12 * 30 * 24 * 4 + 4 * 30 * 24 * 4, 12 * 30 * 24 * 4 + 8 * 30 * 24 * 4]\n",
        "        border1 = border1s[self.set_type]\n",
        "        border2 = border2s[self.set_type]\n",
        "\n",
        "        if self.features == 'M' or self.features == 'MS':\n",
        "            cols_data = df_raw.columns[1:]\n",
        "            df_data = df_raw[cols_data]\n",
        "        elif self.features == 'S':\n",
        "            df_data = df_raw[[self.target]]\n",
        "\n",
        "        if self.scale:\n",
        "            train_data = df_data[border1s[0]:border2s[0]]\n",
        "            self.scaler.fit(train_data.values)\n",
        "            data = self.scaler.transform(df_data.values)\n",
        "        else:\n",
        "            data = df_data.values\n",
        "\n",
        "        df_stamp = df_raw[['date']][border1:border2]\n",
        "        df_stamp['date'] = pd.to_datetime(df_stamp.date)\n",
        "        if self.timeenc == 0:\n",
        "            df_stamp['month'] = df_stamp.date.apply(lambda row: row.month, 1)\n",
        "            df_stamp['day'] = df_stamp.date.apply(lambda row: row.day, 1)\n",
        "            df_stamp['weekday'] = df_stamp.date.apply(lambda row: row.weekday(), 1)\n",
        "            df_stamp['hour'] = df_stamp.date.apply(lambda row: row.hour, 1)\n",
        "            df_stamp['minute'] = df_stamp.date.apply(lambda row: row.minute, 1)\n",
        "            df_stamp['minute'] = df_stamp.minute.map(lambda x: x // 15)\n",
        "            data_stamp = df_stamp.drop(['date'], 1).values\n",
        "        elif self.timeenc == 1:\n",
        "            data_stamp = time_features(pd.to_datetime(df_stamp['date'].values), freq=self.freq)\n",
        "            data_stamp = data_stamp.transpose(1, 0)\n",
        "\n",
        "        self.data_x = data[border1:border2]\n",
        "        self.data_y = data[border1:border2]\n",
        "        self.data_stamp = data_stamp\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        s_begin = index\n",
        "        s_end = s_begin + self.seq_len\n",
        "        r_begin = s_end - self.label_len\n",
        "        r_end = r_begin + self.label_len + self.pred_len\n",
        "\n",
        "        seq_x = self.data_x[s_begin:s_end]\n",
        "        seq_y = self.data_y[r_begin:r_end]\n",
        "        seq_x_mark = self.data_stamp[s_begin:s_end]\n",
        "        seq_y_mark = self.data_stamp[r_begin:r_end]\n",
        "\n",
        "        return seq_x, seq_y, seq_x_mark, seq_y_mark\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_x) - self.seq_len - self.pred_len + 1\n",
        "\n",
        "    def inverse_transform(self, data):\n",
        "        return self.scaler.inverse_transform(data)\n",
        "\n",
        "\n",
        "class Dataset_Custom(Dataset):\n",
        "    def __init__(self, root_path, flag='train', size=None,\n",
        "                 features='S', data_path='ETTh1.csv',\n",
        "                 target='OT', scale=True, timeenc=0, freq='h', train_only=False):\n",
        "        # size [seq_len, label_len, pred_len]\n",
        "        # info\n",
        "        if size == None:\n",
        "            self.seq_len = 24 * 4 * 4\n",
        "            self.label_len = 24 * 4\n",
        "            self.pred_len = 24 * 4\n",
        "        else:\n",
        "            self.seq_len = size[0]\n",
        "            self.label_len = size[1]\n",
        "            self.pred_len = size[2]\n",
        "        # init\n",
        "        assert flag in ['train', 'test', 'val']\n",
        "        type_map = {'train': 0, 'val': 1, 'test': 2}\n",
        "        self.set_type = type_map[flag]\n",
        "\n",
        "        self.features = features\n",
        "        self.target = target\n",
        "        self.scale = scale\n",
        "        self.timeenc = timeenc\n",
        "        self.freq = freq\n",
        "        self.train_only = train_only\n",
        "\n",
        "        self.root_path = root_path\n",
        "        self.data_path = data_path\n",
        "        self.__read_data__()\n",
        "\n",
        "    def __read_data__(self):\n",
        "        self.scaler = StandardScaler()\n",
        "        df_raw = pd.read_csv(os.path.join(self.root_path,\n",
        "                                          self.data_path))\n",
        "\n",
        "        '''\n",
        "        df_raw.columns: ['date', ...(other features), target feature]\n",
        "        '''\n",
        "        cols = list(df_raw.columns)\n",
        "        if self.features == 'S':\n",
        "            cols.remove(self.target)\n",
        "        cols.remove('date')\n",
        "        # print(cols)\n",
        "        num_train = int(len(df_raw) * (0.7 if not self.train_only else 1))\n",
        "        num_test = int(len(df_raw) * 0.2)\n",
        "        num_vali = len(df_raw) - num_train - num_test\n",
        "        border1s = [0, num_train - self.seq_len, len(df_raw) - num_test - self.seq_len]\n",
        "        border2s = [num_train, num_train + num_vali, len(df_raw)]\n",
        "        border1 = border1s[self.set_type]\n",
        "        border2 = border2s[self.set_type]\n",
        "\n",
        "        if self.features == 'M' or self.features == 'MS':\n",
        "            df_raw = df_raw[['date'] + cols]\n",
        "            cols_data = df_raw.columns[1:]\n",
        "            df_data = df_raw[cols_data]\n",
        "        elif self.features == 'S':\n",
        "            df_raw = df_raw[['date'] + cols + [self.target]]\n",
        "            df_data = df_raw[[self.target]]\n",
        "\n",
        "        if self.scale:\n",
        "            train_data = df_data[border1s[0]:border2s[0]]\n",
        "            self.scaler.fit(train_data.values)\n",
        "            # print(self.scaler.mean_)\n",
        "            # exit()\n",
        "            data = self.scaler.transform(df_data.values)\n",
        "        else:\n",
        "            data = df_data.values\n",
        "\n",
        "        df_stamp = df_raw[['date']][border1:border2]\n",
        "        df_stamp['date'] = pd.to_datetime(df_stamp.date)\n",
        "        if self.timeenc == 0:\n",
        "            df_stamp['month'] = df_stamp.date.apply(lambda row: row.month, 1)\n",
        "            df_stamp['day'] = df_stamp.date.apply(lambda row: row.day, 1)\n",
        "            df_stamp['weekday'] = df_stamp.date.apply(lambda row: row.weekday(), 1)\n",
        "            df_stamp['hour'] = df_stamp.date.apply(lambda row: row.hour, 1)\n",
        "            data_stamp = df_stamp.drop(['date'], 1).values\n",
        "        elif self.timeenc == 1:\n",
        "            data_stamp = time_features(pd.to_datetime(df_stamp['date'].values), freq=self.freq)\n",
        "            data_stamp = data_stamp.transpose(1, 0)\n",
        "\n",
        "        self.data_x = data[border1:border2]\n",
        "        self.data_y = data[border1:border2]\n",
        "        self.data_stamp = data_stamp\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        s_begin = index\n",
        "        s_end = s_begin + self.seq_len\n",
        "        r_begin = s_end - self.label_len\n",
        "        r_end = r_begin + self.label_len + self.pred_len\n",
        "\n",
        "        seq_x = self.data_x[s_begin:s_end]\n",
        "        seq_y = self.data_y[r_begin:r_end]\n",
        "        seq_x_mark = self.data_stamp[s_begin:s_end]\n",
        "        seq_y_mark = self.data_stamp[r_begin:r_end]\n",
        "\n",
        "        return seq_x, seq_y, seq_x_mark, seq_y_mark\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_x) - self.seq_len - self.pred_len + 1\n",
        "\n",
        "    def inverse_transform(self, data):\n",
        "        return self.scaler.inverse_transform(data)\n",
        "\n",
        "\n",
        "class Dataset_Pred(Dataset):\n",
        "    def __init__(self, root_path, flag='pred', size=None,\n",
        "                 features='S', data_path='ETTh1.csv',\n",
        "                 target='OT', scale=True, inverse=False, timeenc=0, freq='15min', cols=None, train_only=False):\n",
        "        # size [seq_len, label_len, pred_len]\n",
        "        # info\n",
        "        if size == None:\n",
        "            self.seq_len = 24 * 4 * 4\n",
        "            self.label_len = 24 * 4\n",
        "            self.pred_len = 24 * 4\n",
        "        else:\n",
        "            self.seq_len = size[0]\n",
        "            self.label_len = size[1]\n",
        "            self.pred_len = size[2]\n",
        "        # init\n",
        "        assert flag in ['pred']\n",
        "\n",
        "        self.features = features\n",
        "        self.target = target\n",
        "        self.scale = scale\n",
        "        self.inverse = inverse\n",
        "        self.timeenc = timeenc\n",
        "        self.freq = freq\n",
        "        self.cols = cols\n",
        "        self.root_path = root_path\n",
        "        self.data_path = data_path\n",
        "        self.__read_data__()\n",
        "\n",
        "    def __read_data__(self):\n",
        "        self.scaler = StandardScaler()\n",
        "        df_raw = pd.read_csv(os.path.join(self.root_path,\n",
        "                                          self.data_path))\n",
        "        '''\n",
        "        df_raw.columns: ['date', ...(other features), target feature]\n",
        "        '''\n",
        "        if self.cols:\n",
        "            cols = self.cols.copy()\n",
        "        else:\n",
        "            cols = list(df_raw.columns)\n",
        "            self.cols = cols.copy()\n",
        "            cols.remove('date')\n",
        "        if self.features == 'S':\n",
        "            cols.remove(self.target)\n",
        "        border1 = len(df_raw) - self.seq_len\n",
        "        border2 = len(df_raw)\n",
        "\n",
        "        if self.features == 'M' or self.features == 'MS':\n",
        "            df_raw = df_raw[['date'] + cols]\n",
        "            cols_data = df_raw.columns[1:]\n",
        "            df_data = df_raw[cols_data]\n",
        "        elif self.features == 'S':\n",
        "            df_raw = df_raw[['date'] + cols + [self.target]]\n",
        "            df_data = df_raw[[self.target]]\n",
        "\n",
        "        if self.scale:\n",
        "            self.scaler.fit(df_data.values)\n",
        "            data = self.scaler.transform(df_data.values)\n",
        "        else:\n",
        "            data = df_data.values\n",
        "\n",
        "        tmp_stamp = df_raw[['date']][border1:border2]\n",
        "        tmp_stamp['date'] = pd.to_datetime(tmp_stamp.date)\n",
        "        pred_dates = pd.date_range(tmp_stamp.date.values[-1], periods=self.pred_len + 1, freq=self.freq)\n",
        "\n",
        "        df_stamp = pd.DataFrame(columns=['date'])\n",
        "        df_stamp.date = list(tmp_stamp.date.values) + list(pred_dates[1:])\n",
        "        self.future_dates = list(pred_dates[1:])\n",
        "        if self.timeenc == 0:\n",
        "            df_stamp['month'] = df_stamp.date.apply(lambda row: row.month, 1)\n",
        "            df_stamp['day'] = df_stamp.date.apply(lambda row: row.day, 1)\n",
        "            df_stamp['weekday'] = df_stamp.date.apply(lambda row: row.weekday(), 1)\n",
        "            df_stamp['hour'] = df_stamp.date.apply(lambda row: row.hour, 1)\n",
        "            df_stamp['minute'] = df_stamp.date.apply(lambda row: row.minute, 1)\n",
        "            df_stamp['minute'] = df_stamp.minute.map(lambda x: x // 15)\n",
        "            data_stamp = df_stamp.drop(['date'], 1).values\n",
        "        elif self.timeenc == 1:\n",
        "            data_stamp = time_features(pd.to_datetime(df_stamp['date'].values), freq=self.freq)\n",
        "            data_stamp = data_stamp.transpose(1, 0)\n",
        "\n",
        "        self.data_x = data[border1:border2]\n",
        "        if self.inverse:\n",
        "            self.data_y = df_data.values[border1:border2]\n",
        "        else:\n",
        "            self.data_y = data[border1:border2]\n",
        "        self.data_stamp = data_stamp\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        s_begin = index\n",
        "        s_end = s_begin + self.seq_len\n",
        "        r_begin = s_end - self.label_len\n",
        "        r_end = r_begin + self.label_len + self.pred_len\n",
        "\n",
        "        seq_x = self.data_x[s_begin:s_end]\n",
        "        if self.inverse:\n",
        "            seq_y = self.data_x[r_begin:r_begin + self.label_len]\n",
        "        else:\n",
        "            seq_y = self.data_y[r_begin:r_begin + self.label_len]\n",
        "        seq_x_mark = self.data_stamp[s_begin:s_end]\n",
        "        seq_y_mark = self.data_stamp[r_begin:r_end]\n",
        "\n",
        "        return seq_x, seq_y, seq_x_mark, seq_y_mark\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_x) - self.seq_len + 1\n",
        "\n",
        "    def inverse_transform(self, data):\n",
        "        return self.scaler.inverse_transform(data)"
      ],
      "metadata": {
        "id": "qhF8zP_w1buC"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Datamodule"
      ],
      "metadata": {
        "id": "5NTY0pc31jOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, args):\n",
        "        super().__init__()\n",
        "        self.args = args\n",
        "        self.data_dict = {\n",
        "            'ETTh1': Dataset_ETT_hour,\n",
        "            'ETTh2': Dataset_ETT_hour,\n",
        "            'ETTm1': Dataset_ETT_minute,\n",
        "            'ETTm2': Dataset_ETT_minute,\n",
        "            'custom': Dataset_Custom,\n",
        "        }\n",
        "        self.Data = self.data_dict[args.data]\n",
        "        self.timeenc = 0 if args.embed != 'timeF' else 1\n",
        "        self.train_only = args.train_only\n",
        "\n",
        "    def setup(self, stage: str):\n",
        "        shuffle_flag = True\n",
        "        drop_last = True\n",
        "        batch_size = args.batch_size\n",
        "        freq = args.freq\n",
        "        Data = self.Data\n",
        "        args = self.args\n",
        "        timeenc = self.timeenc\n",
        "        train_only = self.train_only\n",
        "\n",
        "        if stage == \"fit\":\n",
        "            self.batch_size = batch_size\n",
        "            self.shuffle_flag = shuffle_flag\n",
        "            self.drop_last = drop_last\n",
        "            self.train = Data(\n",
        "                root_path=args.root_path,\n",
        "                data_path=args.data_path,\n",
        "                flag=\"train\",\n",
        "                size=[args.seq_len, args.label_len, args.pred_len],\n",
        "                features=args.features,\n",
        "                target=args.target,\n",
        "                timeenc=timeenc,\n",
        "                freq=freq,\n",
        "                train_only=train_only\n",
        "            )\n",
        "\n",
        "        if stage == 'validate':\n",
        "            self.batch_size = batch_size\n",
        "            self.shuffle_flag = shuffle_flag\n",
        "            self.drop_last = drop_last\n",
        "\n",
        "            self.val = Data(\n",
        "                root_path=args.root_path,\n",
        "                data_path=args.data_path,\n",
        "                flag=\"val\",\n",
        "                size=[args.seq_len, args.label_len, args.pred_len],\n",
        "                features=args.features,\n",
        "                target=args.target,\n",
        "                timeenc=timeenc,\n",
        "                freq=freq,\n",
        "                train_only=train_only\n",
        "            )\n",
        "\n",
        "        if stage == 'test':\n",
        "            shuffle_flag = False\n",
        "            drop_last = False\n",
        "            batch_size = args.batch_size\n",
        "            freq = args.freq\n",
        "\n",
        "            self.batch_size = batch_size\n",
        "            self.shuffle_flag = shuffle_flag\n",
        "            self.drop_last = drop_last\n",
        "\n",
        "            self.test = Data(\n",
        "                root_path=args.root_path,\n",
        "                data_path=args.data_path,\n",
        "                flag=\"test\",\n",
        "                size=[args.seq_len, args.label_len, args.pred_len],\n",
        "                features=args.features,\n",
        "                target=args.target,\n",
        "                timeenc=timeenc,\n",
        "                freq=freq,\n",
        "                train_only=train_only\n",
        "            )\n",
        "\n",
        "\n",
        "        if stage == 'predict':\n",
        "            shuffle_flag = False\n",
        "            drop_last = False\n",
        "            batch_size = 1\n",
        "            freq = args.freq\n",
        "            Data = Dataset_Pred\n",
        "\n",
        "            self.batch_size = batch_size\n",
        "            self.shuffle_flag = shuffle_flag\n",
        "            self.drop_last = drop_last\n",
        "\n",
        "            self.predict = Data(\n",
        "                root_path=args.root_path,\n",
        "                data_path=args.data_path,\n",
        "                flag=\"pred\",\n",
        "                size=[args.seq_len, args.label_len, args.pred_len],\n",
        "                features=args.features,\n",
        "                target=args.target,\n",
        "                timeenc=timeenc,\n",
        "                freq=freq,\n",
        "                train_only=train_only\n",
        "            )\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train,\n",
        "                        batch_size=self.batch_size,\n",
        "                        shuffle=self.shuffle_flag,\n",
        "                        num_workers=self.args.num_workers,\n",
        "                        drop_last=self.drop_last)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val,\n",
        "                        batch_size=self.batch_size,\n",
        "                        shuffle=self.shuffle_flag,\n",
        "                        num_workers=self.args.num_workers,\n",
        "                        drop_last=self.drop_last)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.test,\n",
        "                        batch_size=self.batch_size,\n",
        "                        shuffle=self.shuffle_flag,\n",
        "                        num_workers=self.args.num_workers,\n",
        "                        drop_last=self.drop_last)\n",
        "\n",
        "    def predict_dataloader(self):\n",
        "        return DataLoader(self.predict,\n",
        "                        batch_size=self.batch_size,\n",
        "                        shuffle=self.shuffle_flag,\n",
        "                        num_workers=self.args.num_workers,\n",
        "                        drop_last=self.drop_last)"
      ],
      "metadata": {
        "id": "37YF2vGhSYjl"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "S84zQ9UjigKj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "Oq9gmXnKGmVP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OFFCIAL IMPLEMENTATIONS 🔽"
      ],
      "metadata": {
        "id": "wriuLMemIEtl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NLinear"
      ],
      "metadata": {
        "id": "9VKcjh66BBAJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class NLinear(nn.Module):\n",
        "    \"\"\"\n",
        "    Normalization-Linear\n",
        "    \"\"\"\n",
        "    def __init__(self, covariate_size, d_model, layers, individual = True):\n",
        "        super(NLinear, self).__init__()\n",
        "        self.seq_len = covariate_size\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Use this line if you want to visualize the weights\n",
        "        # self.Linear.weight = nn.Parameter((1/self.seq_len)*torch.ones([self.pred_len,self.seq_len]))\n",
        "        self.channels = layers\n",
        "        self.individual = individual\n",
        "        if self.individual:\n",
        "            self.Linear = nn.ModuleList()\n",
        "            for i in range(self.channels):\n",
        "                self.Linear.append(nn.Linear(self.seq_len,self.d_model))\n",
        "        else:\n",
        "            self.Linear = nn.Linear(self.seq_len, self.d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [Batch, Input length, Channel]\n",
        "        seq_last = x[:,-1:,:].detach()\n",
        "        x = x - seq_last\n",
        "        if self.individual:\n",
        "            output = torch.zeros([x.size(0),self.d_model,x.size(2)],dtype=x.dtype).to(x.device)\n",
        "            for i in range(self.channels):\n",
        "                output[:,:,i] = self.Linear[i](x[:,:,i])\n",
        "            x = output\n",
        "        else:\n",
        "            x = self.Linear(x.permute(0,2,1)).permute(0,2,1)\n",
        "        x = x + seq_last\n",
        "        return x # [Batch, Output length, Channel]"
      ],
      "metadata": {
        "id": "wb7zI3HuBDo0"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand((16, 8, 201))\n",
        "model = NLinear(8, 320, 4)\n",
        "x = model(x)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gxE8HLUF_xn",
        "outputId": "43eb88a6-4079-43cd-9354-2ce8108c3461"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 320, 201])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## My Model 🔽"
      ],
      "metadata": {
        "id": "YP8y2ZUZP0ew"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Functions"
      ],
      "metadata": {
        "id": "WD_TKtHHOdZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def adjust_learning_rate(optimizer, epoch, args):\n",
        "    # lr = args.learning_rate * (0.2 ** (epoch // 2))\n",
        "    if args.lradj == 'type1':\n",
        "        lr_adjust = {epoch: args.learning_rate * (0.5 ** ((epoch - 1) // 1))}\n",
        "    elif args.lradj == 'type2':\n",
        "        lr_adjust = {\n",
        "            2: 5e-5, 4: 1e-5, 6: 5e-6, 8: 1e-6,\n",
        "            10: 5e-7, 15: 1e-7, 20: 5e-8\n",
        "        }\n",
        "    elif args.lradj == '3':\n",
        "        lr_adjust = {epoch: args.learning_rate if epoch < 10 else args.learning_rate*0.1}\n",
        "    elif args.lradj == '4':\n",
        "        lr_adjust = {epoch: args.learning_rate if epoch < 15 else args.learning_rate*0.1}\n",
        "    elif args.lradj == '5':\n",
        "        lr_adjust = {epoch: args.learning_rate if epoch < 25 else args.learning_rate*0.1}\n",
        "    elif args.lradj == '6':\n",
        "        lr_adjust = {epoch: args.learning_rate if epoch < 5 else args.learning_rate*0.1}\n",
        "    if epoch in lr_adjust.keys():\n",
        "        lr = lr_adjust[epoch]\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "        print('Updating learning rate to {}'.format(lr))"
      ],
      "metadata": {
        "id": "c5yyhXUrOc30"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SLinear"
      ],
      "metadata": {
        "id": "RX3ADmnN_M_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SLinear(pl.LightningModule):\n",
        "    def __init__(self, configs):\n",
        "        super(SLinear, self).__init__()\n",
        "        self.configs = configs\n",
        "        self.seq_len = configs.seq_len\n",
        "        self.pred_len = configs.pred_len\n",
        "\n",
        "        # Use this line if you want to visualize the weights\n",
        "        # self.NLinear.weight = nn.Parameter((1/self.seq_len)*torch.ones([self.pred_len,self.seq_len]))\n",
        "        self.channels = configs.enc_in\n",
        "        self.individual = configs.individual\n",
        "        if self.individual:\n",
        "            self.Linear = nn.ModuleList()\n",
        "            for i in range(self.channels):\n",
        "                self.Linear.append(nn.Linear(self.seq_len,self.pred_len))\n",
        "        else:\n",
        "            self.Linear = nn.Linear(self.seq_len, self.pred_len)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        model_optim = optim.Adam(self.model.parameters(), lr=self.configs.learning_rate)\n",
        "        return model_optim\n",
        "\n",
        "    def _get_loss(self):\n",
        "        criterion = nn.MSELoss()\n",
        "        return criterion\n",
        "\n",
        "    def on_train_epoch_start(self):\n",
        "        self.train_loss = []\n",
        "\n",
        "    def on_validation_epoch_start(self):\n",
        "        self.val_loss = []\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "        train_loss = np.average(self.train_loss)\n",
        "        self.log(\"train_loss\", train_loss)\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        val_loss = np.average(self.val_loss)\n",
        "        self.log(\"val_loss\", val_loss)\n",
        "        model_optim = self.optimizers(use_pl_optimizer=False)\n",
        "        adjust_learning_rate(model_optim, self.current_epoch + 1, self.configs)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y, z = batch\n",
        "        batch_x = batch_x.float()\n",
        "        batch_y = batch_y.float()\n",
        "        batch_x_mark = batch_x_mark.float()\n",
        "        batch_y_mark = batch_y_mark.float()\n",
        "        outputs = self(x)\n",
        "        f_dim = -1 if self.configs.features == 'MS' else 0\n",
        "        outputs = outputs[:, -self.pred_len:, f_dim:]\n",
        "        batch_y = batch_y[:, -self.pred_len:, f_dim:]\n",
        "        criterion = self._get_loss()\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        self.train_loss.append(loss.item())\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y, z = batch\n",
        "        batch_x = batch_x.float()\n",
        "        batch_y = batch_y.float()\n",
        "        batch_x_mark = batch_x_mark.float()\n",
        "        batch_y_mark = batch_y_mark.float()\n",
        "        outputs = self(x)\n",
        "        f_dim = -1 if self.configs.features == 'MS' else 0\n",
        "        outputs = outputs[:, -self.pred_len:, f_dim:]\n",
        "        batch_y = batch_y[:, -self.pred_len:, f_dim:]\n",
        "        criterion = self._get_loss()\n",
        "        pred = outputs.detach().cpu()\n",
        "        true = batch_y.detach().cpu()\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        self.val_loss.append(loss)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [Batch, Input length, Channel]\n",
        "        # Normalization\n",
        "        mean_x = x.mean(1, keepdim=True).detach() # B x 1 x E\n",
        "        x = x - mean_x\n",
        "        std_x = torch.sqrt(torch.var(x, dim=1, keepdim=True, unbiased=False) + 1e-5).detach() # B x 1 x E\n",
        "        x = x / std_x\n",
        "        if self.individual:\n",
        "            output = torch.zeros([x.size(0),self.pred_len,x.size(2)],dtype=x.dtype).to(x.device)\n",
        "            for i in range(self.channels):\n",
        "                output[:,:,i] = self.Linear[i](x[:,:,i])\n",
        "            x = output\n",
        "        else:\n",
        "            x = self.Linear(x.permute(0,2,1)).permute(0,2,1)\n",
        "        # De-normalization\n",
        "        x = x * std_x + mean_x\n",
        "        return x # [Batch, Output length, Channel]"
      ],
      "metadata": {
        "id": "0zXBfuqt_PDq"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "DYNOokNm3LCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train function\n",
        "def train(batch_size, datamodule, model, model_name, max_epochs = None, max_steps = -1,\n",
        "          check_val_every_n_epoch = 1, resume_training = True, load_model = False,\n",
        "          enable_checkpoint = True, monitor_metric = \"val_loss\", checkpoint_dir = None,\n",
        "          early_stopping = True, deterministic = False, configs = None):\n",
        "\n",
        "    # check monitor metric\n",
        "    assert monitor_metric in [\"train_loss\", \"val_loss\"], \"metric to monitor is invalid\"\n",
        "\n",
        "    # initialize callbacks array\n",
        "    callbacks = [TQDMProgressBar(refresh_rate=20)]\n",
        "\n",
        "    # add checkpoints to callbacks\n",
        "    checkpoint_callback = None\n",
        "    if enable_checkpoint and checkpoint_dir is not None:\n",
        "        checkpoint_callback = ModelCheckpoint(dirpath=checkpoint_dir,  monitor = monitor_metric, filename=model_name + '{epoch:02d}-{' + monitor_metric + ':.2f}',\n",
        "                                         save_last =True, save_on_train_epoch_end = True)\n",
        "        callbacks.append(checkpoint_callback)\n",
        "\n",
        "    # add early stopping to the callbacks\n",
        "    if early_stopping:\n",
        "        callbacks.append(EarlyStopping(monitor=\"val_loss\", patience = configs.patience, mode=\"min\", check_on_train_epoch_end = False))\n",
        "\n",
        "    # create the Trainer\n",
        "    trainer = pl.Trainer(enable_checkpointing=enable_checkpoint, devices=1, accelerator=\"auto\",\n",
        "                         max_epochs=max_epochs, max_steps=max_steps, callbacks=callbacks,\n",
        "                         check_val_every_n_epoch = check_val_every_n_epoch,\n",
        "                         deterministic = deterministic)\n",
        "\n",
        "    ckpt_path = None\n",
        "    if resume_training:\n",
        "        ckpt_path = checkpoint_dir + \"/last.ckpt\"\n",
        "    trainer.fit(ckpt_path = ckpt_path, model=model, datamodule=datamodule)\n",
        "    if checkpoint_callback is not None:\n",
        "        log.info(checkpoint_callback.best_model_path)\n",
        "\n",
        "    return trainer"
      ],
      "metadata": {
        "id": "q-936er_-snn"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "Oqxoxjh9C0rC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seeds = [1,2,3]\n",
        "results = []\n",
        "\n",
        "for seed in seeds:\n",
        "    for pred_len in datasets_pred_lens[DATASET]\n",
        "        args = parser.parse_args([\"--is_training=1\",\n",
        "                                \"--root_path=./dataset\", f\"--data_path={datasets_name[DATASET]}\",\n",
        "                                f\"--model_id={MODEL_ID}\", f\"--model={MODEL}\",\n",
        "                                f\"--data={DATASET}\", f\"--seq_len={SEQ_LEN}\",\n",
        "                                f\"--seed={seed}\", f\"--pred_len={pred_len}\",\n",
        "                                  f\"--enc_in={1 if UNIVARIATE else enc_in[DATASET]}\",\n",
        "                                  \"--individual\", f\"batch_size={BATCH_SIZE}\",\n",
        "                                  \"--feature=S\", f\"--learning_rate={LR}\"])\n",
        "        # initialize dataset\n",
        "        # datamodule = ElectricityDataModule(datasets_path[ELECTRICITY] + datasets_processed_name[ELECTRICITY], BATCH_SIZE, L)\n",
        "        datamodule = CustomDataModule(args)\n",
        "\n",
        "        # set seed if deterministic\n",
        "        if DETERMINISTIC:\n",
        "            seed_everything(seed)\n",
        "        # initialize model, or load an extisting one\n",
        "        if LOAD_MODEL:\n",
        "            log.info(\"loading model...\")\n",
        "            model = eval(MODEL).load_from_checkpoint(CHECKPOINT_FOLDER + \"/last.ckpt\")\n",
        "        else:\n",
        "            model = eval(MODEL)(args)\n",
        "\n",
        "        setting = '{}_{}_{}_{}_{}'.format(\n",
        "                args.model_id,\n",
        "                args.model,\n",
        "                args.data,\n",
        "                args.pred_len,\n",
        "                args.seed)\n",
        "\n",
        "        log.info(setting)\n",
        "\n",
        "        if TRAIN:\n",
        "            trainer = train(args.batch_size, datamodule, model, MODEL,\n",
        "                            max_epochs = args.train_epochs, check_val_every_n_epoch = 1,\n",
        "                            load_model = LOAD_MODEL,\n",
        "                            resume_training = RESUME_TRAINING,  monitor_metric = \"val_loss\",\n",
        "                            checkpoint_dir = CHECKPOINT_FOLDER,\n",
        "                            early_stopping = True, deterministic = DETERMINISTIC, configs = args)\n",
        "        # if EVALUATE:\n",
        "        #     encoding_loader, data_shape = prepare_encoding(datamodule)\n",
        "        #     if LOAD_ENCODE:\n",
        "        #         log.info(\"load encoding...\")\n",
        "        #         repr = pkl_load(ENCODING_FOLDER + \"/last.pkl\")\n",
        "        #     else:\n",
        "        #         repr = encode(model, data_shape, encoding_loader, ENCODE_BATCH_SIZE, DEVICE, ENCODING_FOLDER)\n",
        "        # if EVALUATE:\n",
        "        #     train_slice = datamodule.train_slice\n",
        "        #     valid_slice = datamodule.valid_slice\n",
        "        #     test_slice = datamodule.test_slice\n",
        "        #     data = datamodule.data\n",
        "        #     n_covariate_cols = datamodule.n_covariate_cols\n",
        "        #     scaler = datamodule.scaler\n",
        "        #     padding = PADDING\n",
        "        #     pred_lens = datasets_pred_lens[DATASET]\n",
        "\n",
        "        #     out, eval_res, y_labels_mse, y_labels_mae = eval_forecasting(repr, data, train_slice, valid_slice, test_slice,\n",
        "        #                                     n_covariate_cols, scaler, padding, pred_lens)\n",
        "\n",
        "        #     results.append([y_labels_mse, y_labels_mae])\n",
        "\n",
        "        #     # data = [[x, y] for (x, y) in zip(pred_lens, y_labels_mse)]\n",
        "        #     # table = wandb.Table(data=data, columns = [\"pred_lens\", \"mse\"])\n",
        "        #     # wandb.log(\n",
        "        #     # {f\"{SETTINGS_STRING} forecasting MSE plot\" : wandb.plot.scatter(table, \"pred_lens\", \"mse\",\n",
        "        #     #     title=f\"{SETTINGS_STRING} forecasting MSE norm plot\")})\n",
        "\n",
        "        #     # data = [[x, y] for (x, y) in zip(pred_lens, y_labels_mae)]\n",
        "        #     # table = wandb.Table(data=data, columns = [\"pred_lens\", \"mae\"])\n",
        "        #     # wandb.log(\n",
        "        #     # {f\"{SETTINGS_STRING} forecasting MAE plot\" : wandb.plot.scatter(table, \"pred_lens\", \"mae\",\n",
        "        #     #     title=f\"{SETTINGS_STRING} forecasting MAE norm plot\")})\n",
        "        #     # wandb.log({\"eval_forecasting\": eval_res})\n",
        "        #     pkl_save(FORECASTING_RESULT + \"/out.pkl\", out)\n",
        "        #     pkl_save(FORECASTING_RESULT + \"/eval_res.pkl\", eval_res)"
      ],
      "metadata": {
        "id": "AvftO5iOK9ZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mse = []\n",
        "# mae = []\n",
        "# for i, pred_len in enumerate(pred_lens):\n",
        "#     mse.append(0)\n",
        "#     mae.append(0)\n",
        "#     for res in results:\n",
        "#         mse[i] += res[0][i] / len(results)\n",
        "#         mae[i] += res[1][i] / len(results)\n",
        "\n",
        "# log.info(mse)\n",
        "# log.info(mae)\n",
        "\n",
        "# data = [[x, y] for (x, y) in zip(pred_lens, mse)]\n",
        "# table = wandb.Table(data=data, columns = [\"pred_lens\", \"mse\"])\n",
        "# wandb.log(\n",
        "# {f\"{SETTINGS_STRING} forecasting MSE plot\" : wandb.plot.scatter(table, \"pred_lens\", \"mse\",\n",
        "#     title=f\"{SETTINGS_STRING} forecasting MSE norm plot\")})\n",
        "\n",
        "# data = [[x, y] for (x, y) in zip(pred_lens, mae)]\n",
        "# table = wandb.Table(data=data, columns = [\"pred_lens\", \"mae\"])\n",
        "# wandb.log(\n",
        "# {f\"{SETTINGS_STRING} forecasting MAE plot\" : wandb.plot.scatter(table, \"pred_lens\", \"mae\",\n",
        "#     title=f\"{SETTINGS_STRING} forecasting MAE norm plot\")})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXZsEiDVmyrB",
        "outputId": "55dd6277-e237-434a-d163-f4157ea6294d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:APP:[0.02767434649844249, 0.04918789892038547, 0.06709897723587861, 0.11782806062256995, 0.1639527146436185]\n",
            "INFO:APP:[0.1182246483926824, 0.1616641315562232, 0.19234729375800472, 0.26075920336493075, 0.309325898105969]\n"
          ]
        }
      ]
    }
  ]
}