{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/whoami-Lory271/thesis-project/blob/refactor-model/thesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Symbol legend\n",
        "\n",
        "* B: batch size \n",
        "* M: number of channel\n",
        "* P: patch dimension\n",
        "* N: number of patches\n",
        "* L: lookback window\n"
      ],
      "metadata": {
        "id": "7s9odzFFQWyj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installations and imports\n"
      ],
      "metadata": {
        "id": "w7opc0NsjlNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-lightning==2.0.1.post0 --quiet\n",
        "!pip install einops==0.6.1 --quiet\n",
        "!pip install ipdb --quiet"
      ],
      "metadata": {
        "id": "ehQC2AKyci-4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57ed46f7-1636-4f2e-8f8f-c1a301efd76a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m718.6/718.6 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import logging\n",
        "# https://github.com/gotcha/ipdb\n",
        "import ipdb\n",
        "import copy\n",
        "from google.colab import drive\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import random\n",
        "# https://theaisummer.com/einsum-attention/\n",
        "import einops\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import Ridge\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning.callbacks.progress import TQDMProgressBar\n",
        "from pytorch_lightning.callbacks import EarlyStopping"
      ],
      "metadata": {
        "id": "WuaX4Ts_jqmd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4F86VEC4VtL8",
        "outputId": "70df67e1-38f3-4938-93a8-4b90dfa8814c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Functions"
      ],
      "metadata": {
        "id": "J2UVU6VqgizJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# setup logger function\n",
        "def setup_log(self, level):\n",
        "    log = logging.getLogger(self.__class__.__name__)\n",
        "    log.setLevel(level)\n",
        "    return log"
      ],
      "metadata": {
        "id": "gkBTe3nko46m"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write log on a file\n",
        "def write_log(a):\n",
        "    with open(\"log.txt\", 'w') as file:\n",
        "        for row in a:\n",
        "            file.write(str(row))\n",
        "        log.debug(\"object logged\")"
      ],
      "metadata": {
        "id": "i8LE_3TogiZn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_patch_num(patch_len, num_var, stride):\n",
        "    return (max(patch_len, num_var)-patch_len) // stride + 2"
      ],
      "metadata": {
        "id": "Ke58BUkLo-dd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pad array with nan\n",
        "def pad_nan(arr, left=0, right=0, dim=0):\n",
        "    # padding the right side \n",
        "    if left > 0:\n",
        "        padshape = list(arr.shape)\n",
        "        padshape[dim] = left\n",
        "        arr = torch.cat((torch.full(padshape, np.nan).to(arr.device), arr), dim=dim)\n",
        "\n",
        "    # padding the left side\n",
        "    if right > 0:\n",
        "        padshape = list(arr.shape)\n",
        "        padshape[dim] = right\n",
        "        arr = torch.cat((arr, torch.full(padshape, np.nan)).to(arr.device), dim=dim)\n",
        "    return arr"
      ],
      "metadata": {
        "id": "T_LjdLhhiyGm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split with nan\n",
        "def split_with_nan(x, sections, axis=0):\n",
        "    assert x.dtype in [np.float16, np.float32, np.float64]\n",
        "    arrs = np.array_split(x, sections, axis=axis)\n",
        "    target_length = arrs[0].shape[axis]\n",
        "    for i in range(len(arrs)):\n",
        "        arrs[i] = pad_nan_to_target(arrs[i], target_length, axis=axis)\n",
        "    return arrs"
      ],
      "metadata": {
        "id": "8Z2FotiSArtJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pad with nan\n",
        "def pad_nan_to_target(array, target_length, axis=0):\n",
        "    assert array.dtype in [np.float16, np.float32, np.float64]\n",
        "    pad_size = target_length - array.shape[axis]\n",
        "    if pad_size <= 0:\n",
        "        return array\n",
        "    npad = [(0, 0)] * array.ndim\n",
        "    npad[axis] = (0, pad_size)\n",
        "    \n",
        "    return np.pad(array, pad_width=npad, mode='constant', constant_values=np.nan)"
      ],
      "metadata": {
        "id": "1SAjsR_-A3RB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def centerize_vary_length_series(x):\n",
        "    prefix_zeros = np.argmax(~np.isnan(x).all(axis=-1), axis=1)\n",
        "    suffix_zeros = np.argmax(~np.isnan(x[:, ::-1]).all(axis=-1), axis=1)\n",
        "    offset = (prefix_zeros + suffix_zeros) // 2 - prefix_zeros\n",
        "    rows, column_indices = np.ogrid[:x.shape[0], :x.shape[1]]\n",
        "    offset[offset < 0] += x.shape[1]\n",
        "    column_indices = column_indices - offset[:, np.newaxis]\n",
        "    \n",
        "    return x[rows, column_indices]"
      ],
      "metadata": {
        "id": "eK2M1qV3JBdl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# random seed functions\n",
        "def seed_everything(seed):\n",
        "    pl.seed_everything(seed, workers=True)\n",
        "    random.seed(seed) \n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "mzBQAeoIi8l2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Constants"
      ],
      "metadata": {
        "id": "94xSbfaKkOmq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# logger\n",
        "LOG_LEVEL = logging.DEBUG\n",
        "\n",
        "# datasets\n",
        "ELECTRICITY = \"electricity\"\n",
        "\n",
        "# models\n",
        "CoPST = \"CoPST\"\n",
        "\n",
        "# device\n",
        "\n",
        "DEVICE = torch.device('cpu')\n",
        "\n",
        "if torch.cuda.is_available:\n",
        "    DEVICE = torch.device('cuda')\n",
        "\n",
        "#paths\n",
        "ROOT_FOLDER = \"/content/drive/MyDrive/Tesi/code\"\n",
        "MODEL_FOLDER = ROOT_FOLDER + \"/models\"\n",
        "CHECKPOINT_FOLDER = ROOT_FOLDER + \"/checkpoints\"\n",
        "LOGS_FOLDER = ROOT_FOLDER + \"/logs\"\n",
        "\n",
        "#hyperparameters\n",
        "\n",
        "# Train\n",
        "BATCH_SIZE = 4\n",
        "STRIDE = 4\n",
        "PATCH_LEN = 8\n",
        "L = 32\n",
        "PATCH_NUM = get_patch_num(PATCH_LEN, L, STRIDE)\n",
        "M = 370 # TODO: generalize for all datasets\n",
        "DICT_MOMENTUM_SIZE = 100\n",
        "\n",
        "# Eval\n",
        "MAX_TRAIN_LENGTH = 101\n",
        "PADDING = MAX_TRAIN_LENGTH-1\n",
        "EVAL_PATCH_NUM = get_patch_num(PATCH_LEN, MAX_TRAIN_LENGTH, STRIDE)\n",
        "EVAL_BATCH_SIZE = 32\n",
        "\n",
        "\n",
        "# training\n",
        "DETERMINISTIC = True\n",
        "LOAD_MODEL = False"
      ],
      "metadata": {
        "id": "ySxfaOHQkQ_B"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logger"
      ],
      "metadata": {
        "id": "8j5-8dn5ppGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create logger\n",
        "log = logging.getLogger('APP')\n",
        "log.setLevel(LOG_LEVEL)\n",
        "logging.basicConfig(level=LOG_LEVEL)\n",
        "\n",
        "# # create console handler and set level to debug\n",
        "# ch = logging.StreamHandler()\n",
        "# ch.setLevel(logging.INFO)\n",
        "\n",
        "# # create formatter\n",
        "# formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# # add formatter to ch\n",
        "# ch.setFormatter(formatter)\n",
        "\n",
        "# # add ch to logger\n",
        "# logger.addHandler(ch)"
      ],
      "metadata": {
        "id": "iRLWiTu4mlx9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "ma655OWbiZ0h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Datasets"
      ],
      "metadata": {
        "id": "GS_gE31AieGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datasets_path = {\n",
        "    ELECTRICITY: ROOT_FOLDER + \"/datasets/electricity\"\n",
        "}\n",
        "\n",
        "datasets_name = {\n",
        "    ELECTRICITY: \"/LD2011_2014.txt\"    \n",
        "}\n",
        "datasets_processed_name = {\n",
        "    ELECTRICITY: \"/electricity.npy\"\n",
        "}"
      ],
      "metadata": {
        "id": "nX8g0950j36s"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Electricity"
      ],
      "metadata": {
        "id": "S84zQ9UjigKj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing**"
      ],
      "metadata": {
        "id": "o-wdZhMWZybk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df = pd.read_csv(datasets_path[ELECTRICITY] + datasets_name[ELECTRICITY], sep = ';')\n",
        "# df.rename(columns={df.columns[0]: 'Date'},inplace=True)\n",
        "# values = df.values\n",
        "# values = values[:, 1:].astype(str)\n",
        "# for i, value in enumerate(values):\n",
        "#   values[i] = np.char.replace(value, \",\", \".\")\n",
        "# values = values.astype(np.float32)\n",
        "# np.save(datasets_path[ELECTRICITY] + \"/electricity\", values)"
      ],
      "metadata": {
        "id": "YsyolJAOjZVS"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # call this function to initialize the csv file\n",
        "# def electricity_preprocess(path):\n",
        "#     data_ecl = pd.read_csv(path + '/LD2011_2014.txt', parse_dates=True, sep=';', decimal=',', index_col=0)\n",
        "#     data_ecl = data_ecl.resample('1h', closed='right').sum()\n",
        "#     data_ecl = data_ecl.loc[:, data_ecl.cumsum(axis=0).iloc[8920] != 0]  # filter out instances with missing values\n",
        "#     data_ecl.index = data_ecl.index.rename('date')\n",
        "#     data_ecl = data_ecl['2012':]\n",
        "#     data_ecl.to_csv(path + '/electricity.csv')\n",
        "#     log.info(\"electriciy.csv created!\")"
      ],
      "metadata": {
        "id": "HqM-Bf5vWJDg"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# electricity_preprocess(datasets_path[ELECTRICITY])"
      ],
      "metadata": {
        "id": "YS1CZBhuWei-"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ElectricityDataset(Dataset):\n",
        "    def __init__(self, data, look_window, eval_mode = False, p = 0.5, multiplier = 10):\n",
        "        super().__init__()\n",
        "        self.data = data\n",
        "        self.look_window = look_window\n",
        "        self.eval_mode = eval_mode\n",
        "        self.p = p\n",
        "        self.multiplier = multiplier\n",
        "        self.epsilon = torch.empty(1).normal_(mean = 0, std = 0.5)\n",
        "        self.N, self.T, self.D = data.shape # num_ts, time, dim\n",
        "\n",
        "    def __len__(self):\n",
        "        # return self.data.shape[0] // self.look_window\n",
        "        return self.data.shape[0] * self.multiplier\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        out: [B x MAX_TRAIN_LEN x TIME_DIM] \n",
        "        \"\"\"\n",
        "        # # rescale the index\n",
        "        # start = idx * self.look_window\n",
        "        # end = start + self.look_window\n",
        "        # # tale all the channels and skip the date\n",
        "        # ts = self.data[start:end]\n",
        "        # # convert in a tensor\n",
        "        # ts = torch.from_numpy(ts) # [L x M]\n",
        "        # ts = einops.rearrange(ts, 'l m -> m l')\n",
        "\n",
        "        # # during the evaluation of the encoder we want the untouched time serie\n",
        "        # if self.eval_mode:\n",
        "        #     return ts\n",
        "        ts = self.data[idx % self.N]\n",
        "\n",
        "        return self.transform(ts), self.transform(ts)\n",
        "\n",
        "    def get_len(self):\n",
        "        return self.__len__()\n",
        "\n",
        "    # def get_channels(self):\n",
        "    #     return self.data.iloc[0, 1:].astype(str).str.replace(',', '.').astype('float32').shape[0]\n",
        "    \n",
        "    def transform(self, x):\n",
        "        return self.jitter(self.shift(self.scale(x)))\n",
        "\n",
        "    def jitter(self, x):\n",
        "        if random.random() > self.p:\n",
        "            return x\n",
        "        return x + torch.empty(1).normal_(mean = 0, std = 0.5)\n",
        "\n",
        "    def scale(self, x):\n",
        "        if random.random() > self.p:\n",
        "            return x\n",
        "        return x * self.epsilon\n",
        "\n",
        "    def shift(self, x):\n",
        "        if random.random() > self.p:\n",
        "            return x\n",
        "        return x + self.epsilon\n",
        "\n",
        "class ElectricityDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, path, batch_size, look_window, max_train_length = None, train_size = 0.6, test_size = 0.2):\n",
        "        super().__init__()\n",
        "        self.path = path # path to csv file\n",
        "        self.batch_size = batch_size\n",
        "        self.look_window = look_window\n",
        "        self.max_train_length = max_train_length\n",
        "        self.train_size = train_size\n",
        "        self.test_size = test_size\n",
        "        # self.data = np.load(path)\n",
        "\n",
        "    # def prepare_data(self):\n",
        "    #     # download\n",
        "\n",
        "    def _get_time_features(self,dt):\n",
        "        return np.stack([\n",
        "                dt.minute.to_numpy(),\n",
        "                dt.hour.to_numpy(),\n",
        "                dt.dayofweek.to_numpy(),\n",
        "                dt.day.to_numpy(),\n",
        "                dt.dayofyear.to_numpy(),\n",
        "                dt.month.to_numpy(),\n",
        "                dt.weekofyear.to_numpy(),\n",
        "            ], axis=1).astype(np.float)\n",
        "\n",
        "    def _load_forecast_csv(self, path):\n",
        "        data = pd.read_csv(path, index_col='date', parse_dates=True)\n",
        "        dt_embed = self.get_time_features(data.index)\n",
        "        n_covariate_cols = dt_embed.shape[-1]\n",
        "        data = data.to_numpy()\n",
        "        # compute slices\n",
        "        self.train_slice = slice(None, int(self.train_size * len(data)))\n",
        "        self.valid_slice = slice(int(self.train_size * len(data)), - int(self.test_size * len(data)))\n",
        "        self.test_slice = slice(- int(self.test_size * len(data)), None)\n",
        "\n",
        "        return data, dt_embed, n_covariate_cols\n",
        "\n",
        "    def _scale_and_transform(self, data, dt_embed, n_covariate_cols):\n",
        "        # scale data\n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(data[self.train_slice])\n",
        "        data = scaler.transform(data)\n",
        "        # NUM_ROWS x NUM_FEATURES -> NUM_FEATURES x NUM_ROWS x 1\n",
        "        data = np.expand_dims(data.T, -1)  # Each variable is an instance rather than a feature\n",
        "        if n_covariate_cols > 0:\n",
        "            dt_scaler = StandardScaler().fit(dt_embed[train_slice])\n",
        "            dt_embed = np.expand_dims(dt_scaler.transform(dt_embed), 0)\n",
        "            data = np.concatenate([np.repeat(dt_embed, data.shape[0], axis=0), data], axis=-1)\n",
        "        pred_lens = [24, 48, 96, 288, 672]\n",
        "\n",
        "        return data, scaler, pred_lens, n_covariate_cols\n",
        "\n",
        "    def _fit_setup(self, train_data):\n",
        "        if self.max_train_length is not None:\n",
        "            sections = train_data.shape[1] // self.max_train_length\n",
        "        if sections >= 2:\n",
        "            train_data = np.concatenate(split_with_nan(train_data, sections, axis=1), axis=0)\n",
        "\n",
        "        temporal_missing = np.isnan(train_data).all(axis=-1).any(axis=0)\n",
        "        if temporal_missing[0] or temporal_missing[-1]:\n",
        "            train_data = centerize_vary_length_series(train_data)\n",
        "        \n",
        "        train_data = train_data[~np.isnan(train_data).all(axis=2).all(axis=1)]\n",
        "\n",
        "        multiplier = 1 if train_data.shape[0] >= self.batch_size else math.ceil(self.batch_size / train_data.shape[0])\n",
        "\n",
        "        return train_data, multiplier\n",
        "\n",
        "\n",
        "    def setup(self, stage: str):\n",
        "        # load csv\n",
        "        data, dt_embed, n_covariate_cols = self._load_forecast_csv(self.path)\n",
        "\n",
        "        # scale and transform\n",
        "        data, scaler, pred_lens, n_covariate_cols = self._scale_and_transform(data, dt_embed, n_covariate_cols)\n",
        "        \n",
        "        # Assign train/val datasets for use in dataloaders\n",
        "        if stage == \"fit\":\n",
        "            train_data = data[self.train_slice]\n",
        "            train_data, multiplier = _fit_setup(self, train_data)\n",
        "            # fit setup\n",
        "            self.train = ElectricityDataset(torch.from_numpy(train_data).to(torch.float), self.look_window, multiplier = multiplier)\n",
        "            # self.validate = ElectricityDataset(data[self.valid_slice], self.look_window)\n",
        "\n",
        "        # Assign test dataset for use in dataloader(s)\n",
        "        if stage == \"test\":\n",
        "            self.test = ElectricityDataset(data[self.test_slice], self.look_window)\n",
        "\n",
        "        # if stage == \"predict\":\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train, batch_size=self.batch_size, drop_last = True)\n",
        "\n",
        "    # def val_dataloader(self):\n",
        "    #     return DataLoader(self.validate, batch_size=self.batch_size, drop_last = True)\n",
        "\n",
        "    # def test_dataloader(self):\n",
        "    #     return DataLoader(self.test, batch_size=self.batch_size, drop_last = True)\n",
        "\n",
        "    # def predict_dataloader(self):\n",
        "        \n"
      ],
      "metadata": {
        "id": "pWDN4NROaOAR"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "Oq9gmXnKGmVP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PatchTST"
      ],
      "metadata": {
        "id": "QD52xt7xGsd9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility functions\n",
        "\n",
        "def create_patches(xb, patch_len, stride):\n",
        "    \"\"\"\n",
        "    xb -> [B x L x M]\n",
        "    output -> [B x N x M x P], N\n",
        "    \"\"\"\n",
        "    _, num_var, _ = xb.shape\n",
        "    # compute number of patches\n",
        "    patch_num = (max(patch_len, num_var)-patch_len) // stride + 2\n",
        "\n",
        "    # we repeat the last variable of the sequence to have equal patches\n",
        "    tail = torch.repeat_interleave(xb[:,-1:,:], stride, dim = 1)\n",
        "    xb = torch.concatenate((xb, tail), axis = 1)\n",
        "\n",
        "    # create patches\n",
        "    xb = xb.unfold(dimension=1, size=patch_len, step=stride)\n",
        "\n",
        "    assert patch_num == xb.shape[1], f\"wrong number of computed patches, expected {patch_num} but computed {xb.shape[1]}\"\n",
        "\n",
        "    return xb, patch_num\n",
        "\n",
        "\"\"\"\n",
        "ref: https://kazemnejad.com/blog/transformer_architecture_positional_encoding/\n",
        "\"\"\"\n",
        "\n",
        "def positional_encoding(batch_size, patch_num, d_model):\n",
        "    \"\"\"\n",
        "    output -> [B x N x D]\n",
        "    \"\"\"\n",
        "    pe = torch.zeros(batch_size, patch_num, d_model)\n",
        "    # create a positional array\n",
        "    position = torch.arange(0, patch_num).unsqueeze(1)\n",
        "    # div term for half of positions\n",
        "    div_term = torch.pow(10000.0, torch.arange(0, d_model, 2) / d_model) \n",
        "    # even positions\n",
        "    pe[:, :, 0::2] = torch.sin(position * div_term)\n",
        "    # odd positions\n",
        "    pe[:, :, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "    # if normalize:\n",
        "    #     pe = pe - pe.mean()\n",
        "    #     pe = pe / (pe.std() * 10)\n",
        "    \n",
        "    return nn.parameter.Parameter(pe, requires_grad= False)"
      ],
      "metadata": {
        "id": "hbRO5NzrNy8b"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PatchTST\n",
        "\n",
        "class PatchTSTEncoder(nn.Module):\n",
        "    def __init__(self, num_channels, num_var, patch_len, stride, batch_size, d_model = 128, n_layers = 3, n_heads = 16, dropout = 0.2):\n",
        "        super(PatchTSTEncoder, self).__init__()\n",
        "\n",
        "        self.log = setup_log(self, LOG_LEVEL)\n",
        "\n",
        "        self.num_channels = num_channels\n",
        "        self.patch_num = (max(patch_len, num_var)-patch_len) // stride + 2\n",
        "        self.patch_len = patch_len\n",
        "        self.stride = stride\n",
        "        self.d_model = d_model\n",
        "        self.n_layers = n_layers\n",
        "        self.n_heads = n_heads\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # instance normalization\n",
        "        \"\"\"\n",
        "        ref: https://wandb.ai/wandb_fc/Normalization-Series/reports/Instance-Normalization-in-PyTorch-With-Examples---VmlldzoxNDIyNTQx\n",
        "        \"\"\"\n",
        "        self.inst_norm = nn.InstanceNorm2d(num_channels)\n",
        "\n",
        "        # patch creation\n",
        "        self.create_patch = create_patches\n",
        "\n",
        "        # embedding\n",
        "        self.W_p = nn.Linear(patch_len, d_model, bias = False)\n",
        "\n",
        "        # positional encoding\n",
        "        self.W_pos = positional_encoding(batch_size * num_channels, self.patch_num, d_model)\n",
        "\n",
        "        # dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # encoder\n",
        "        self.encoders = nn.ModuleList([VanillaTransformerEncoder(d_model) for _ in range(n_layers)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x -> [B x L x M] // [B x MAX_TRAIN_LENGTH x TIME_DIM]\n",
        "        output -> [(B M) x N x D]\n",
        "        \"\"\"\n",
        "        # [B x MAX_TRAIN_LENGTH x TIME_DIM] -> [(B / M) x M x MAX_TRAIN_LENGTH x TIME_DIM]\n",
        "        x = einops.rearrange(x, '(b m) l t -> b m l t', m=self.num_channels)\n",
        "        # we need to reshape dimensione before apply instance normalization\n",
        "        x = einops.rearrange(self.inst_norm(x), 'b m l t -> b l m')\n",
        "\n",
        "        # create patches\n",
        "        x, patch_num = self.create_patch(x, self.patch_len, self.stride)\n",
        "\n",
        "        assert self.patch_num == patch_num, f\"wrong number for patch_num {self.patch_num} != {patch_num}\"\n",
        "\n",
        "        # reshape the tensor from [B x M x P x N] -> [(B M) x P x N]\n",
        "        x = einops.rearrange(x, 'b n m p -> (b m) n p')\n",
        "        # now it can be provided to our transformer implementation\n",
        "\n",
        "        # project into transformer latent space\n",
        "        x = self.W_p(x) + self.W_pos\n",
        "\n",
        "        for layer in self.encoders:\n",
        "            x = layer(x)\n",
        "\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "bhiJYUZcGvr_"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand((20, 5, 8))\n",
        "print(x)\n",
        "x = einops.rearrange(x, '(b m) l t -> b m l t', m = 5)\n",
        "print(x)"
      ],
      "metadata": {
        "id": "FFDgrPckJT9_",
        "outputId": "fb2959f7-772c-4863-dba8-eaad3c86c867",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.0771, 0.9551, 0.5750, 0.8607, 0.2842, 0.2567, 0.1030, 0.8928],\n",
            "         [0.5179, 0.2541, 0.5152, 0.9714, 0.9914, 0.6119, 0.8334, 0.6464],\n",
            "         [0.4312, 0.5070, 0.4355, 0.0455, 0.3608, 0.5046, 0.1488, 0.6574],\n",
            "         [0.8790, 0.6925, 0.7008, 0.9928, 0.0425, 0.0401, 0.0765, 0.6839],\n",
            "         [0.5815, 0.6027, 0.0860, 0.9602, 0.7579, 0.9200, 0.4548, 0.4238]],\n",
            "\n",
            "        [[0.4695, 0.2174, 0.4989, 0.6476, 0.0256, 0.5851, 0.7632, 0.0012],\n",
            "         [0.4695, 0.8846, 0.9695, 0.9371, 0.6236, 0.7619, 0.1567, 0.7923],\n",
            "         [0.6194, 0.5394, 0.8709, 0.9609, 0.7827, 0.5662, 0.2582, 0.6775],\n",
            "         [0.8408, 0.6874, 0.5933, 0.4990, 0.9933, 0.9455, 0.9197, 0.4248],\n",
            "         [0.1294, 0.2644, 0.1834, 0.8223, 0.3425, 0.1129, 0.3553, 0.2498]],\n",
            "\n",
            "        [[0.2912, 0.5762, 0.3932, 0.2855, 0.6231, 0.5789, 0.6942, 0.5631],\n",
            "         [0.8057, 0.9157, 0.9950, 0.1274, 0.3900, 0.5570, 0.5648, 0.0524],\n",
            "         [0.9241, 0.2446, 0.9494, 0.1966, 0.5180, 0.8692, 0.3382, 0.7674],\n",
            "         [0.6744, 0.3396, 0.8171, 0.1413, 0.5960, 0.7448, 0.2513, 0.5411],\n",
            "         [0.4597, 0.4150, 0.6108, 0.0389, 0.2278, 0.2270, 0.1878, 0.5040]],\n",
            "\n",
            "        [[0.1677, 0.0807, 0.4778, 0.8831, 0.1192, 0.8484, 0.2734, 0.4269],\n",
            "         [0.7713, 0.6187, 0.8431, 0.3023, 0.9914, 0.3873, 0.2077, 0.1579],\n",
            "         [0.2060, 0.3545, 0.0533, 0.4118, 0.0709, 0.2546, 0.2185, 0.4240],\n",
            "         [0.0023, 0.3043, 0.0572, 0.4242, 0.2485, 0.0222, 0.8283, 0.5598],\n",
            "         [0.6107, 0.0264, 0.7372, 0.9120, 0.9742, 0.8995, 0.4828, 0.2519]],\n",
            "\n",
            "        [[0.8667, 0.6288, 0.3292, 0.4692, 0.4606, 0.4587, 0.6561, 0.2163],\n",
            "         [0.0896, 0.9427, 0.6055, 0.8340, 0.9469, 0.9675, 0.1050, 0.3489],\n",
            "         [0.8870, 0.1289, 0.4788, 0.9838, 0.5894, 0.6015, 0.9905, 0.9626],\n",
            "         [0.9345, 0.6857, 0.4490, 0.1425, 0.5935, 0.5018, 0.9190, 0.3185],\n",
            "         [0.9445, 0.0249, 0.3106, 0.5257, 0.4820, 0.1517, 0.1680, 0.0302]],\n",
            "\n",
            "        [[0.4268, 0.6375, 0.0055, 0.0045, 0.2023, 0.2630, 0.9367, 0.7208],\n",
            "         [0.3083, 0.9205, 0.3227, 0.4795, 0.0597, 0.8118, 0.7037, 0.7579],\n",
            "         [0.5536, 0.3852, 0.6663, 0.6201, 0.4723, 0.5508, 0.9355, 0.5183],\n",
            "         [0.9609, 0.0225, 0.5532, 0.1060, 0.3861, 0.1484, 0.9974, 0.1094],\n",
            "         [0.0129, 0.0186, 0.0431, 0.4348, 0.3542, 0.1835, 0.5795, 0.1104]],\n",
            "\n",
            "        [[0.2250, 0.4664, 0.8415, 0.5839, 0.7725, 0.1609, 0.1711, 0.0102],\n",
            "         [0.5449, 0.8569, 0.6088, 0.8829, 0.3068, 0.8941, 0.3952, 0.9210],\n",
            "         [0.2714, 0.8952, 0.6167, 0.6960, 0.4728, 0.1630, 0.4038, 0.7975],\n",
            "         [0.8747, 0.4076, 0.5861, 0.8641, 0.3053, 0.5829, 0.5373, 0.8588],\n",
            "         [0.5755, 0.9236, 0.3294, 0.9634, 0.5055, 0.9028, 0.9130, 0.4907]],\n",
            "\n",
            "        [[0.6990, 0.2592, 0.2796, 0.5846, 0.7334, 0.8556, 0.7336, 0.4115],\n",
            "         [0.7487, 0.3979, 0.0156, 0.0865, 0.6589, 0.4407, 0.0259, 0.6857],\n",
            "         [0.1871, 0.2449, 0.1360, 0.6393, 0.2372, 0.3938, 0.4993, 0.2950],\n",
            "         [0.8377, 0.4248, 0.3884, 0.2237, 0.2937, 0.0136, 0.3406, 0.7185],\n",
            "         [0.4365, 0.5054, 0.5094, 0.9207, 0.7891, 0.2606, 0.7853, 0.7909]],\n",
            "\n",
            "        [[0.9129, 0.4867, 0.6084, 0.6001, 0.4941, 0.6555, 0.2244, 0.9097],\n",
            "         [0.3954, 0.4054, 0.4244, 0.4162, 0.2251, 0.9304, 0.7979, 0.1966],\n",
            "         [0.8943, 0.5588, 0.0182, 0.1271, 0.1078, 0.8110, 0.3700, 0.6996],\n",
            "         [0.4986, 0.1621, 0.8002, 0.3389, 0.2172, 0.0398, 0.0168, 0.9779],\n",
            "         [0.8281, 0.6049, 0.4793, 0.2170, 0.4459, 0.1156, 0.0618, 0.5077]],\n",
            "\n",
            "        [[0.6856, 0.4368, 0.5295, 0.7169, 0.2335, 0.1447, 0.6826, 0.3160],\n",
            "         [0.0514, 0.8759, 0.4911, 0.0994, 0.0330, 0.0392, 0.7278, 0.4935],\n",
            "         [0.4301, 0.0552, 0.8641, 0.5010, 0.2685, 0.5028, 0.4148, 0.2090],\n",
            "         [0.2476, 0.1715, 0.5327, 0.3870, 0.9061, 0.4181, 0.1855, 0.5190],\n",
            "         [0.1513, 0.4291, 0.2916, 0.4414, 0.7790, 0.2795, 0.2204, 0.1568]],\n",
            "\n",
            "        [[0.3969, 0.4837, 0.7582, 0.8573, 0.9119, 0.7340, 0.9006, 0.3476],\n",
            "         [0.8464, 0.9208, 0.2742, 0.3694, 0.9141, 0.3221, 0.3752, 0.5255],\n",
            "         [0.9086, 0.0532, 0.0545, 0.3808, 0.0969, 0.7522, 0.3994, 0.3960],\n",
            "         [0.0097, 0.6327, 0.7613, 0.7733, 0.0223, 0.2164, 0.9301, 0.6302],\n",
            "         [0.3901, 0.4909, 0.8396, 0.4127, 0.6166, 0.4727, 0.8176, 0.4927]],\n",
            "\n",
            "        [[0.6430, 0.3529, 0.4205, 0.5768, 0.3089, 0.9822, 0.8701, 0.5999],\n",
            "         [0.1055, 0.6618, 0.4106, 0.3875, 0.0769, 0.3296, 0.1936, 0.5057],\n",
            "         [0.3829, 0.4368, 0.6313, 0.7966, 0.5294, 0.0921, 0.0012, 0.3333],\n",
            "         [0.8099, 0.0708, 0.9673, 0.6858, 0.4191, 0.2344, 0.6071, 0.0381],\n",
            "         [0.2219, 0.7499, 0.5287, 0.4696, 0.9340, 0.8509, 0.2862, 0.5211]],\n",
            "\n",
            "        [[0.9256, 0.4901, 0.0362, 0.2260, 0.3892, 0.4633, 0.3554, 0.5292],\n",
            "         [0.8584, 0.7366, 0.3376, 0.2377, 0.9407, 0.3414, 0.4816, 0.7355],\n",
            "         [0.4995, 0.9460, 0.6750, 0.3713, 0.5637, 0.5708, 0.0360, 0.7222],\n",
            "         [0.8102, 0.1161, 0.9743, 0.9326, 0.0489, 0.5399, 0.3516, 0.0761],\n",
            "         [0.6171, 0.6104, 0.9749, 0.9336, 0.9592, 0.4859, 0.1691, 0.6213]],\n",
            "\n",
            "        [[0.2553, 0.9059, 0.0353, 0.2625, 0.5492, 0.4369, 0.2917, 0.0989],\n",
            "         [0.1290, 0.3076, 0.9632, 0.7681, 0.2181, 0.1244, 0.8864, 0.1612],\n",
            "         [0.6635, 0.2182, 0.8984, 0.4128, 0.5264, 0.3617, 0.0472, 0.5689],\n",
            "         [0.5185, 0.7900, 0.1954, 0.6284, 0.4087, 0.5854, 0.6558, 0.9883],\n",
            "         [0.5984, 0.4295, 0.4386, 0.1953, 0.4196, 0.3372, 0.7392, 0.8538]],\n",
            "\n",
            "        [[0.2529, 0.5405, 0.8596, 0.8807, 0.5874, 0.9413, 0.3763, 0.4531],\n",
            "         [0.8836, 0.1732, 0.9583, 0.9941, 0.4321, 0.5993, 0.0748, 0.5664],\n",
            "         [0.9405, 0.1931, 0.9936, 0.6038, 0.1361, 0.3474, 0.5169, 0.2091],\n",
            "         [0.5680, 0.1486, 0.5100, 0.2746, 0.7196, 0.7894, 0.0317, 0.2002],\n",
            "         [0.2422, 0.7475, 0.7382, 0.5858, 0.1247, 0.4749, 0.3836, 0.4820]],\n",
            "\n",
            "        [[0.5676, 0.3304, 0.3139, 0.9907, 0.9626, 0.6453, 0.9000, 0.1395],\n",
            "         [0.7258, 0.4061, 0.9857, 0.0236, 0.1480, 0.4635, 0.0226, 0.7184],\n",
            "         [0.8115, 0.9341, 0.3084, 0.6922, 0.5624, 0.9227, 0.1907, 0.3122],\n",
            "         [0.6199, 0.3574, 0.9543, 0.4832, 0.3269, 0.4114, 0.0083, 0.3845],\n",
            "         [0.5129, 0.6735, 0.6946, 0.1691, 0.2202, 0.4146, 0.5549, 0.3418]],\n",
            "\n",
            "        [[0.1716, 0.1755, 0.4351, 0.8659, 0.3077, 0.5225, 0.0317, 0.0394],\n",
            "         [0.4464, 0.4077, 0.8449, 0.4118, 0.2060, 0.3828, 0.9356, 0.4541],\n",
            "         [0.8490, 0.9457, 0.6313, 0.0673, 0.5334, 0.2086, 0.5437, 0.6834],\n",
            "         [0.1571, 0.3478, 0.2378, 0.0568, 0.3606, 0.2946, 0.6713, 0.1492],\n",
            "         [0.4002, 0.3484, 0.4997, 0.2277, 0.9506, 0.9378, 0.0986, 0.3568]],\n",
            "\n",
            "        [[0.5640, 0.8989, 0.2349, 0.1036, 0.8064, 0.8658, 0.3853, 0.8048],\n",
            "         [0.2044, 0.1927, 0.0627, 0.4901, 0.3902, 0.4576, 0.6171, 0.3744],\n",
            "         [0.7932, 0.9041, 0.0453, 0.9527, 0.9241, 0.3817, 0.5096, 0.4822],\n",
            "         [0.7291, 0.7188, 0.2764, 0.9127, 0.1074, 0.0613, 0.3319, 0.8336],\n",
            "         [0.8200, 0.9231, 0.2557, 0.4743, 0.1838, 0.8513, 0.1468, 0.3103]],\n",
            "\n",
            "        [[0.3161, 0.4024, 0.7024, 0.4607, 0.0161, 0.2781, 0.3693, 0.7287],\n",
            "         [0.5030, 0.9817, 0.4593, 0.5800, 0.3953, 0.8279, 0.3023, 0.4987],\n",
            "         [0.6932, 0.9768, 0.3193, 0.4808, 0.0211, 0.6093, 0.0547, 0.1612],\n",
            "         [0.2441, 0.4907, 0.2435, 0.3382, 0.9204, 0.4271, 0.1702, 0.3280],\n",
            "         [0.4320, 0.3901, 0.2292, 0.3230, 0.5728, 0.1506, 0.6042, 0.8699]],\n",
            "\n",
            "        [[0.0198, 0.3856, 0.8792, 0.4851, 0.5635, 0.4841, 0.6062, 0.8955],\n",
            "         [0.3890, 0.9672, 0.5498, 0.6164, 0.7041, 0.6676, 0.3767, 0.0888],\n",
            "         [0.8064, 0.9818, 0.3980, 0.2198, 0.2022, 0.4971, 0.6399, 0.9888],\n",
            "         [0.4058, 0.9437, 0.1916, 0.4659, 0.9395, 0.3267, 0.0476, 0.7374],\n",
            "         [0.6736, 0.5231, 0.1456, 0.6605, 0.9653, 0.4320, 0.9229, 0.5563]]])\n",
            "tensor([[[[0.0771, 0.9551, 0.5750, 0.8607, 0.2842, 0.2567, 0.1030, 0.8928],\n",
            "          [0.5179, 0.2541, 0.5152, 0.9714, 0.9914, 0.6119, 0.8334, 0.6464],\n",
            "          [0.4312, 0.5070, 0.4355, 0.0455, 0.3608, 0.5046, 0.1488, 0.6574],\n",
            "          [0.8790, 0.6925, 0.7008, 0.9928, 0.0425, 0.0401, 0.0765, 0.6839],\n",
            "          [0.5815, 0.6027, 0.0860, 0.9602, 0.7579, 0.9200, 0.4548, 0.4238]],\n",
            "\n",
            "         [[0.4695, 0.2174, 0.4989, 0.6476, 0.0256, 0.5851, 0.7632, 0.0012],\n",
            "          [0.4695, 0.8846, 0.9695, 0.9371, 0.6236, 0.7619, 0.1567, 0.7923],\n",
            "          [0.6194, 0.5394, 0.8709, 0.9609, 0.7827, 0.5662, 0.2582, 0.6775],\n",
            "          [0.8408, 0.6874, 0.5933, 0.4990, 0.9933, 0.9455, 0.9197, 0.4248],\n",
            "          [0.1294, 0.2644, 0.1834, 0.8223, 0.3425, 0.1129, 0.3553, 0.2498]],\n",
            "\n",
            "         [[0.2912, 0.5762, 0.3932, 0.2855, 0.6231, 0.5789, 0.6942, 0.5631],\n",
            "          [0.8057, 0.9157, 0.9950, 0.1274, 0.3900, 0.5570, 0.5648, 0.0524],\n",
            "          [0.9241, 0.2446, 0.9494, 0.1966, 0.5180, 0.8692, 0.3382, 0.7674],\n",
            "          [0.6744, 0.3396, 0.8171, 0.1413, 0.5960, 0.7448, 0.2513, 0.5411],\n",
            "          [0.4597, 0.4150, 0.6108, 0.0389, 0.2278, 0.2270, 0.1878, 0.5040]],\n",
            "\n",
            "         [[0.1677, 0.0807, 0.4778, 0.8831, 0.1192, 0.8484, 0.2734, 0.4269],\n",
            "          [0.7713, 0.6187, 0.8431, 0.3023, 0.9914, 0.3873, 0.2077, 0.1579],\n",
            "          [0.2060, 0.3545, 0.0533, 0.4118, 0.0709, 0.2546, 0.2185, 0.4240],\n",
            "          [0.0023, 0.3043, 0.0572, 0.4242, 0.2485, 0.0222, 0.8283, 0.5598],\n",
            "          [0.6107, 0.0264, 0.7372, 0.9120, 0.9742, 0.8995, 0.4828, 0.2519]],\n",
            "\n",
            "         [[0.8667, 0.6288, 0.3292, 0.4692, 0.4606, 0.4587, 0.6561, 0.2163],\n",
            "          [0.0896, 0.9427, 0.6055, 0.8340, 0.9469, 0.9675, 0.1050, 0.3489],\n",
            "          [0.8870, 0.1289, 0.4788, 0.9838, 0.5894, 0.6015, 0.9905, 0.9626],\n",
            "          [0.9345, 0.6857, 0.4490, 0.1425, 0.5935, 0.5018, 0.9190, 0.3185],\n",
            "          [0.9445, 0.0249, 0.3106, 0.5257, 0.4820, 0.1517, 0.1680, 0.0302]]],\n",
            "\n",
            "\n",
            "        [[[0.4268, 0.6375, 0.0055, 0.0045, 0.2023, 0.2630, 0.9367, 0.7208],\n",
            "          [0.3083, 0.9205, 0.3227, 0.4795, 0.0597, 0.8118, 0.7037, 0.7579],\n",
            "          [0.5536, 0.3852, 0.6663, 0.6201, 0.4723, 0.5508, 0.9355, 0.5183],\n",
            "          [0.9609, 0.0225, 0.5532, 0.1060, 0.3861, 0.1484, 0.9974, 0.1094],\n",
            "          [0.0129, 0.0186, 0.0431, 0.4348, 0.3542, 0.1835, 0.5795, 0.1104]],\n",
            "\n",
            "         [[0.2250, 0.4664, 0.8415, 0.5839, 0.7725, 0.1609, 0.1711, 0.0102],\n",
            "          [0.5449, 0.8569, 0.6088, 0.8829, 0.3068, 0.8941, 0.3952, 0.9210],\n",
            "          [0.2714, 0.8952, 0.6167, 0.6960, 0.4728, 0.1630, 0.4038, 0.7975],\n",
            "          [0.8747, 0.4076, 0.5861, 0.8641, 0.3053, 0.5829, 0.5373, 0.8588],\n",
            "          [0.5755, 0.9236, 0.3294, 0.9634, 0.5055, 0.9028, 0.9130, 0.4907]],\n",
            "\n",
            "         [[0.6990, 0.2592, 0.2796, 0.5846, 0.7334, 0.8556, 0.7336, 0.4115],\n",
            "          [0.7487, 0.3979, 0.0156, 0.0865, 0.6589, 0.4407, 0.0259, 0.6857],\n",
            "          [0.1871, 0.2449, 0.1360, 0.6393, 0.2372, 0.3938, 0.4993, 0.2950],\n",
            "          [0.8377, 0.4248, 0.3884, 0.2237, 0.2937, 0.0136, 0.3406, 0.7185],\n",
            "          [0.4365, 0.5054, 0.5094, 0.9207, 0.7891, 0.2606, 0.7853, 0.7909]],\n",
            "\n",
            "         [[0.9129, 0.4867, 0.6084, 0.6001, 0.4941, 0.6555, 0.2244, 0.9097],\n",
            "          [0.3954, 0.4054, 0.4244, 0.4162, 0.2251, 0.9304, 0.7979, 0.1966],\n",
            "          [0.8943, 0.5588, 0.0182, 0.1271, 0.1078, 0.8110, 0.3700, 0.6996],\n",
            "          [0.4986, 0.1621, 0.8002, 0.3389, 0.2172, 0.0398, 0.0168, 0.9779],\n",
            "          [0.8281, 0.6049, 0.4793, 0.2170, 0.4459, 0.1156, 0.0618, 0.5077]],\n",
            "\n",
            "         [[0.6856, 0.4368, 0.5295, 0.7169, 0.2335, 0.1447, 0.6826, 0.3160],\n",
            "          [0.0514, 0.8759, 0.4911, 0.0994, 0.0330, 0.0392, 0.7278, 0.4935],\n",
            "          [0.4301, 0.0552, 0.8641, 0.5010, 0.2685, 0.5028, 0.4148, 0.2090],\n",
            "          [0.2476, 0.1715, 0.5327, 0.3870, 0.9061, 0.4181, 0.1855, 0.5190],\n",
            "          [0.1513, 0.4291, 0.2916, 0.4414, 0.7790, 0.2795, 0.2204, 0.1568]]],\n",
            "\n",
            "\n",
            "        [[[0.3969, 0.4837, 0.7582, 0.8573, 0.9119, 0.7340, 0.9006, 0.3476],\n",
            "          [0.8464, 0.9208, 0.2742, 0.3694, 0.9141, 0.3221, 0.3752, 0.5255],\n",
            "          [0.9086, 0.0532, 0.0545, 0.3808, 0.0969, 0.7522, 0.3994, 0.3960],\n",
            "          [0.0097, 0.6327, 0.7613, 0.7733, 0.0223, 0.2164, 0.9301, 0.6302],\n",
            "          [0.3901, 0.4909, 0.8396, 0.4127, 0.6166, 0.4727, 0.8176, 0.4927]],\n",
            "\n",
            "         [[0.6430, 0.3529, 0.4205, 0.5768, 0.3089, 0.9822, 0.8701, 0.5999],\n",
            "          [0.1055, 0.6618, 0.4106, 0.3875, 0.0769, 0.3296, 0.1936, 0.5057],\n",
            "          [0.3829, 0.4368, 0.6313, 0.7966, 0.5294, 0.0921, 0.0012, 0.3333],\n",
            "          [0.8099, 0.0708, 0.9673, 0.6858, 0.4191, 0.2344, 0.6071, 0.0381],\n",
            "          [0.2219, 0.7499, 0.5287, 0.4696, 0.9340, 0.8509, 0.2862, 0.5211]],\n",
            "\n",
            "         [[0.9256, 0.4901, 0.0362, 0.2260, 0.3892, 0.4633, 0.3554, 0.5292],\n",
            "          [0.8584, 0.7366, 0.3376, 0.2377, 0.9407, 0.3414, 0.4816, 0.7355],\n",
            "          [0.4995, 0.9460, 0.6750, 0.3713, 0.5637, 0.5708, 0.0360, 0.7222],\n",
            "          [0.8102, 0.1161, 0.9743, 0.9326, 0.0489, 0.5399, 0.3516, 0.0761],\n",
            "          [0.6171, 0.6104, 0.9749, 0.9336, 0.9592, 0.4859, 0.1691, 0.6213]],\n",
            "\n",
            "         [[0.2553, 0.9059, 0.0353, 0.2625, 0.5492, 0.4369, 0.2917, 0.0989],\n",
            "          [0.1290, 0.3076, 0.9632, 0.7681, 0.2181, 0.1244, 0.8864, 0.1612],\n",
            "          [0.6635, 0.2182, 0.8984, 0.4128, 0.5264, 0.3617, 0.0472, 0.5689],\n",
            "          [0.5185, 0.7900, 0.1954, 0.6284, 0.4087, 0.5854, 0.6558, 0.9883],\n",
            "          [0.5984, 0.4295, 0.4386, 0.1953, 0.4196, 0.3372, 0.7392, 0.8538]],\n",
            "\n",
            "         [[0.2529, 0.5405, 0.8596, 0.8807, 0.5874, 0.9413, 0.3763, 0.4531],\n",
            "          [0.8836, 0.1732, 0.9583, 0.9941, 0.4321, 0.5993, 0.0748, 0.5664],\n",
            "          [0.9405, 0.1931, 0.9936, 0.6038, 0.1361, 0.3474, 0.5169, 0.2091],\n",
            "          [0.5680, 0.1486, 0.5100, 0.2746, 0.7196, 0.7894, 0.0317, 0.2002],\n",
            "          [0.2422, 0.7475, 0.7382, 0.5858, 0.1247, 0.4749, 0.3836, 0.4820]]],\n",
            "\n",
            "\n",
            "        [[[0.5676, 0.3304, 0.3139, 0.9907, 0.9626, 0.6453, 0.9000, 0.1395],\n",
            "          [0.7258, 0.4061, 0.9857, 0.0236, 0.1480, 0.4635, 0.0226, 0.7184],\n",
            "          [0.8115, 0.9341, 0.3084, 0.6922, 0.5624, 0.9227, 0.1907, 0.3122],\n",
            "          [0.6199, 0.3574, 0.9543, 0.4832, 0.3269, 0.4114, 0.0083, 0.3845],\n",
            "          [0.5129, 0.6735, 0.6946, 0.1691, 0.2202, 0.4146, 0.5549, 0.3418]],\n",
            "\n",
            "         [[0.1716, 0.1755, 0.4351, 0.8659, 0.3077, 0.5225, 0.0317, 0.0394],\n",
            "          [0.4464, 0.4077, 0.8449, 0.4118, 0.2060, 0.3828, 0.9356, 0.4541],\n",
            "          [0.8490, 0.9457, 0.6313, 0.0673, 0.5334, 0.2086, 0.5437, 0.6834],\n",
            "          [0.1571, 0.3478, 0.2378, 0.0568, 0.3606, 0.2946, 0.6713, 0.1492],\n",
            "          [0.4002, 0.3484, 0.4997, 0.2277, 0.9506, 0.9378, 0.0986, 0.3568]],\n",
            "\n",
            "         [[0.5640, 0.8989, 0.2349, 0.1036, 0.8064, 0.8658, 0.3853, 0.8048],\n",
            "          [0.2044, 0.1927, 0.0627, 0.4901, 0.3902, 0.4576, 0.6171, 0.3744],\n",
            "          [0.7932, 0.9041, 0.0453, 0.9527, 0.9241, 0.3817, 0.5096, 0.4822],\n",
            "          [0.7291, 0.7188, 0.2764, 0.9127, 0.1074, 0.0613, 0.3319, 0.8336],\n",
            "          [0.8200, 0.9231, 0.2557, 0.4743, 0.1838, 0.8513, 0.1468, 0.3103]],\n",
            "\n",
            "         [[0.3161, 0.4024, 0.7024, 0.4607, 0.0161, 0.2781, 0.3693, 0.7287],\n",
            "          [0.5030, 0.9817, 0.4593, 0.5800, 0.3953, 0.8279, 0.3023, 0.4987],\n",
            "          [0.6932, 0.9768, 0.3193, 0.4808, 0.0211, 0.6093, 0.0547, 0.1612],\n",
            "          [0.2441, 0.4907, 0.2435, 0.3382, 0.9204, 0.4271, 0.1702, 0.3280],\n",
            "          [0.4320, 0.3901, 0.2292, 0.3230, 0.5728, 0.1506, 0.6042, 0.8699]],\n",
            "\n",
            "         [[0.0198, 0.3856, 0.8792, 0.4851, 0.5635, 0.4841, 0.6062, 0.8955],\n",
            "          [0.3890, 0.9672, 0.5498, 0.6164, 0.7041, 0.6676, 0.3767, 0.0888],\n",
            "          [0.8064, 0.9818, 0.3980, 0.2198, 0.2022, 0.4971, 0.6399, 0.9888],\n",
            "          [0.4058, 0.9437, 0.1916, 0.4659, 0.9395, 0.3267, 0.0476, 0.7374],\n",
            "          [0.6736, 0.5231, 0.1456, 0.6605, 0.9653, 0.4320, 0.9229, 0.5563]]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vanilla Transformer Encoder"
      ],
      "metadata": {
        "id": "lnbN5rUYocvE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VanillaTransformer encoder\n",
        "\n",
        "\"\"\"\n",
        "https://arxiv.org/pdf/1706.03762.pdf\n",
        "\"\"\"\n",
        "\n",
        "class VanillaTransformerEncoder(nn.Module):\n",
        "    def __init__(self, d_model, n_heads = 16, dropout = 0.2):\n",
        "        super(VanillaTransformerEncoder, self).__init__()\n",
        "\n",
        "        self.log = setup_log(self, LOG_LEVEL)\n",
        "        \n",
        "        self.mha = MultiHeadAttention(d_model, n_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model) # maybe batch normalization\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "\n",
        "        self.pffn = PositionWiseFeedForwardNetwork(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # new variable because of residual connection\n",
        "        z = self.mha(x,x,x)\n",
        "        z = self.dropout1(z)\n",
        "        z = self.norm1(z + x)\n",
        "\n",
        "        # set the new value for the residual connection\n",
        "        x = z\n",
        "        z = self.pffn(z)\n",
        "        z = self.dropout2(z)\n",
        "        return self.norm2(z + x)\n",
        "\n",
        "\"\"\"\n",
        "ref: https://d2l.ai/chapter_attention-mechanisms-and-transformers/multihead-attention.html\n",
        "\"\"\"\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads = 16):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "        self.log = setup_log(self, LOG_LEVEL)\n",
        "\n",
        "        assert d_model % n_heads == 0, \"n_heads must be a multiple of d_model\"\n",
        "\n",
        "        self.n_heads = n_heads\n",
        "        self.d_k = d_model // n_heads\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model, bias = False)\n",
        "        self.W_k = nn.Linear(d_model, d_model, bias = False)\n",
        "        self.W_v = nn.Linear(d_model, d_model, bias = False)\n",
        "        self.W_o = nn.Linear(d_model, d_model, bias = False)\n",
        "\n",
        "    # reshape to compute in parallel the several heads\n",
        "    def reshape_vector(self, x, inverse = False):\n",
        "        \"\"\"\n",
        "        x: [B x N x D] || [B x N x H x DIM]\n",
        "        output: [B x N x H x DIM] || [B x N x D]\n",
        "        \"\"\"\n",
        "        out = None\n",
        "\n",
        "        if not inverse:\n",
        "            out = einops.rearrange(x, 'b n (dim h) -> b h n dim', h=self.n_heads)\n",
        "        else:\n",
        "            out = einops.rearrange(x, 'b h n dim -> b n (dim h)')\n",
        "\n",
        "        return out\n",
        "\n",
        "    \"\"\"\n",
        "    ref: https://machinelearningmastery.com/the-transformer-attention-mechanism/\n",
        "    \"\"\"\n",
        "\n",
        "    def scaled_attention(self, q, k, v, dk):\n",
        "        \"\"\"\n",
        "        q: [B x H x N x DIM], k: [B x H x N x DIM] , v: [B x H x N x DIM]\n",
        "        output: [B x H x N x DIM]\n",
        "        \"\"\"\n",
        "        sqrt_d_k = math.sqrt(dk)\n",
        "\n",
        "        # using einsum to perform batch matrix multiplication\n",
        "        score = einops.einsum(q, k, 'b h n d_k, b h n_1 d_k -> b h n n_1') / sqrt_d_k\n",
        "\n",
        "        weights = F.softmax(score, dim = -1)\n",
        "\n",
        "        res = einops.einsum(weights, v, 'b h n n_1, b h n_1 d_k -> b h n d_k')\n",
        "\n",
        "        return res\n",
        "\n",
        "    def forward(self, q, k, v):\n",
        "        \"\"\"\n",
        "        q, k, v: [B x N x D]\n",
        "        output: [B x N x D]\n",
        "        \"\"\"\n",
        "        q = self.reshape_vector(self.W_q(q))\n",
        "        k = self.reshape_vector(self.W_k(k))\n",
        "        v = self.reshape_vector(self.W_v(v))\n",
        "\n",
        "        # parallel computation\n",
        "        out = self.scaled_attention(q, k, v, self.d_k)\n",
        "        out_concat = self.reshape_vector(out, inverse = True)\n",
        "\n",
        "        return self.W_o(out_concat)\n",
        "\n",
        "class PositionWiseFeedForwardNetwork(nn.Module):\n",
        "    def __init__(self, d_model, d_inner = 256):\n",
        "        super(PositionWiseFeedForwardNetwork, self).__init__()\n",
        "\n",
        "        self.log = setup_log(self, LOG_LEVEL)\n",
        "        \n",
        "        self.W_1 = nn.Linear(d_model, d_inner)\n",
        "        self.act = nn.GELU()\n",
        "        self.W_2 = nn.Linear(d_inner, d_model)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.W_1(x)\n",
        "        x = self.act(x)\n",
        "        return self.W_2(x)"
      ],
      "metadata": {
        "id": "f7JI-qgOpWbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cost"
      ],
      "metadata": {
        "id": "DTlGPLvHrJ5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Cost\n",
        "\n",
        "class Cost(nn.Module):\n",
        "    def __init__(self, patch_num, batch_size, d_model = 128, d_s = 64, d_t = 64, n_layers = 3, n_heads = 16, dropout = 0.2):\n",
        "        super(Cost, self).__init__()\n",
        "\n",
        "        self.log = setup_log(self, LOG_LEVEL)\n",
        "\n",
        "        # Dropout for seasonal representation output\n",
        "        self.seasonal_drop = nn.Dropout(0.1)\n",
        "\n",
        "        # Trend Feature Disentangler\n",
        "        self.tfd = TrendFeatureDisentangler(d_model, d_t, patch_num)\n",
        "\n",
        "        # Seasonal Feature Disentangler\n",
        "        self.sfd = SeasonalFeatureDisentangler(d_model, d_s, patch_num)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: [(B M) x N x D]\n",
        "        outputs: {[(B M) x N x d_t], [(B M) x N x d_s]}\n",
        "        \"\"\"\n",
        "        out_tfd = self.tfd(x)\n",
        "\n",
        "        out_svd = self.sfd(x)\n",
        "\n",
        "        out_svd = self.seasonal_drop(out_svd) \n",
        "\n",
        "        return out_tfd, out_svd \n",
        "\n",
        "     \n",
        "\n",
        "# Causal Convolution (dilated)\n",
        "\n",
        "class CausalConv1d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, dilation=1):\n",
        "        super(CausalConv1d, self).__init__()\n",
        "\n",
        "        self.log = setup_log(self, LOG_LEVEL)\n",
        "\n",
        "        self.kernel_size = kernel_size\n",
        "        pad = (kernel_size - 1) * dilation\n",
        "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, padding=pad, dilation=dilation)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        input: [(B M) x N x i_C]\n",
        "        output: [(B M) x N_out x o_C]\n",
        "        \"\"\"\n",
        "        # we need to reshape before applying the convolution\n",
        "        x = einops.rearrange(x, 'b n i_c -> b i_c n')\n",
        "        x = self.conv(x)\n",
        "\n",
        "        # we need to remove the trailing padding zeros (except for the fist layer) from the values\n",
        "        if self.kernel_size > 1:\n",
        "            x = x[...,0:-(self.kernel_size-1)]\n",
        "\n",
        "        # rearrange to the original shape\n",
        "        x = einops.rearrange(x, 'b o_c n -> b n o_c')\n",
        "\n",
        "        return x\n",
        "\n",
        "# TFD\n",
        "\n",
        "class TrendFeatureDisentangler(nn.Module):\n",
        "    def __init__(self, d_model, d_t, patch_num):\n",
        "        super(TrendFeatureDisentangler, self).__init__()\n",
        "\n",
        "        self.log = setup_log(self, LOG_LEVEL)\n",
        "        self.d_model = d_model\n",
        "        self.d_t = d_t\n",
        "        self.patch_num = patch_num\n",
        "        \n",
        "        # https://discuss.pytorch.org/t/causal-convolution/3456/3\n",
        "        # https://arxiv.org/pdf/1609.03499v2.pdf\n",
        "\n",
        "        # floor(log(N/2)) autoregressive expert\n",
        "        self.conv_num = math.floor(math.log2(patch_num / 2)) + 1\n",
        "        self.convolutions = nn.ModuleList([CausalConv1d(d_model, d_t, 2**i) for i in range(self.conv_num)])\n",
        "\n",
        "    def avg_pooling(self, input):\n",
        "        \"\"\"\n",
        "        input: [LIST x (B M) x N x d_t]\n",
        "        \"\"\"\n",
        "        return einops.reduce(input, 'list b n d_t -> b n d_t', 'mean')\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: [(B M) x N x D]\n",
        "        output: [(B M) x N x d_t]\n",
        "        \"\"\"\n",
        "        batch_size, patch_num, d_model = x.shape\n",
        "\n",
        "        assert patch_num == self.patch_num and d_model == self.d_model, \"wrong input dimensions\"\n",
        "\n",
        "        # create the result tensor\n",
        "        out = torch.zeros(self.conv_num, batch_size, patch_num, self.d_t, device = x.device)\n",
        "\n",
        "        for i, conv in enumerate(self.convolutions):\n",
        "            out[i,...] = conv(x)\n",
        "\n",
        "        # apply the average pooling operation\n",
        "        out = self.avg_pooling(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "# SVD\n",
        "\n",
        "class SeasonalFeatureDisentangler(nn.Module):\n",
        "    def __init__(self, d_model, d_s, patch_num):\n",
        "        super(SeasonalFeatureDisentangler, self).__init__()\n",
        "\n",
        "        self.log = setup_log(self, LOG_LEVEL)\n",
        "\n",
        "        self.patch_num = patch_num\n",
        "\n",
        "        # number of frequencies after dft\n",
        "        self.f = patch_num // 2 + 1\n",
        "\n",
        "        # discrete fast fourier transform, rfft output contains only the positive frequencies below the Nyquist frequency\n",
        "        self.dft = torch.fft.rfft\n",
        "\n",
        "        # Learnable Fourier Layer\n",
        "        self.fl = FourierLayer(self.f, d_model, d_s, patch_num)\n",
        "\n",
        "        # inverse of discrete fast fourier transform\n",
        "        self.idft = torch.fft.irfft\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: [(B M) x N x D]\n",
        "        output: [(B M) x N x d_s]\n",
        "        \"\"\"\n",
        "        # we apply dft along the temporal dimension\n",
        "        x = self.dft(x, dim = 1)\n",
        "\n",
        "        assert self.f == x.shape[1], \"wrong dimension of dft\"\n",
        "\n",
        "        # apply fourier layer\n",
        "        x = self.fl(x)\n",
        "\n",
        "        # compute the inverse of dft to come back to time domain\n",
        "        x = self.idft(x, n = self.patch_num, dim = 1) # pass also the legth in order to avoid odd-length problems\n",
        "        \n",
        "        return x\n",
        "\n",
        "class FourierLayer(nn.Module):\n",
        "    def __init__(self, f, d_model, d_s, patch_num):\n",
        "        super(FourierLayer, self).__init__()\n",
        "\n",
        "        self.log = setup_log(self, LOG_LEVEL)\n",
        "\n",
        "        self.f = f\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.A = nn.Parameter(torch.rand((f, d_model, d_s), dtype=torch.cfloat))\n",
        "        self.B = nn.Parameter(torch.rand((f, d_s), dtype=torch.cfloat))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: [(B M) x F x D]\n",
        "        out: [(B M) x F x d_s]\n",
        "        \"\"\"\n",
        "        batch_size, f, _ = x.shape\n",
        "        \n",
        "        assert f == self.f, \"wrong dimensions of x\"\n",
        "\n",
        "        out = einops.einsum(self.A, x, 'f d d_s, b f d -> b f d_s') + self.B\n",
        "\n",
        "        return out\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-U9fYJUKrN4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CoPST Encoder"
      ],
      "metadata": {
        "id": "omgfLK9mFicv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %debug\n",
        "class CoPSTEncoder(nn.Module):\n",
        "    def __init__(self, patch_num, num_channels, num_var, patch_len, stride, \n",
        "                 batch_size, d_model = 128, d_s = 64, d_t = 64, n_layers = 3, \n",
        "                 n_heads = 16, dropout = 0.2):\n",
        "        super(CoPSTEncoder, self).__init__()\n",
        "        # self.save_hyperparameters()\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.log = setup_log(self, LOG_LEVEL)\n",
        "\n",
        "        # PatchTST layer (backbone encoder)\n",
        "        self.ptst = PatchTSTEncoder(num_channels, num_var, patch_len, stride, batch_size)\n",
        "        assert patch_num == self.ptst.patch_num, f\"wrong number of patches ({patch_num} != {self.ptst.patch_num})\"\n",
        "        self.patch_num = self.ptst.patch_num\n",
        "\n",
        "        # CoST layer (disentangler)\n",
        "        self.cost = Cost(self.patch_num, batch_size)\n",
        "    \n",
        "    def encode(self, loader, ts_l, device, sliding_length=1, padding=200):\n",
        "        sliding_length = 1\n",
        "        reprs = []\n",
        "\n",
        "        org_training = self.training\n",
        "        self.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = []\n",
        "            for batch in tqdm(loader, desc=\"input encoding\"):\n",
        "                x = batch # [B x L x M]\n",
        "                x = x.to(device)\n",
        "                # self.log.debug(type(x))\n",
        "                # self.log.debug(f\"shape of batch: {x.shape}\")\n",
        "\n",
        "                # if self.batch_size != batch_size:\n",
        "                #     self.log.debug(\"different batch size return same batch\")\n",
        "                #     return batch\n",
        "\n",
        "                for i in range(0, ts_l, sliding_length):\n",
        "                    l = i - padding # sliding_padding=200\n",
        "                    r = i + sliding_length\n",
        "                    # self.log.debug(x.device)\n",
        "                    x_sliding = pad_nan(\n",
        "                        x[:, max(l, 0) : min(r, ts_l)],\n",
        "                        left=-l if l<0 else 0,\n",
        "                        right=r-ts_l if r>ts_l else 0,\n",
        "                        dim=1\n",
        "                    )\n",
        "                    if x_sliding.shape[1] != padding+1:\n",
        "                        self.log.debug(f\"l: {l}, r: {r}, ts_l: {ts_l}\")\n",
        "                    # self.log.debug(x_sliding.device)\n",
        "                    # self.log.debug(f\"shape of x_sliding: {x_sliding.shape}\")\n",
        "                    # self.log.debug(x_sliding.shape)\n",
        "                    reprs.append(self.eval_with_pooling(x_sliding))\n",
        "\n",
        "                out = torch.cat(reprs, dim=1)\n",
        "                self.log.debug(f\"out.shape: {out.shape}\")\n",
        "                output.append(out)\n",
        "\n",
        "            output = torch.cat(output, dim=0)\n",
        "\n",
        "        self.train(org_training)\n",
        "        return output.numpy()\n",
        "\n",
        "    def eval_with_pooling(self, x):\n",
        "        out_t, out_s = self(x)\n",
        "        out = torch.cat([out_t[:, -1], out_s[:, -1]], dim=-1)\n",
        "        return einops.rearrange(out.cpu(), 'b d -> b () d')\n",
        "\n",
        "           \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: [B x L x M] // [B x MAX_TRAIN_LENGTH x TIME_DIM]\n",
        "        outputs: {[(B M) x N x d_t], [(B M) x N x d_s]}\n",
        "        \"\"\"\n",
        "        nan_mask = ~x.isnan().any(axis=-1)\n",
        "        x[~nan_mask] = 0\n",
        "\n",
        "        x = self.ptst(x)\n",
        "\n",
        "        x = self.cost(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "uSr7enmSFptR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CoPST Model"
      ],
      "metadata": {
        "id": "niFGAd70x41j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Constrastive model similar to MoCo\n",
        "\"\"\"\n",
        "https://arxiv.org/pdf/1911.05722.pdf\n",
        "https://github.com/facebookresearch/moco/blob/main/moco/builder.py\n",
        "\"\"\"\n",
        "\n",
        "class CoPSTModel(pl.LightningModule):\n",
        "    def __init__(self, patch_num, num_channels, look_window, patch_len, stride, batch_size, max_train_length \n",
        "                 comp_dimension = 64, alpha = 5e-4, K = 65536, m = 0.999, T = 0.07,\n",
        "                 lr = 1e-3, om = 0.9, wd = 1e-4):\n",
        "        super(CoPSTModel, self).__init__()\n",
        "        self.save_hyperparameters(ignore=['encoder_q', 'encoder_k'])\n",
        "\n",
        "        self.max_train_length = max_train_length\n",
        "\n",
        "        self.K = K\n",
        "        self.m = m\n",
        "        self.T = T\n",
        "\n",
        "        self.alpha = alpha\n",
        "\n",
        "        self.lr = lr\n",
        "        self.om = om\n",
        "        self.wd = wd\n",
        "\n",
        "        self.encoder_q = CoPSTEncoder(patch_num, num_channels, look_window, patch_len, stride, batch_size)\n",
        "        self.encoder_k = copy.deepcopy(self.encoder_q)\n",
        "\n",
        "        self.patch_num = self.encoder_q.patch_num\n",
        "\n",
        "        # projections head for queries and keyes\n",
        "        self.head_q = nn.Sequential(\n",
        "            nn.Linear(comp_dimension, comp_dimension),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(comp_dimension, comp_dimension)\n",
        "        )\n",
        "        self.head_k = nn.Sequential(\n",
        "            nn.Linear(comp_dimension, comp_dimension),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(comp_dimension, comp_dimension)\n",
        "        )\n",
        "\n",
        "        # initialize the parameters of the keyes encoder and projection head\n",
        "        for param_q, param_k in zip(self.encoder_q.parameters(), self.encoder_k.parameters()):\n",
        "            param_k.data.copy_(param_q.data)\n",
        "            param_k.requires_grad = False # the keyes encoder will be updated by the momentum update\n",
        "        \n",
        "        for param_q, param_k in zip(self.head_q.parameters(), self.head_k.parameters()):\n",
        "            param_k.data.copy_(param_q.data)\n",
        "            param_k.requires_grad = False # the head_k will be updated by the momentum update\n",
        "\n",
        "        # register a dictionary buffer as a queue (decouped from the minibatch size)\n",
        "        # https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_buffer\n",
        "        self.register_buffer('queue', F.normalize(torch.randn(comp_dimension, K), dim=0))\n",
        "        self.register_buffer('queue_ptr', torch.zeros(1, dtype=torch.long))\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # set parameters of SGD\n",
        "        optimizer = torch.optim.SGD(self.parameters(), lr = self.lr, momentum = self.om, weight_decay = self.wd)\n",
        "        # cosine annelling is a wrapper for SGD\n",
        "        cosine_anneling = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer = optimizer, T_max = 100)\n",
        "        return optimizer\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _momentum_update_key_encoder(self):\n",
        "        \"\"\"\n",
        "        Momentum update for key encoder\n",
        "        \"\"\"\n",
        "        for param_q, param_k in zip(self.encoder_q.parameters(), self.encoder_k.parameters()):\n",
        "            param_k.data = param_k.data * self.m + param_q.data * (1 - self.m)\n",
        "\n",
        "        for param_q, param_k in zip(self.head_q.parameters(), self.head_k.parameters()):\n",
        "            param_k.data = param_k.data * self.m + param_q.data * (1 - self.m)\n",
        "\n",
        "    def compute_loss(self, q, k, k_negs):\n",
        "        # compute logits\n",
        "        # positive logits: Bx1 (one timestamp as postive)\n",
        "        l_pos = einops.einsum(q, k, 'b c,b c->b').unsqueeze(-1)\n",
        "        # negative logits: BxK\n",
        "        l_neg = einops.einsum(q, k_negs, 'b c,c k->b k')\n",
        "\n",
        "        # logits: Bx(1+K)\n",
        "        logits = torch.cat([l_pos, l_neg], dim=1)\n",
        "\n",
        "        # apply temperature\n",
        "        logits /= self.T\n",
        "\n",
        "        # labels: positive key indicators - first dim of each batch (it will be considered the positive sample)\n",
        "        # so we can consider this as a classification problem and use the CE\n",
        "        labels = torch.zeros(logits.shape[0], dtype=torch.long, device = logits.device)\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "        return loss\n",
        "    \n",
        "    def get_polar(self, x):\n",
        "        mod = x.abs()\n",
        "        phase = x.angle()\n",
        "\n",
        "        return (mod, phase)\n",
        "\n",
        "    def instance_contrastive_loss(self, z1, z2):\n",
        "        B = z1.shape[0]\n",
        "        z = torch.cat([z1, z2], dim=0)  # 2B x F x d_s\n",
        "        z = einops.rearrange(z, 'b f d_s -> f b d_s')  # F x 2B x d_s\n",
        "        sim = einops.einsum(z, z, 'f b_1 d_s, f b_2 d_s -> f b_1 b_2')  # F x 2B x 2B\n",
        "        logits = torch.tril(sim, diagonal=-1)[:, :, :-1]  # F x 2B x (2B-1)\n",
        "        logits += torch.triu(sim, diagonal=1)[:, :, 1:]\n",
        "        logits = -F.log_softmax(logits, dim=-1)\n",
        "        # log.debug(logits)\n",
        "\n",
        "        i = torch.arange(B)\n",
        "        loss = (logits[:, i, B + i - 1].mean() + logits[:, B + i, i].mean()) / 2\n",
        "        return loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _dequeue_and_enqueue(self, keys):\n",
        "        batch_size = keys.shape[0]\n",
        "\n",
        "        ptr = int(self.queue_ptr)\n",
        "        assert self.K % batch_size == 0, \"K must be a multiple of batch_size\"\n",
        "\n",
        "        # replace keys at ptr (dequeue and enqueue)\n",
        "        self.queue[:, ptr:ptr + batch_size] = keys.T\n",
        "\n",
        "        ptr = (ptr + batch_size) % self.K\n",
        "        self.queue_ptr[0] = ptr\n",
        "    \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x_q, x_k = batch\n",
        "\n",
        "        if self.max_train_length is not None and x_q.size(1) > self.max_train_length:\n",
        "            window_offset = np.random.randint(x_q.size(1) - self.max_train_length + 1)\n",
        "            x_q = x_q[:, window_offset : window_offset + self.max_train_length]\n",
        "            x_k = x_k[:, window_offset : window_offset + self.max_train_length]\n",
        "\n",
        "        loss = self.forward(x_q, x_k)\n",
        "\n",
        "        # logs metrics for each training_step,\n",
        "        # and the average across the epoch, to the progress bar and logger\n",
        "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "\n",
        "    # def validation_step(self, batch, batch_idx):\n",
        "    #     x_q, x_k = batch\n",
        "    #     loss = self.forward(x_q, x_k)\n",
        "\n",
        "    #     # logs metrics for each training_step,\n",
        "    #     # and the average across the epoch, to the progress bar and logger\n",
        "    #     self.log(\"val_loss\", loss)\n",
        "    #     return loss\n",
        "\n",
        "    # def test_step(self, batch, batch_idx):\n",
        "    #     x_q, x_k = batch\n",
        "    #     loss = self.forward(x_q, x_k)\n",
        "\n",
        "    #     # logs metrics for each training_step,\n",
        "    #     # and the average across the epoch, to the progress bar and logger\n",
        "    #     self.log(\"test_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "    #     return loss\n",
        "\n",
        "    def forward(self, x_q, x_k):\n",
        "        \"\"\"\n",
        "        x_q, x_k: [B x L x M] // [B x MAX_TRAIN_LENGTH x TIME_DIM]\n",
        "        \"\"\"\n",
        "        # select a random timestamp\n",
        "        rand_idx = np.random.randint(0, self.patch_num)\n",
        "\n",
        "        # trend and seasonal queries\n",
        "        q_t, q_s = self.encoder_q(x_q)\n",
        "\n",
        "        q_t = F.normalize(self.head_q(q_t[:, rand_idx]), dim=-1)\n",
        "\n",
        "        # compute key features\n",
        "        with torch.no_grad():  # no gradient update for keys (momentum update will be used)\n",
        "            self._momentum_update_key_encoder()  # update key encoder using momentum\n",
        "            k_t, k_s = self.encoder_k(x_k)\n",
        "            k_t = F.normalize(self.head_k(k_t[:, rand_idx]), dim=-1)\n",
        "\n",
        "        loss = 0\n",
        "\n",
        "        loss += self.compute_loss(q_t, k_t, self.queue.clone().detach())\n",
        "        self._dequeue_and_enqueue(k_t)\n",
        "\n",
        "        q_s = F.normalize(q_s, dim=-1)\n",
        "        _, k_s = self.encoder_q(x_k)\n",
        "        k_s = F.normalize(k_s, dim=-1)\n",
        "\n",
        "        # the frequency and phase lost must be computed in the frequency domain\n",
        "        q_s_freq = torch.fft.rfft(q_s, dim=1)\n",
        "        k_s_freq = torch.fft.rfft(k_s, dim=1)\n",
        "        q_s_amp, q_s_phase = self.get_polar(q_s_freq)\n",
        "        k_s_amp, k_s_phase = self.get_polar(k_s_freq)\n",
        "\n",
        "        seasonal_loss = self.instance_contrastive_loss(q_s_amp, k_s_amp) + \\\n",
        "                        self.instance_contrastive_loss(q_s_phase,k_s_phase)\n",
        "        loss += (self.alpha * (seasonal_loss/2))\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "xSBWEMomx4KG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Forecasting Evaluation"
      ],
      "metadata": {
        "id": "UrfWPDRev3PO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.load(datasets_path[ELECTRICITY] + datasets_processed_name[ELECTRICITY])\n",
        "dataset = ElectricityDataset(data, L, True)\n",
        "loader = DataLoader(dataset, batch_size=EVAL_BATCH_SIZE, drop_last = True)\n",
        "log.info(f\"device: {DEVICE}\")\n",
        "encoder = CoPSTEncoder(EVAL_PATCH_NUM, M, MAX_TRAIN_LENGTH, PATCH_LEN, STRIDE, EVAL_BATCH_SIZE).to(DEVICE)\n",
        "\n",
        "\n",
        "# callbacks = [TQDMProgressBar(refresh_rate=20)]\n",
        "# trainer = pl.Trainer(devices=1, accelerator=\"auto\", callbacks=callbacks)"
      ],
      "metadata": {
        "id": "2w0iIA_sx3cF",
        "outputId": "bfd7a176-1827-44ee-8d2e-4d4cd6d0bd0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:APP:device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.encode(loader, window_len=L, device=DEVICE, padding=PADDING)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3l_4jLK2LkrH",
        "outputId": "4971b207-4682-4746-b1f5-3eb6574dc66e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "input encoding:   0%|          | 0/136 [00:00<?, ?it/s]DEBUG:CoPSTEncoder:out.shape: torch.Size([11840, 32, 128])\n",
            "input encoding:   1%|          | 1/136 [00:09<20:25,  9.07s/it]DEBUG:CoPSTEncoder:out.shape: torch.Size([11840, 64, 128])\n",
            "input encoding:   1%|▏         | 2/136 [00:18<20:23,  9.13s/it]DEBUG:CoPSTEncoder:out.shape: torch.Size([11840, 96, 128])\n",
            "input encoding:   2%|▏         | 3/136 [00:27<20:29,  9.24s/it]DEBUG:CoPSTEncoder:out.shape: torch.Size([11840, 128, 128])\n",
            "input encoding:   3%|▎         | 4/136 [00:37<20:35,  9.36s/it]DEBUG:CoPSTEncoder:out.shape: torch.Size([11840, 160, 128])\n",
            "input encoding:   4%|▎         | 5/136 [00:47<20:50,  9.55s/it]DEBUG:CoPSTEncoder:out.shape: torch.Size([11840, 192, 128])\n",
            "input encoding:   4%|▍         | 6/136 [00:57<20:59,  9.69s/it]DEBUG:CoPSTEncoder:out.shape: torch.Size([11840, 224, 128])\n",
            "input encoding:   5%|▌         | 7/136 [01:07<21:15,  9.89s/it]DEBUG:CoPSTEncoder:out.shape: torch.Size([11840, 256, 128])\n",
            "input encoding:   6%|▌         | 8/136 [01:18<21:45, 10.20s/it]DEBUG:CoPSTEncoder:out.shape: torch.Size([11840, 288, 128])\n",
            "input encoding:   7%|▋         | 9/136 [01:30<22:43, 10.73s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pred = trainer.predict(encoder, loader)\n",
        "# if dataset.__len__() % BATCH_SIZE:\n",
        "#     pred = pred[:-2] # drop last batch \n",
        "# pred = torch.cat(pred, dim=0)"
      ],
      "metadata": {
        "id": "pXug9zzMzGWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred"
      ],
      "metadata": {
        "id": "gkJZLSd8mf3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "HKVmJP1d-q7M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper functions"
      ],
      "metadata": {
        "id": "lNnO72b3clMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train function\n",
        "def train(batch_size, datamodule, model, model_name, max_epochs = 500, checkpoint_every_n_epochs = 5, \n",
        "          check_val_every_n_epoch = 5, resume_training = True, load_model = False, \n",
        "          enable_checkpoint = True, monitor_metric = \"val_loss\", checkpoint_dir = None, logs_dir = None, \n",
        "          early_stopping = True, deterministic = False):\n",
        "    \n",
        "    # check monitor metric\n",
        "    assert monitor_metric in [\"train_loss\", \"vall_loss\"], \"metric to monitor is invalid\"\n",
        "    \n",
        "    # initialize callbacks array\n",
        "    callbacks = [TQDMProgressBar(refresh_rate=20)]\n",
        "\n",
        "    # add checkpoints to callbacks\n",
        "    checkpoint_callback = None\n",
        "    if enable_checkpoint and checkpoint_dir is not None:\n",
        "        checkpoint_callback = ModelCheckpoint(dirpath=checkpoint_dir,  monitor = monitor_metric, filename=model_name + '-mnist-{epoch:02d}-{' + monitor_metric + ':.2f}',\n",
        "                                         save_last =True, every_n_epochs = checkpoint_every_n_epochs, save_on_train_epoch_end = True)\n",
        "        callbacks.append(checkpoint_callback)\n",
        "        \n",
        "    # add early stopping to the callbacks\n",
        "    if early_stopping:\n",
        "        callbacks.append(EarlyStopping(monitor=\"val_loss\", min_delta = 0.1, patience = 3, mode=\"min\", check_on_train_epoch_end = False))\n",
        "                              \n",
        "    # define the logger object\n",
        "    logger = None\n",
        "    if logs_dir is not None:\n",
        "        logger = TensorBoardLogger(logs_dir, name=model_name)\n",
        "\n",
        "    # create the Trainer\n",
        "    trainer = pl.Trainer(enable_checkpointing=enable_checkpoint, devices=1, accelerator=\"auto\", \n",
        "                         max_epochs=max_epochs, max_steps = 2, logger=logger, callbacks=callbacks,  ## remove\n",
        "                         check_val_every_n_epoch = check_val_every_n_epoch, detect_anomaly=True, ## remove\n",
        "                         deterministic = deterministic)\n",
        "\n",
        "    ckpt_path = None\n",
        "    if resume_training:\n",
        "        ckpt_path = checkpoint_dir + \"last\"\n",
        "    trainer.fit(ckpt_path = ckpt_path, model=model, datamodule=datamodule)\n",
        "    if checkpoint_callback is not None:\n",
        "        log.info(checkpoint_callback.best_model_path)"
      ],
      "metadata": {
        "id": "q-936er_-snn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main"
      ],
      "metadata": {
        "id": "Oqxoxjh9C0rC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 1\n",
        "while True:    \n",
        "    # initialize dataset\n",
        "    datamodule = ElectricityDataModule(datasets_path[ELECTRICITY] + datasets_processed_name[ELECTRICITY], BATCH_SIZE, L)\n",
        "\n",
        "    # set seed if deterministic\n",
        "    if DETERMINISTIC:\n",
        "        seed_everything(seed)\n",
        "    # initialize model, or load an extisting one\n",
        "    model = CoPSTModel(PATCH_NUM, M, L, PATCH_LEN, STRIDE, BATCH_SIZE, K = BATCH_SIZE * M * DICT_MOMENTUM_SIZE)\n",
        "    if LOAD_MODEL:\n",
        "        model = CoPSTModel.load_from_checkpoint(CHECKPOINT_FOLDER)\n",
        "\n",
        "    train(BATCH_SIZE, datamodule, model, CoPST, max_epochs = 100, check_val_every_n_epoch = None, \n",
        "        resume_training = False, monitor_metric = \"train_loss\", checkpoint_dir = CHECKPOINT_FOLDER, \n",
        "        logs_dir = LOGS_FOLDER, early_stopping = False, deterministic = DETERMINISTIC)\n",
        "    seed += 1"
      ],
      "metadata": {
        "id": "AvftO5iOK9ZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train(BATCH_SIZE, datamodule, model, CoPST, max_epochs = 100, check_val_every_n_epoch = None, \n",
        "#       resume_training = False, monitor_metric = \"train_loss\", checkpoint_dir = CHECKPOINT_FOLDER, \n",
        "#       logs_dir = LOGS_FOLDER, early_stopping = False, deterministic = DETERMINISTIC)"
      ],
      "metadata": {
        "id": "M-K47tMzHvIK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}