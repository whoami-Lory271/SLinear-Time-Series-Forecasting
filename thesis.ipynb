{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "https://github.com/whoami-Lory271/thesis-project/blob/main/thesis.ipynb",
      "authorship_tag": "ABX9TyMXHmlEv4ShrhO8Io+XB6i+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/whoami-Lory271/thesis-project/blob/main/thesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Symbol legend\n",
        "\n",
        "* B: batch size \n",
        "* M: number of channel\n",
        "* P: patch dimension\n",
        "* N: number of patches\n",
        "* L: lookback window\n"
      ],
      "metadata": {
        "id": "7s9odzFFQWyj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installations and imports\n"
      ],
      "metadata": {
        "id": "w7opc0NsjlNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-lightning==2.0.1.post0 --quiet\n",
        "!pip install einops==0.6.1 --quiet"
      ],
      "metadata": {
        "id": "ehQC2AKyci-4",
        "outputId": "a85e4696-d84c-4bab-c356-8df92777dfc2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m718.6/718.6 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.3/269.3 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import logging\n",
        "from google.colab import drive\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import einops\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pytorch_lightning as pl"
      ],
      "metadata": {
        "id": "WuaX4Ts_jqmd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4F86VEC4VtL8",
        "outputId": "8027da60-33dd-4b92-95c1-296efa638a46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create logger\n",
        "log = logging.getLogger('model_application')\n",
        "log.setLevel(logging.DEBUG)\n",
        "\n",
        "# # create console handler and set level to debug\n",
        "# ch = logging.StreamHandler()\n",
        "# ch.setLevel(logging.INFO)\n",
        "\n",
        "# # create formatter\n",
        "# formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# # add formatter to ch\n",
        "# ch.setFormatter(formatter)\n",
        "\n",
        "# # add ch to logger\n",
        "# logger.addHandler(ch)"
      ],
      "metadata": {
        "id": "iRLWiTu4mlx9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 'application' code\n",
        "log.debug('debug message')\n",
        "log.info('info message')\n",
        "# logger.warning('warn message')\n",
        "# logger.error('error message')\n",
        "# logger.critical('critical message')"
      ],
      "metadata": {
        "id": "4HMM8XFyTvfk",
        "outputId": "d8a4c691-6a6d-4718-ac53-4164c653ba83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:model_application:debug message\n",
            "INFO:model_application:info message\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Constants"
      ],
      "metadata": {
        "id": "94xSbfaKkOmq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#paths\n",
        "ELECTRICITY = \"electricity\"\n",
        "ROOT_FOLDER = \"/content/drive/MyDrive/Università/Magistrale/Tesi/code\"\n",
        "\n",
        "#hyperparameters\n",
        "BATCH_SIZE = 16"
      ],
      "metadata": {
        "id": "ySxfaOHQkQ_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "ma655OWbiZ0h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Datasets"
      ],
      "metadata": {
        "id": "GS_gE31AieGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datasets_path = {\n",
        "    ELECTRICITY: ROOT_FOLDER + \"/datasets/electricity\"\n",
        "}\n",
        "\n",
        "datasets_name = {\n",
        "    ELECTRICITY: \"/LD2011_2014.txt\"    \n",
        "}\n",
        "datasets_processed_name = {\n",
        "    ELECTRICITY: \"/electricity.pkl\"\n",
        "}"
      ],
      "metadata": {
        "id": "nX8g0950j36s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Electricity"
      ],
      "metadata": {
        "id": "S84zQ9UjigKj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing**"
      ],
      "metadata": {
        "id": "o-wdZhMWZybk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df = pd.read_csv(datasets_path[ELECTRICITY] + datasets_name[ELECTRICITY], sep = ';')\n",
        "# df.rename(columns={df.columns[0]: 'Date'},inplace=True)\n",
        "# df.to_pickle(datasets_path[ELECTRICITY] + datasets_processed_name[ELECTRICITY])"
      ],
      "metadata": {
        "id": "YsyolJAOjZVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_pickle(datasets_path[ELECTRICITY] + datasets_processed_name[ELECTRICITY])"
      ],
      "metadata": {
        "id": "CqBi00jLX4oZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ElectricityDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        super().__init__()\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ts = self.data.iloc[idx, 1:]\n",
        "        return ts\n",
        "\n",
        "class ElectricityDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, path, batch_size, train_size = 0.6, test_size = 0.4):\n",
        "        super().__init__()\n",
        "        self.path = path\n",
        "        data = pd.read_pickle(path)\n",
        "        self.train_data, self.validate_data ,self.test_data =  np.split(data, [int(train_size*len(data)), int(test_size*len(data))])     \n",
        "\n",
        "    # def prepare_data(self):\n",
        "    #     # download\n",
        "\n",
        "    def setup(self, stage: str):\n",
        "        # Assign train/val datasets for use in dataloaders\n",
        "        if stage == \"fit\":\n",
        "            self.train = ElectricityDataset(self.train_data)\n",
        "            self.validate = ElectricityDataset(self.validate_data)\n",
        "\n",
        "        # Assign test dataset for use in dataloader(s)\n",
        "        if stage == \"test\":\n",
        "            self.test = ElectricityDataset(self.test_data)\n",
        "\n",
        "        # if stage == \"predict\":\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train, batch_size=self.batch_size, drop_last = True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.validation, batch_size=self.batch_size, drop_last = True)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.test, batch_size=self.batch_size, drop_last = True)\n",
        "\n",
        "    # def predict_dataloader(self):\n",
        "        \n"
      ],
      "metadata": {
        "id": "pWDN4NROaOAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "Oq9gmXnKGmVP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PatchTST"
      ],
      "metadata": {
        "id": "QD52xt7xGsd9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randint(20, size = (4,20,2))\n",
        "print(x.shape)\n",
        "print(\"---------------------------------\")\n",
        "tail = x[:,-1:,:]\n",
        "tail = torch.repeat_interleave(tail, 2, dim = 1)\n",
        "x = torch.concatenate((x,tail), axis = 1)\n",
        "print(x.shape)\n",
        "x = x.unfold(dimension=1, size=5, step=2)\n",
        "print(x.shape)"
      ],
      "metadata": {
        "id": "rRpCGPiqTtaE",
        "outputId": "e8ba5094-9d01-4271-ccae-6b483546d00a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 20, 2])\n",
            "---------------------------------\n",
            "torch.Size([4, 22, 2])\n",
            "torch.Size([4, 9, 2, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "log.debug(\"my term\")\n",
        "div_term = torch.pow(10000.0, torch.arange(0, 16, 2) / 16) \n",
        "print(div_term)\n",
        "log.debug(\"other term\")\n",
        "div_term = torch.exp(torch.arange(0, 16, 2) * -(math.log(10000.0) / 16))\n",
        "print(div_term)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEHXaMcFaX2y",
        "outputId": "5fc559d7-1994-4198-8a85-0f8c41a2eb03"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:model_application:my term\n",
            "DEBUG:model_application:other term\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.0000e+00, 3.1623e+00, 1.0000e+01, 3.1623e+01, 1.0000e+02, 3.1623e+02,\n",
            "        1.0000e+03, 3.1623e+03])\n",
            "tensor([1.0000e+00, 3.1623e-01, 1.0000e-01, 3.1623e-02, 1.0000e-02, 3.1623e-03,\n",
            "        1.0000e-03, 3.1623e-04])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility functions\n",
        "\n",
        "def create_patches(xb, patch_len, stride):\n",
        "    \"\"\"\n",
        "    xb -> [B x L x M]\n",
        "    output -> [B x N x M x P], N\n",
        "    \"\"\"\n",
        "    batch_size, num_channels, num_var = xb.shape\n",
        "    # compute number of patches\n",
        "    patch_num = max(patch_len, num_var)-patch_len // stride + 2\n",
        "\n",
        "    # we repeat the last variable of the sequence to have equal patches\n",
        "    tail = torch.repeat_interleave(x[:,-1:,:], stride, dim = 1)\n",
        "    xb = torch.concatenate((xb, tail), axis = 1)\n",
        "\n",
        "    # create patches\n",
        "    xb = xb.unfold(dimension=1, size=patch_len, step=stride)  \n",
        "\n",
        "    return xb, patch_num\n",
        "\n",
        "\"\"\"\n",
        "ref: https://kazemnejad.com/blog/transformer_architecture_positional_encoding/\n",
        "\"\"\"\n",
        "\n",
        "def positional_encoding(patch_num, d_model):\n",
        "    \"\"\"\n",
        "    output -> [N x D]\n",
        "    \"\"\"\n",
        "    pe = torch.zeros(patch_num, d_model)\n",
        "    # create a positional array\n",
        "    position = torch.arange(0, patch_num).unsqueeze(1)\n",
        "    # div term for half of positions\n",
        "    div_term = torch.pow(10000.0, torch.arange(0, d_model, 2) / d_model) \n",
        "    # even positions\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    # odd positions\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "    # if normalize:\n",
        "    #     pe = pe - pe.mean()\n",
        "    #     pe = pe / (pe.std() * 10)\n",
        "    \n",
        "    return nn.parameter.Parameter(pe, requires_grad= False)"
      ],
      "metadata": {
        "id": "hbRO5NzrNy8b"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PatchTST\n",
        "\n",
        "class PatchTSTEncoder(nn.Module):\n",
        "    def __init__(self, batch_size, num_channels, patch_num, patch_len, stride, d_model = 128, n_layers = 3, n_heads = 16, dropout = 0.2):\n",
        "        super().__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.num_channels = num_channels\n",
        "        self.patch_num = patch_num\n",
        "        self.patch_len = patch_len\n",
        "        self.stride = stride\n",
        "        self.d_model = d_model\n",
        "        self.n_layers = n_layers\n",
        "        self.n_heads = n_heads\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # instance normalization\n",
        "        \"\"\"\n",
        "        ref: https://wandb.ai/wandb_fc/Normalization-Series/reports/Instance-Normalization-in-PyTorch-With-Examples---VmlldzoxNDIyNTQx\n",
        "        \"\"\"\n",
        "        self.inst_norm = nn.InstanceNorm1d(num_channels)\n",
        "\n",
        "        # patch creation\n",
        "        self.create_patch = create_patches\n",
        "\n",
        "        # embedding\n",
        "        self.W_p = nn.Linear(patch_len, d_model)\n",
        "\n",
        "        # positional encoding\n",
        "        self.W_pos = positional_encoding(patch_len, d_model)\n",
        "\n",
        "        # dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # encoder\n",
        "        self.encoders = nn.ModuleList([VanillaTransformerEncoder(d_model) for _ in range(n_layers)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x -> [B x L x M]\n",
        "        output -> [B x N x M x P]\n",
        "        \"\"\"\n",
        "        # we need to reshape dimensione before apply instance normalization\n",
        "        x = einops.rearrange(self.inst_norm(einops.rearrange(x, 'b l m -> b m l')), 'b m l -> b l m')\n",
        "\n",
        "        # create patches\n",
        "        x, patch_num = self.create_patch(x, self.patch_len, self.stride)\n",
        "\n",
        "        assert patch_num == self.patch_num, \"wrong number of computed patches\"\n",
        "\n",
        "        # reshape the tensor from [B x M x P x N] -> [(B M) x P x N]\n",
        "        x = einops.rearrange(x, 'b m p n -> (b m) p n')\n",
        "        # now it can be provided to our transformer implementation\n",
        "\n",
        "        # project into transformer latent space\n",
        "        x = self.W_p(x) + self.W_pos\n",
        "\n",
        "        x = self.encoders(x)\n",
        "\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "bhiJYUZcGvr_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VanillaTransformer encoder\n",
        "class VanillaTransformerEncoder(nn.Module):\n",
        "    def __init__(self, d_model, n_heads = 16, dropout = 0.2):\n",
        "        super.__init__()\n",
        "        \n",
        "        self.mha = MultiHeadAttention(d_model, n_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model) # maybe batch normalization\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "\n",
        "        self.pffn = PositionWiseFeedForwardNetwork(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # new variable because of residual connection\n",
        "        z = self.mha(x)\n",
        "        z = self.dropout1(z)\n",
        "        z = self.norm1(z + x)\n",
        "\n",
        "        # set the new value for the residual connection\n",
        "        x = z\n",
        "        z = self.pffn(z)\n",
        "        z = self.dropout3(z)\n",
        "        return self.norm2(z + x)\n",
        "\n",
        "\"\"\"\n",
        "ref: https://d2l.ai/chapter_attention-mechanisms-and-transformers/multihead-attention.html\n",
        "\"\"\"\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads = 16):\n",
        "        super().__init__()\n",
        "\n",
        "        assert d_model % n_heads == 0, \"n_heads must be a multiple of d_model\"\n",
        "\n",
        "        self.n_heads = n_heads\n",
        "        self.d_k = d_model // n_heads\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model, bias = False)\n",
        "        self.W_k = nn.Linear(d_model, d_model, bias = False)\n",
        "        self.W_v = nn.Linear(d_model, d_model, bias = False)\n",
        "        self.W_o = nn.Linear(d_model, d_model, bias = False)\n",
        "\n",
        "    # reshape to compute in parallel the several heads\n",
        "    def reshape_vector(x, inverse = False):\n",
        "        \"\"\"\n",
        "        x: [B x N x D] || [B x N x H x DIM]\n",
        "        output: [B x N x H x DIM] || [B x N x D]\n",
        "        \"\"\"\n",
        "        out = None\n",
        "\n",
        "        if not inverse:\n",
        "            out = einops.rearrange(x, 'b n (dim h) -> b n h dim', h=self.n_heads)\n",
        "        else:\n",
        "            out = einops.rearrange(x, 'b n dim h -> b n d')\n",
        "\n",
        "        return out\n",
        "\n",
        "    \"\"\"\n",
        "    ref: https://machinelearningmastery.com/the-transformer-attention-mechanism/\n",
        "    \"\"\"\n",
        "\n",
        "    def scaled_attention(q, k, v, dk):\n",
        "        \"\"\"\n",
        "        q: [B x P x d_k], k: [B x P x d_k] , v: [B x P x d_v]\n",
        "        output: [B x P x d_v]\n",
        "        \"\"\"\n",
        "        sqrt_d_k = math.sqrt(dk)\n",
        "\n",
        "        # transpose k\n",
        "        k_T = einops.rearrange(k, 'b p d_k -> b d_k p')\n",
        "\n",
        "        score = torch.bmm(q, k_T) / sqrt_d_k\n",
        "        log.debug(score.shape)\n",
        "        weights = F.softmax(score, dim = -1)\n",
        "\n",
        "        return torch.bmm(weights, v)\n",
        "\n",
        "    def forward(self, q, k, v):\n",
        "        \"\"\"\n",
        "        q, k, v: [B x N x D]\n",
        "        output: [B x N x D]\n",
        "        \"\"\"\n",
        "        q = self.reshape_vector(self.W_q(q))\n",
        "        k = self.reshape_vector(self.W_k(k))\n",
        "        v = self.reshape_vector(self.W_v(v))\n",
        "\n",
        "        # parallel computation\n",
        "        out = self.scaled_attention(q, k, v, self.d_k)\n",
        "        out_concat = self.reshape_vector(out, inverse = True)\n",
        "\n",
        "        return self.W_o(out_concat)\n",
        "\n",
        "class PositionWiseFeedForwardNetwork(nn.Module):\n",
        "    def __init__(self, d_model, d_inner = 256):\n",
        "        super().__init__()\n",
        "        self.W_1 = nn.Linear(d_model, d_inner)\n",
        "        self.act = nn.GELU()\n",
        "        self.W_2 = nn.Linear(d_inner, d_model)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.W_1(x)\n",
        "        x = self.act(x)\n",
        "        return self.W_2(x)"
      ],
      "metadata": {
        "id": "f7JI-qgOpWbs"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q = torch.randint(20, size = (5,2,10), dtype=float)\n",
        "k = torch.randint(20, size = (5,2,10), dtype=float)\n",
        "v = torch.randint(20, size = (5,2,10), dtype=float)"
      ],
      "metadata": {
        "id": "nsk_8II50vM0"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}