{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0fae4c6a87734dcd86e6432a445533e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ad86da52b6a04da58a28cee893e3cd3a",
              "IPY_MODEL_d482781d86564d5f87b63c657247ef8a"
            ],
            "layout": "IPY_MODEL_130ca2553bf4432582fa7cecc7fd981e"
          }
        },
        "ad86da52b6a04da58a28cee893e3cd3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50ee33b60bf447a0b27a11532280516e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b46713bd463c40419878109165e29a4a",
            "value": "0.012 MB of 0.012 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "d482781d86564d5f87b63c657247ef8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_add6e9dbe8ee47ab8380882725df8826",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_60f2e311bf424d2dbd1d8f084d327c46",
            "value": 1
          }
        },
        "130ca2553bf4432582fa7cecc7fd981e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50ee33b60bf447a0b27a11532280516e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b46713bd463c40419878109165e29a4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "add6e9dbe8ee47ab8380882725df8826": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60f2e311bf424d2dbd1d8f084d327c46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e20f15880d6d4e4f971b721c911188db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a5a54e5404c3445f9e197516247e129c",
              "IPY_MODEL_47d34426b6844d289cf50dd79c8f771a"
            ],
            "layout": "IPY_MODEL_c6c598dd76ff4e1bb9626740fd89ade6"
          }
        },
        "a5a54e5404c3445f9e197516247e129c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c21c43e782444b9a90a336b320fd040",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5b55df71e1e14ef5b84953f2c94c92fa",
            "value": "Waiting for wandb.init()...\r"
          }
        },
        "47d34426b6844d289cf50dd79c8f771a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76bd7841d6bc41fa8b7bc67a2ea8fcd9",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_54085c19c7044b0799416fa2d1b9218a",
            "value": 1
          }
        },
        "c6c598dd76ff4e1bb9626740fd89ade6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c21c43e782444b9a90a336b320fd040": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b55df71e1e14ef5b84953f2c94c92fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76bd7841d6bc41fa8b7bc67a2ea8fcd9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54085c19c7044b0799416fa2d1b9218a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3668f36def784138adeef5ec51b6a78e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_706de14f79ba492e8634d244762a7b20",
              "IPY_MODEL_f44759d148f846108986f740858b32ae",
              "IPY_MODEL_b006b8c4c91941f5be8eb5fa9a9277de"
            ],
            "layout": "IPY_MODEL_32ba2130e1384ab9bef808d02dea7da0"
          }
        },
        "706de14f79ba492e8634d244762a7b20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2b78466c75c48c8abcc4effe579c8ba",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a0bf7ecfe2eb465bab68ecce50a054f9",
            "value": "Epoch 599: 100%"
          }
        },
        "f44759d148f846108986f740858b32ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb58d691ad464eba9c2f25b2ce1c052e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6f5d5648047242609268c50b13582b9f",
            "value": 1
          }
        },
        "b006b8c4c91941f5be8eb5fa9a9277de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e3ded6670cf425795d2f66131ba7d3f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_78020c19767f4d7e95336e974a63635a",
            "value": " 1/1 [00:03&lt;00:00,  3.96s/it, v_num=2, train_loss_step=2.460, train_loss_epoch=2.460]"
          }
        },
        "32ba2130e1384ab9bef808d02dea7da0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "a2b78466c75c48c8abcc4effe579c8ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0bf7ecfe2eb465bab68ecce50a054f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb58d691ad464eba9c2f25b2ce1c052e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f5d5648047242609268c50b13582b9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8e3ded6670cf425795d2f66131ba7d3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78020c19767f4d7e95336e974a63635a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "46779a05a75d41eda354ab0eb2e41405": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4e33b47b545949b1811cc06bc8ef6f2e",
              "IPY_MODEL_fddf10833e674ea5b9a04f00798d4fe3",
              "IPY_MODEL_c2f4bcb9d4b048ffba565c31733d12e3"
            ],
            "layout": "IPY_MODEL_0a3f68cf791043fca953625ff0da05f2"
          }
        },
        "4e33b47b545949b1811cc06bc8ef6f2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a1348be21144324981a5099468c3a07",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_162f253284a943b5bdd5cbac1b92463f",
            "value": "forecasting evaluation:  67%"
          }
        },
        "fddf10833e674ea5b9a04f00798d4fe3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b27b85c871954687911110abc6efe35f",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_51a4164135624cbc8508188a1e4d38e0",
            "value": 2
          }
        },
        "c2f4bcb9d4b048ffba565c31733d12e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d34cb4842764799b52f86bc64799f3d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_86f1c1fc799b49f7ae687bce3a336029",
            "value": " 2/3 [00:28&lt;00:14, 14.84s/it]"
          }
        },
        "0a3f68cf791043fca953625ff0da05f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a1348be21144324981a5099468c3a07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "162f253284a943b5bdd5cbac1b92463f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b27b85c871954687911110abc6efe35f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51a4164135624cbc8508188a1e4d38e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6d34cb4842764799b52f86bc64799f3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86f1c1fc799b49f7ae687bce3a336029": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/whoami-Lory271/thesis-project/blob/main/thesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Symbol legend\n",
        "\n",
        "* B: batch size\n",
        "* L: lookback window (aka input_size)\n"
      ],
      "metadata": {
        "id": "7s9odzFFQWyj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installations and imports\n"
      ],
      "metadata": {
        "id": "w7opc0NsjlNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-lightning==2.0.1.post0 --quiet\n",
        "!pip install einops==0.6.1 --quiet\n",
        "!pip install ipdb --quiet\n",
        "!pip install wandb --quiet\n",
        "# !pip install objsize --quiet"
      ],
      "metadata": {
        "id": "ehQC2AKyci-4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import logging\n",
        "# https://github.com/gotcha/ipdb\n",
        "import ipdb\n",
        "import copy\n",
        "from google.colab import drive\n",
        "from tqdm.notebook import tqdm\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import TensorDataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "import numpy as np\n",
        "import random\n",
        "# https://theaisummer.com/einsum-attention/\n",
        "import einops\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import Ridge\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning.callbacks.progress import TQDMProgressBar\n",
        "from pytorch_lightning.callbacks import EarlyStopping\n",
        "from pytorch_lightning.profilers import PyTorchProfiler\n",
        "\n",
        "import wandb\n",
        "import sys\n",
        "import pickle\n",
        "import time\n",
        "import os\n",
        "# import objsize"
      ],
      "metadata": {
        "id": "WuaX4Ts_jqmd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "!cp /content/drive/MyDrive/Tesi/code/.netrc /root/\n",
        "wandb.login()\n",
        "# 024a3906e525e6d2640af94c364128bb3d33e44b"
      ],
      "metadata": {
        "id": "4F86VEC4VtL8",
        "outputId": "7d157222-c6b0-48ee-b2f3-19650852d89c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdesantis-1849114\u001b[0m (\u001b[33mdesantis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Functions"
      ],
      "metadata": {
        "id": "J2UVU6VqgizJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# setup logger function\n",
        "def setup_log(self, level):\n",
        "    log = logging.getLogger(self.__class__.__name__)\n",
        "    log.setLevel(level)\n",
        "    return log"
      ],
      "metadata": {
        "id": "gkBTe3nko46m"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write log on a file\n",
        "def write_log(a):\n",
        "    with open(\"log.txt\", 'w') as file:\n",
        "        for row in a:\n",
        "            file.write(str(row))\n",
        "        log.debug(\"object logged\")"
      ],
      "metadata": {
        "id": "i8LE_3TogiZn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_patch_num(patch_len, num_var, stride):\n",
        "    return (max(patch_len, num_var)-patch_len) // stride + 2"
      ],
      "metadata": {
        "id": "Ke58BUkLo-dd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pad array with nan\n",
        "def pad_nan(arr, left=0, right=0, dim=0):\n",
        "    # padding the right side\n",
        "    if left > 0:\n",
        "        padshape = list(arr.shape)\n",
        "        padshape[dim] = left\n",
        "        arr = torch.cat((torch.full(padshape, np.nan).to(arr.device), arr), dim=dim)\n",
        "\n",
        "    # padding the left side\n",
        "    if right > 0:\n",
        "        padshape = list(arr.shape)\n",
        "        padshape[dim] = right\n",
        "        arr = torch.cat((arr, torch.full(padshape, np.nan)).to(arr.device), dim=dim)\n",
        "    return arr"
      ],
      "metadata": {
        "id": "T_LjdLhhiyGm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split with nan\n",
        "def split_with_nan(x, sections, axis=0):\n",
        "    assert x.dtype in [np.float16, np.float32, np.float64]\n",
        "    arrs = np.array_split(x, sections, axis=axis)\n",
        "    target_length = arrs[0].shape[axis]\n",
        "    for i in range(len(arrs)):\n",
        "        arrs[i] = pad_nan_to_target(arrs[i], target_length, axis=axis)\n",
        "    return arrs"
      ],
      "metadata": {
        "id": "8Z2FotiSArtJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pad with nan\n",
        "def pad_nan_to_target(array, target_length, axis=0):\n",
        "    assert array.dtype in [np.float16, np.float32, np.float64]\n",
        "    pad_size = target_length - array.shape[axis]\n",
        "    if pad_size <= 0:\n",
        "        return array\n",
        "    npad = [(0, 0)] * array.ndim\n",
        "    npad[axis] = (0, pad_size)\n",
        "\n",
        "    return np.pad(array, pad_width=npad, mode='constant', constant_values=np.nan)"
      ],
      "metadata": {
        "id": "1SAjsR_-A3RB"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def centerize_vary_length_series(x):\n",
        "    prefix_zeros = np.argmax(~np.isnan(x).all(axis=-1), axis=1)\n",
        "    suffix_zeros = np.argmax(~np.isnan(x[:, ::-1]).all(axis=-1), axis=1)\n",
        "    offset = (prefix_zeros + suffix_zeros) // 2 - prefix_zeros\n",
        "    rows, column_indices = np.ogrid[:x.shape[0], :x.shape[1]]\n",
        "    offset[offset < 0] += x.shape[1]\n",
        "    column_indices = column_indices - offset[:, np.newaxis]\n",
        "\n",
        "    return x[rows, column_indices]"
      ],
      "metadata": {
        "id": "eK2M1qV3JBdl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# random seed functions\n",
        "def seed_everything(seed):\n",
        "    pl.seed_everything(seed, workers=True)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "mzBQAeoIi8l2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pickle helper\n",
        "def pkl_save(name, var):\n",
        "    os.makedirs(os.path.dirname(name), exist_ok=True)\n",
        "    with open(name, 'wb') as f:\n",
        "        pickle.dump(var, f)\n",
        "\n",
        "def pkl_load(name):\n",
        "    with open(name, 'rb') as f:\n",
        "        return pickle.load(f)"
      ],
      "metadata": {
        "id": "NxBH76S1gdKm"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Constants"
      ],
      "metadata": {
        "id": "94xSbfaKkOmq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# logger\n",
        "LOG_LEVEL = logging.DEBUG\n",
        "\n",
        "# datasets name\n",
        "ELECTRICITY = \"electricity\"\n",
        "M5 = \"M5\"\n",
        "ETTh1 = \"ETTh1\"\n",
        "ETTh2 = \"ETTh2\"\n",
        "WEATHER = \"WTH\"\n",
        "\n",
        "\n",
        "# models\n",
        "CoSpy = \"CoSpy\"\n",
        "\n",
        "MODEL = CoSpy\n",
        "\n",
        "# device\n",
        "\n",
        "DEVICE = torch.device('cpu')\n",
        "\n",
        "if torch.cuda.is_available:\n",
        "    DEVICE = torch.device('cuda')\n",
        "\n",
        "#hyperparameters\n",
        "\n",
        "DATASET = ELECTRICITY\n",
        "\n",
        "# Train\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 600\n",
        "L = 201\n",
        "UNIVARIATE = True\n",
        "\n",
        "# Eval\n",
        "MAX_TRAIN_LENGTH = 201\n",
        "PADDING = MAX_TRAIN_LENGTH-1\n",
        "ENCODE_BATCH_SIZE = 256\n",
        "\n",
        "\n",
        "# training\n",
        "TRAIN = True\n",
        "DETERMINISTIC = True\n",
        "LOAD_MODEL = False\n",
        "RESUME_TRAINING = False\n",
        "MEMORY_PROFILING = False\n",
        "LOAD_ENCODE = False\n",
        "\n",
        "# wandb\n",
        "\n",
        "RUN_ID = \"RUN_ID\"\n",
        "RESUME_RUN = False\n",
        "RUN_ID = wandb.util.generate_id() if not RESUME_RUN else RUN_ID\n",
        "print(f\"current RUN_ID is {RUN_ID}\")\n",
        "\n",
        "config = dict(\n",
        "    epochs= EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    dataset=DATASET,\n",
        "    architecture=MODEL,\n",
        "    run_id = RUN_ID)\n",
        "\n",
        "\n",
        "#paths\n",
        "ROOT_FOLDER = \"/content/drive/MyDrive/Tesi/code\"\n",
        "MODEL_FOLDER = ROOT_FOLDER + \"/models\"\n",
        "CHECKPOINT_FOLDER = ROOT_FOLDER + \"/checkpoints\" + \"/\" + MODEL\n",
        "LOGS_FOLDER = ROOT_FOLDER + \"/logs\"\n",
        "ENCODING_FOLDER = ROOT_FOLDER + \"/encoding/\" + MODEL\n",
        "FORECASTING_RESULT = ROOT_FOLDER + \"/forecasting_result/\" + MODEL"
      ],
      "metadata": {
        "id": "ySxfaOHQkQ_B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecd45c7d-7abc-4a9f-a9fa-3bcf25946c3b"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "current RUN_ID is eff5e1y4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datasets_path = {\n",
        "    ELECTRICITY: ROOT_FOLDER + \"/datasets/electricity\"\n",
        "}\n",
        "\n",
        "datasets_name = {\n",
        "    ELECTRICITY: \"/LD2011_2014.txt\"\n",
        "}\n",
        "datasets_processed_name = {\n",
        "    ELECTRICITY: \"/electricity.csv\" #\"/electricity.npy\"\n",
        "}\n",
        "\n",
        "datasets_pred_lens = {\n",
        "    ELECTRICITY: [24, 48], # [24, 48, 168, 336, 720]\n",
        "    ETTh1: [24, 48],\n",
        "    ETTh2: [24, 48],\n",
        "    WEATHER: [24, 48],\n",
        "    M5: [28]\n",
        "}"
      ],
      "metadata": {
        "id": "XgPwzK8WeJ7D"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WanDB"
      ],
      "metadata": {
        "id": "WXYPOAZ8O2zL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# start a new experiment\n",
        "run = wandb.init(project=config[\"architecture\"], config = config, id = config[\"run_id\"], resume = 'allow')\n",
        "ENCODING_FOLDER += \"/\" + run.name\n",
        "FORECASTING_RESULT += \"/\" + run.name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336,
          "referenced_widgets": [
            "0fae4c6a87734dcd86e6432a445533e6",
            "ad86da52b6a04da58a28cee893e3cd3a",
            "d482781d86564d5f87b63c657247ef8a",
            "130ca2553bf4432582fa7cecc7fd981e",
            "50ee33b60bf447a0b27a11532280516e",
            "b46713bd463c40419878109165e29a4a",
            "add6e9dbe8ee47ab8380882725df8826",
            "60f2e311bf424d2dbd1d8f084d327c46",
            "e20f15880d6d4e4f971b721c911188db",
            "a5a54e5404c3445f9e197516247e129c",
            "47d34426b6844d289cf50dd79c8f771a",
            "c6c598dd76ff4e1bb9626740fd89ade6",
            "9c21c43e782444b9a90a336b320fd040",
            "5b55df71e1e14ef5b84953f2c94c92fa",
            "76bd7841d6bc41fa8b7bc67a2ea8fcd9",
            "54085c19c7044b0799416fa2d1b9218a"
          ]
        },
        "id": "sC_id8hdO5i_",
        "outputId": "e886125a-0938-41e9-8118-f0f51200ed05"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:q4iaooe6) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, maxâ€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0fae4c6a87734dcd86e6432a445533e6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">summer-violet-50</strong> at: <a href='https://wandb.ai/desantis/CoSpy/runs/q4iaooe6' target=\"_blank\">https://wandb.ai/desantis/CoSpy/runs/q4iaooe6</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230814_100257-q4iaooe6/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:q4iaooe6). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01666890603334347, max=1.0)â€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e20f15880d6d4e4f971b721c911188db"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230814_100658-eff5e1y4</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/desantis/CoSpy/runs/eff5e1y4' target=\"_blank\">fresh-planet-51</a></strong> to <a href='https://wandb.ai/desantis/CoSpy' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/desantis/CoSpy' target=\"_blank\">https://wandb.ai/desantis/CoSpy</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/desantis/CoSpy/runs/eff5e1y4' target=\"_blank\">https://wandb.ai/desantis/CoSpy/runs/eff5e1y4</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logger"
      ],
      "metadata": {
        "id": "8j5-8dn5ppGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create logger\n",
        "log = logging.getLogger('APP')\n",
        "log.setLevel(LOG_LEVEL)\n",
        "logging.basicConfig(level=LOG_LEVEL)\n",
        "\n",
        "# # create console handler and set level to debug\n",
        "# ch = logging.StreamHandler()\n",
        "# ch.setLevel(logging.INFO)\n",
        "\n",
        "# # create formatter\n",
        "# formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# # add formatter to ch\n",
        "# ch.setFormatter(formatter)\n",
        "\n",
        "# # add ch to logger\n",
        "# logger.addHandler(ch)"
      ],
      "metadata": {
        "id": "iRLWiTu4mlx9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset and Datamodule"
      ],
      "metadata": {
        "id": "ma655OWbiZ0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, sigma, eval_mode = False,p = 0.5, multiplier = 10):\n",
        "        super().__init__()\n",
        "        self.data = data\n",
        "        self.sigma = sigma\n",
        "        self.eval_mode = eval_mode\n",
        "        self.p = p\n",
        "        self.multiplier = multiplier\n",
        "        self.N, self.T, self.D = data.shape # num_ts, time, dim\n",
        "\n",
        "    def __len__(self):\n",
        "        # return self.data.shape[0] // self.look_window\n",
        "        return self.data.shape[0] * self.multiplier\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ts = self.data[idx % self.N]\n",
        "\n",
        "        return self.transform(ts), self.transform(ts)\n",
        "\n",
        "    def get_len(self):\n",
        "        return self.__len__()\n",
        "\n",
        "    # def get_channels(self):\n",
        "    #     return self.data.iloc[0, 1:].astype(str).str.replace(',', '.').astype('float32').shape[0]\n",
        "\n",
        "    def transform(self, x):\n",
        "        return self.jitter(self.shift(self.scale(x)))\n",
        "\n",
        "    def jitter(self, x):\n",
        "        if random.random() > self.p:\n",
        "            return x\n",
        "        return x + (torch.empty(x.shape).normal_(mean = 0, std = 0.5) * self.sigma)\n",
        "\n",
        "    def scale(self, x):\n",
        "        if random.random() > self.p:\n",
        "            return x\n",
        "        return x * (torch.empty(x.size(-1)).normal_(mean = 0, std = 0.5) * self.sigma + 1)\n",
        "\n",
        "    def shift(self, x):\n",
        "        if random.random() > self.p:\n",
        "            return x\n",
        "        return x + (torch.empty(x.size(-1)).normal_(mean = 0, std = 0.5) * self.sigma)\n",
        "\n",
        "class CustomDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, name, path, batch_size, max_train_length, pred_lens = [24, 48, 96, 288, 672],\n",
        "                 encode_batch_size = 256, train_size = 0.6, test_size = 0.2, univariate = False):\n",
        "        super().__init__()\n",
        "        self.path = path # path to csv file\n",
        "        self.batch_size = batch_size\n",
        "        self.encode_batch_size = encode_batch_size\n",
        "        self.max_train_length = max_train_length\n",
        "        self.pred_lens = pred_lens\n",
        "        self.padding = max_train_length-1\n",
        "        self.train_size = train_size\n",
        "        self.test_size = test_size\n",
        "        self.univariate = univariate\n",
        "        self.name = name\n",
        "        # self.data = np.load(path)\n",
        "\n",
        "    # def prepare_data(self):\n",
        "    #     # download\n",
        "\n",
        "    def _get_time_features(self,dt):\n",
        "        return np.stack([\n",
        "                dt.minute.to_numpy(),\n",
        "                dt.hour.to_numpy(),\n",
        "                dt.dayofweek.to_numpy(),\n",
        "                dt.day.to_numpy(),\n",
        "                dt.dayofyear.to_numpy(),\n",
        "                dt.month.to_numpy(),\n",
        "                dt.weekofyear.to_numpy(),\n",
        "            ], axis=1).astype(float)\n",
        "\n",
        "    def _load_forecast_csv(self, path, name):\n",
        "        data = pd.read_csv(path, index_col='date', parse_dates=True)\n",
        "        dt_embed = self._get_time_features(data.index)\n",
        "        n_covariate_cols = dt_embed.shape[-1]\n",
        "        if self.univariate:\n",
        "            if name in ('ETTh1', 'ETTh2', 'ETTm1', 'ETTm2'):\n",
        "                data = data[['OT']]\n",
        "            elif name == 'electricity':\n",
        "                data = data[['MT_001']]\n",
        "            elif name == 'WTH':\n",
        "                data = data[['WetBulbCelsius']]\n",
        "            else:\n",
        "                data = data.iloc[:, -1:]\n",
        "        data = data.to_numpy()\n",
        "        # compute slices\n",
        "        if name == 'ETTh1' or name == 'ETTh2':\n",
        "            self.train_slice = slice(None, 12 * 30 * 24)\n",
        "            self.valid_slice = slice(12 * 30 * 24, 16 * 30 * 24)\n",
        "            self.test_slice = slice(16 * 30 * 24, 20 * 30 * 24)\n",
        "        elif name == 'ETTm1' or name == 'ETTm2':\n",
        "            self.train_slice = slice(None, 12 * 30 * 24 * 4)\n",
        "            self.valid_slice = slice(12 * 30 * 24 * 4, 16 * 30 * 24 * 4)\n",
        "            self.test_slice = slice(16 * 30 * 24 * 4, 20 * 30 * 24 * 4)\n",
        "        elif name.startswith('M5'):\n",
        "            self.train_slice = slice(None, int(0.8 * (1913 + 28)))\n",
        "            self.valid_slice = slice(int(0.8 * (1913 + 28)), 1913 + 28)\n",
        "            self.test_slice = slice(1913 + 28 - 1, 1913 + 2 * 28)\n",
        "        else:\n",
        "            self.train_slice = slice(None, int(self.train_size * len(data)))\n",
        "            self.valid_slice = slice(int(self.train_size * len(data)), - int(self.test_size * len(data)))\n",
        "            self.test_slice = slice(- int(self.test_size * len(data)), None)\n",
        "\n",
        "        return data, dt_embed, n_covariate_cols\n",
        "\n",
        "    def _scale_and_transform(self, data, dt_embed, n_covariate_cols, name):\n",
        "        # scale data\n",
        "        scaler = StandardScaler().fit(data[self.train_slice])\n",
        "        data = scaler.transform(data)\n",
        "        # NUM_ROWS x NUM_FEATURES -> NUM_FEATURES x NUM_ROWS x 1\n",
        "        if name in ('electricity') or name.startswith('M5'):\n",
        "            data = np.expand_dims(data.T, -1)  # Each variable is an instance rather than a feature\n",
        "        else:\n",
        "            data = np.expand_dims(data, 0)\n",
        "        if n_covariate_cols > 0:\n",
        "            dt_scaler = StandardScaler().fit(dt_embed[self.train_slice])\n",
        "            dt_embed = np.expand_dims(dt_scaler.transform(dt_embed), 0)\n",
        "            data = np.concatenate([np.repeat(dt_embed, data.shape[0], axis=0), data], axis=-1)\n",
        "\n",
        "        return data, scaler, n_covariate_cols\n",
        "\n",
        "    def _fit_setup(self, train_data):\n",
        "        if self.max_train_length is not None:\n",
        "            sections = train_data.shape[1] // self.max_train_length\n",
        "        if sections >= 2:\n",
        "            train_data = np.concatenate(split_with_nan(train_data, sections, axis=1), axis=0)\n",
        "\n",
        "        temporal_missing = np.isnan(train_data).all(axis=-1).any(axis=0)\n",
        "        if temporal_missing[0] or temporal_missing[-1]:\n",
        "            train_data = centerize_vary_length_series(train_data)\n",
        "\n",
        "        train_data = train_data[~np.isnan(train_data).all(axis=2).all(axis=1)]\n",
        "\n",
        "        multiplier = 1 if train_data.shape[0] >= self.batch_size else math.ceil(self.batch_size / train_data.shape[0])\n",
        "\n",
        "        return train_data, multiplier\n",
        "\n",
        "\n",
        "    def setup(self, stage: str):\n",
        "        # load csv\n",
        "        data, dt_embed, n_covariate_cols = self._load_forecast_csv(self.path, self.name)\n",
        "\n",
        "        # scale and transform\n",
        "        self.data, self.scaler, self.n_covariate_cols = self._scale_and_transform(data, dt_embed, n_covariate_cols, self.name)\n",
        "\n",
        "        # Assign train/val datasets for use in dataloaders\n",
        "        if stage == \"fit\":\n",
        "            train_data = self.data[:, self.train_slice]\n",
        "            train_data, multiplier = self._fit_setup(train_data)\n",
        "            # fit setup\n",
        "            self.train = CustomDataset(torch.from_numpy(train_data).to(torch.float), sigma = 0.5, multiplier = multiplier)\n",
        "            # self.validate = ElectricityDataset(data[self.valid_slice], self.look_window)\n",
        "        if stage == 'encoding':\n",
        "            self.encode = TensorDataset(torch.from_numpy(self.data).to(torch.float))\n",
        "\n",
        "\n",
        "\n",
        "        # Assign test dataset for use in dataloader(s)\n",
        "        # if stage == \"test\":\n",
        "        #     self.test = ElectricityDataset(data[self.test_slice], self.look_window)\n",
        "\n",
        "        # if stage == \"predict\":\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train, batch_size=min(self.batch_size, len(self.train)), drop_last = True, shuffle = True)\n",
        "\n",
        "    def encode_dataloader(self):\n",
        "        return DataLoader(self.encode, batch_size=self.encode_batch_size)\n",
        "\n",
        "    # def val_dataloader(self):\n",
        "    #     return DataLoader(self.validate, batch_size=self.batch_size, drop_last = True)\n",
        "\n",
        "    # def test_dataloader(self):\n",
        "    #     return DataLoader(self.test, batch_size=self.batch_size, drop_last = True)\n",
        "\n",
        "    # def predict_dataloader(self):"
      ],
      "metadata": {
        "id": "37YF2vGhSYjl"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "S84zQ9UjigKj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Electricity"
      ],
      "metadata": {
        "id": "o-wdZhMWZybk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df = pd.read_csv(datasets_path[ELECTRICITY] + datasets_name[ELECTRICITY], sep = ';')\n",
        "# df.rename(columns={df.columns[0]: 'Date'},inplace=True)\n",
        "# values = df.values\n",
        "# values = values[:, 1:].astype(str)\n",
        "# for i, value in enumerate(values):\n",
        "#   values[i] = np.char.replace(value, \",\", \".\")\n",
        "# values = values.astype(np.float32)\n",
        "# np.save(datasets_path[ELECTRICITY] + \"/electricity\", values)"
      ],
      "metadata": {
        "id": "YsyolJAOjZVS"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # call this function to initialize the csv file\n",
        "# def electricity_preprocess(path):\n",
        "#     data_ecl = pd.read_csv(path + '/LD2011_2014.txt', parse_dates=True, sep=';', decimal=',', index_col=0)\n",
        "#     data_ecl = data_ecl.resample('1h', closed='right').sum()\n",
        "#     data_ecl = data_ecl.loc[:, data_ecl.cumsum(axis=0).iloc[8920] != 0]  # filter out instances with missing values\n",
        "#     data_ecl.index = data_ecl.index.rename('date')\n",
        "#     data_ecl = data_ecl['2012':]\n",
        "#     data_ecl.to_csv(path + '/electricity.csv')\n",
        "#     log.info(\"electriciy.csv created!\")"
      ],
      "metadata": {
        "id": "HqM-Bf5vWJDg"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# electricity_preprocess(datasets_path[ELECTRICITY])"
      ],
      "metadata": {
        "id": "YS1CZBhuWei-"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# em = ElectricityDataModule(datasets_path[ELECTRICITY] + datasets_processed_name[ELECTRICITY],\n",
        "#                       BATCH_SIZE, 10, MAX_TRAIN_LENGTH)\n",
        "# em.setup(\"fit\")"
      ],
      "metadata": {
        "id": "6YIgF7LxsPVI"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# log.debug(em.train.__getitem__(0)[0].shape)"
      ],
      "metadata": {
        "id": "Mp3eSgVliKLM"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "Oq9gmXnKGmVP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vanilla Transformer Encoder"
      ],
      "metadata": {
        "id": "lnbN5rUYocvE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VanillaTransformer encoder\n",
        "\n",
        "\"\"\"\n",
        "https://arxiv.org/pdf/1706.03762.pdf\n",
        "\"\"\"\n",
        "\n",
        "class VanillaTransformerEncoder(nn.Module):\n",
        "    def __init__(self, d_model, n_heads = 16, dropout = 0.2):\n",
        "        super(VanillaTransformerEncoder, self).__init__()\n",
        "\n",
        "        self.log = setup_log(self, LOG_LEVEL)\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, n_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model) # maybe batch normalization\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "\n",
        "        self.pffn = PositionWiseFeedForwardNetwork(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # new variable because of residual connection\n",
        "        z = self.mha(x,x,x)\n",
        "        z = self.dropout1(z)\n",
        "        z = self.norm1(z + x)\n",
        "\n",
        "        # set the new value for the residual connection\n",
        "        x = z\n",
        "        z = self.pffn(z)\n",
        "        z = self.dropout2(z)\n",
        "        return self.norm2(z + x)\n",
        "\n",
        "\"\"\"\n",
        "ref: https://d2l.ai/chapter_attention-mechanisms-and-transformers/multihead-attention.html\n",
        "\"\"\"\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, d_k, n_head = 6):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "        self.log = setup_log(self, LOG_LEVEL)\n",
        "\n",
        "        self.n_heads = n_head\n",
        "        self.d_k = d_k\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, n_head * d_k, bias = False)\n",
        "        self.W_k = nn.Linear(d_model, n_head * d_k, bias = False)\n",
        "        self.W_v = nn.Linear(d_model, n_head * d_k, bias = False)\n",
        "        self.W_o = nn.Linear(n_head * d_k, d_model, bias = False)\n",
        "\n",
        "    # reshape to compute in parallel the several heads\n",
        "    def reshape_vector(self, x, inverse = False):\n",
        "        \"\"\"\n",
        "        x: (batch, input_size, d_model) || (batch, n_head, input_size, d_k)\n",
        "        output: (batch, n_head, input_size, d_k) || (batch, input_size, d_model)\n",
        "        \"\"\"\n",
        "        out = None\n",
        "\n",
        "        if not inverse:\n",
        "            out = einops.rearrange(x, 'b l (dim h) -> b h l dim', h=self.n_heads)\n",
        "        else:\n",
        "            out = einops.rearrange(x, 'b h l dim -> b l (dim h)')\n",
        "\n",
        "        return out\n",
        "\n",
        "    \"\"\"\n",
        "    ref: https://machinelearningmastery.com/the-transformer-attention-mechanism/\n",
        "    \"\"\"\n",
        "\n",
        "    def scaled_attention(self, q, k, v, dk, mask = None):\n",
        "        \"\"\"\n",
        "        q, k, v: (batch, n_head, input_size, d_k)\n",
        "        output: (batch, n_head, input_size, d_k), (batch, n_head, input_size, input_size)\n",
        "        \"\"\"\n",
        "        sqrt_d_k = math.sqrt(dk)\n",
        "\n",
        "        # using einsum to perform batch matrix multiplication\n",
        "        score = einops.einsum(q, k, 'b h l d_k, b h l_1 d_k -> b h l l_1') / sqrt_d_k\n",
        "\n",
        "        if mask is not None:\n",
        "            score = score.masked_fill(mask, -1e9)\n",
        "\n",
        "        weights = F.softmax(score, dim = -1)\n",
        "        attn = weights\n",
        "\n",
        "        res = einops.einsum(weights, v, 'b h l l_1, b h l_1 d_k -> b h l d_k')\n",
        "\n",
        "        return res, attn\n",
        "\n",
        "    def forward(self, q, k, v, mask = None):\n",
        "        \"\"\"\n",
        "        q, k, v: (batch, input_size, d_model)\n",
        "        output: (batch, input_size, d_model), (batch, n_head, input_size, input_size)\n",
        "        \"\"\"\n",
        "        residual = q\n",
        "\n",
        "        q = self.reshape_vector(self.W_q(q))\n",
        "        k = self.reshape_vector(self.W_k(k))\n",
        "        v = self.reshape_vector(self.W_v(v))\n",
        "\n",
        "        if mask is not None:\n",
        "            if len(mask.size()) == 3:\n",
        "                mask = mask.unsqueeze(1)  # For head axis broadcasting.\n",
        "\n",
        "        # parallel computation\n",
        "        out, attn = self.scaled_attention(q, k, v, self.d_k, mask)\n",
        "        out_concat = self.reshape_vector(out, inverse = True)\n",
        "\n",
        "        return self.W_o(out_concat) + residual, attn\n",
        "\n",
        "class PositionWiseFeedForwardNetwork(nn.Module):\n",
        "    def __init__(self, d_model, d_inner = 512):\n",
        "        super(PositionWiseFeedForwardNetwork, self).__init__()\n",
        "\n",
        "        self.log = setup_log(self, LOG_LEVEL)\n",
        "\n",
        "        self.W_1 = nn.Linear(d_model, d_inner)\n",
        "        self.act = nn.GELU()\n",
        "        self.W_2 = nn.Linear(d_inner, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: (batch, input_size, d_model)\n",
        "        output: (batch, input_size, d_model)\n",
        "        \"\"\"\n",
        "        res = x\n",
        "        x = self.W_1(x)\n",
        "        x = self.act(x)\n",
        "        return res + self.W_2(x)"
      ],
      "metadata": {
        "id": "f7JI-qgOpWbs"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PyraformerEncoder"
      ],
      "metadata": {
        "id": "j4VKdJgKsshh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper Functions"
      ],
      "metadata": {
        "id": "d0TB55l1yKvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mask(input_size, window_size, inner_size, device):\n",
        "    \"\"\"Get the attention mask of PAM-Naive\"\"\"\n",
        "    # Get the size of all layers\n",
        "    all_size = []\n",
        "    all_size.append(input_size)\n",
        "    # we split the nodes in according with the number of children\n",
        "    for i in range(len(window_size)):\n",
        "        layer_size = math.floor(all_size[i] / window_size[i])\n",
        "        all_size.append(layer_size)\n",
        "\n",
        "    # length of the flattened graph\n",
        "    seq_length = sum(all_size)\n",
        "    # mask matrix\n",
        "    mask = torch.zeros(seq_length, seq_length, device=device)\n",
        "\n",
        "    # get intra-scale mask\n",
        "    inner_window = inner_size // 2\n",
        "    for layer_idx in range(len(all_size)):\n",
        "        start = sum(all_size[:layer_idx])\n",
        "        for i in range(start, start + all_size[layer_idx]):\n",
        "            left_side = max(i - inner_window, start)\n",
        "            right_side = min(i + inner_window + 1, start + all_size[layer_idx])\n",
        "            mask[i, left_side:right_side] = 1\n",
        "\n",
        "    # get inter-scale mask\n",
        "    for layer_idx in range(1, len(all_size)):\n",
        "        start = sum(all_size[:layer_idx])\n",
        "        for i in range(start, start + all_size[layer_idx]):\n",
        "            left_side = (start - all_size[layer_idx - 1]) + (i - start) * window_size[layer_idx - 1]\n",
        "            if i == ( start + all_size[layer_idx] - 1):\n",
        "                right_side = start\n",
        "            else:\n",
        "                right_side = (start - all_size[layer_idx - 1]) + (i - start + 1) * window_size[layer_idx - 1]\n",
        "            mask[i, left_side:right_side] = 1\n",
        "            mask[left_side:right_side, i] = 1\n",
        "\n",
        "    mask = (1 - mask).bool()\n",
        "\n",
        "    return mask, all_size\n",
        "\n",
        "def get_graph_dim(input_size, window_size):\n",
        "    \"\"\" get the dimension of the graph computed by CSCM\"\"\"\n",
        "    res = input_size\n",
        "    for w in window_size:\n",
        "        input_size = math.floor(input_size / w)\n",
        "        res += input_size\n",
        "\n",
        "    return res\n",
        "\n"
      ],
      "metadata": {
        "id": "rZXf-0odyPbJ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "d6HSFST8yIkK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Bottleneck_Construct(nn.Module):\n",
        "    \"\"\"Bottleneck convolution CSCM\"\"\"\n",
        "    def __init__(self, d_model, window_size, d_inner):\n",
        "        super(Bottleneck_Construct, self).__init__()\n",
        "        if not isinstance(window_size, list):\n",
        "            self.conv_layers = nn.ModuleList([\n",
        "                ConvLayer(d_inner, window_size),\n",
        "                ConvLayer(d_inner, window_size),\n",
        "                ConvLayer(d_inner, window_size)\n",
        "                ])\n",
        "        else:\n",
        "            self.conv_layers = []\n",
        "            for i in range(len(window_size)):\n",
        "                self.conv_layers.append(ConvLayer(d_inner, window_size[i]))\n",
        "            self.conv_layers = nn.ModuleList(self.conv_layers)\n",
        "        self.up = nn.Linear(d_inner, d_model)\n",
        "        self.down = nn.Linear(d_model, d_inner)\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, enc_input):\n",
        "\n",
        "        temp_input = self.down(enc_input).permute(0, 2, 1)\n",
        "        all_inputs = []\n",
        "        for i in range(len(self.conv_layers)):\n",
        "            temp_input = self.conv_layers[i](temp_input)\n",
        "            all_inputs.append(temp_input)\n",
        "\n",
        "        all_inputs = torch.cat(all_inputs, dim=2).transpose(1, 2)\n",
        "        all_inputs = self.up(all_inputs)\n",
        "        all_inputs = torch.cat([enc_input, all_inputs], dim=1)\n",
        "\n",
        "        all_inputs = self.norm(all_inputs)\n",
        "\n",
        "        return all_inputs\n",
        "\"\"\" For Electricity Dataset\"\"\"\n",
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEmbedding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        # create a positional array\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        # div term for half of positions\n",
        "        div_term = torch.pow(10000.0, torch.arange(0, d_model, 2) / d_model)\n",
        "        # even positions\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        # odd positions\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "        # if normalize:\n",
        "        #     pe = pe - pe.mean()\n",
        "        #     pe = pe / (pe.std() * 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        output: (1, input_size, d_model)\n",
        "        \"\"\"\n",
        "        return self.pe[:, :x.size(1)]\n",
        "\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, c_in, d_model):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        padding = 1 if torch.__version__>='1.5.0' else 2\n",
        "        self.tokenConv = nn.Conv1d(in_channels=c_in, out_channels=d_model,\n",
        "                                    kernel_size=3, padding=padding, padding_mode='circular')\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv1d):\n",
        "                nn.init.kaiming_normal_(m.weight,mode='fan_in',nonlinearity='leaky_relu')\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: (batch, input_size, enc_in)\n",
        "        output: (batch, input_size, d_model)\n",
        "        \"\"\"\n",
        "        x = self.tokenConv(einops.rearrange(x, 'b l e -> b e l')).transpose(1,2)\n",
        "        return x\n",
        "\n",
        "class CustomEmbedding(nn.Module):\n",
        "    def __init__(self, c_in, d_model, temporal_size, seq_num, dropout=0.1):\n",
        "        super(CustomEmbedding, self).__init__()\n",
        "\n",
        "        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n",
        "        self.position_embedding = PositionalEmbedding(d_model=d_model)\n",
        "        self.temporal_embedding = nn.Linear(temporal_size, d_model)\n",
        "        self.seqid_embedding = nn.Embedding(seq_num, d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, x, x_mark):\n",
        "        \"\"\"\n",
        "        x: (batch, input_size, enc_in)\n",
        "        x_mark: (batch, input_size)\n",
        "        output: (batch, input_size, d_model)\n",
        "        \"\"\"\n",
        "        x = self.value_embedding(x) + self.position_embedding(x) + self.temporal_embedding(x_mark[:, :, :-1]) \\\n",
        "            + self.seqid_embedding(x_mark[:, :, -1].long())\n",
        "\n",
        "\n",
        "\n",
        "        return self.dropout(x)\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    \"\"\" Compose with two layers \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, d_inner, d_k, n_head, dropout=0.1, normalize_before=True, q_k_mask=None, k_q_mask=None):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.slf_attn = MultiHeadAttention(d_model, d_k, n_head)\n",
        "\n",
        "        self.pos_ffn = PositionWiseFeedForwardNetwork(\n",
        "            d_model, d_inner)\n",
        "\n",
        "    def forward(self, enc_input, slf_attn_mask=None):\n",
        "        \"\"\"\n",
        "        enc_input: (batch, input_size, d_model)\n",
        "        output: (batch, input_size, d_model), (batch, n_head, input_size, input_size)\n",
        "        \"\"\"\n",
        "        enc_output, enc_slf_attn = self.slf_attn(enc_input, enc_input, enc_input, mask=slf_attn_mask)\n",
        "\n",
        "        enc_output = self.pos_ffn(enc_output)\n",
        "\n",
        "        return enc_output, enc_slf_attn\n",
        "\n",
        "class ConvLayer(nn.Module):\n",
        "    def __init__(self, c_in, window_size):\n",
        "        super(ConvLayer, self).__init__()\n",
        "        self.downConv = nn.Conv1d(in_channels=c_in,\n",
        "                                  out_channels=c_in,\n",
        "                                  kernel_size=window_size,\n",
        "                                  stride=window_size)\n",
        "        self.norm = nn.BatchNorm1d(c_in)\n",
        "        self.activation = nn.ELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.downConv(x)\n",
        "        x = self.norm(x)\n",
        "        x = self.activation(x)\n",
        "        return x\n",
        "\n",
        "class Bottleneck_Construct(nn.Module):\n",
        "    \"\"\"Bottleneck convolution CSCM\"\"\"\n",
        "    def __init__(self, d_model, window_size, d_inner):\n",
        "        super(Bottleneck_Construct, self).__init__()\n",
        "        if not isinstance(window_size, list):\n",
        "            self.conv_layers = nn.ModuleList([\n",
        "                ConvLayer(d_inner, window_size),\n",
        "                ConvLayer(d_inner, window_size),\n",
        "                ConvLayer(d_inner, window_size)\n",
        "                ])\n",
        "        else:\n",
        "            self.conv_layers = []\n",
        "            for i in range(len(window_size)):\n",
        "                self.conv_layers.append(ConvLayer(d_inner, window_size[i]))\n",
        "            self.conv_layers = nn.ModuleList(self.conv_layers)\n",
        "        self.up = nn.Linear(d_inner, d_model)\n",
        "        self.down = nn.Linear(d_model, d_inner)\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, enc_input):\n",
        "        \"\"\"\n",
        "        enc_input: (batch, input_size, d_model)\n",
        "        output: (batch, graph_size, d_model)\n",
        "        \"\"\"\n",
        "\n",
        "        temp_input = self.down(enc_input).permute(0, 2, 1)\n",
        "        all_inputs = []\n",
        "        for i in range(len(self.conv_layers)):\n",
        "            temp_input = self.conv_layers[i](temp_input)\n",
        "            all_inputs.append(temp_input)\n",
        "\n",
        "        all_inputs = torch.cat(all_inputs, dim=2).transpose(1, 2)\n",
        "        all_inputs = self.up(all_inputs)\n",
        "        # concat the computed new nodes with the input nodes\n",
        "        all_inputs = torch.cat([enc_input, all_inputs], dim=1)\n",
        "\n",
        "        all_inputs = self.norm(all_inputs)\n",
        "\n",
        "        return all_inputs\n",
        "\n",
        "class PyraformerEncoder(nn.Module):\n",
        "    \"\"\" A encoder model with self attention mechanism. \"\"\"\n",
        "\n",
        "    def __init__(self, d_model = 256, d_k = 128, window_size = [4,4,4], inner_size = 3,\n",
        "                 input_size = 201, d_inner_hid = 512, n_head = 6, n_layer = 4,\n",
        "                 # Dataloader parameters\n",
        "                 enc_in = 1, covariate_size = 7, seq_num = 321,\n",
        "                 CSCM = \"Bottleneck_Construct\", d_bottleneck = 128, device = 'cpu'):\n",
        "        super(PyraformerEncoder, self).__init__()\n",
        "        self.log = setup_log(self, LOG_LEVEL)\n",
        "\n",
        "        self.d_model = d_model # size of the latent vector\n",
        "        self.d_k = d_k # size of the inner dimension of q, k, v\n",
        "        self.window_size = window_size # The number of children of a parent node\n",
        "        self.inner_size = inner_size # The number of ajacent nodes\n",
        "        self.input_size = input_size # length of the sequence\n",
        "        self.d_inner_hid = d_inner_hid # inner size of the PostitionalFeedForward\n",
        "        self.n_head = n_head\n",
        "        self.n_layer = n_layer\n",
        "        self.enc_in = enc_in\n",
        "        self.covariate_size = covariate_size # number of temporal covariate\n",
        "        self.seq_num = seq_num # size of the time series\n",
        "        self.CSCM = CSCM # called coarser-scale construction module\n",
        "        self.g_size = get_graph_dim(input_size, window_size)\n",
        "        self.d_bottleneck = d_bottleneck #\n",
        "        self.mask, self.all_size = get_mask(self.input_size, self.window_size, self.inner_size, device)\n",
        "        self.layers = nn.ModuleList([\n",
        "                EncoderLayer(self.d_model, self.d_inner_hid, self.d_k, self.n_head) for i in range(self. n_layer)\n",
        "                ])\n",
        "        self.enc_embedding = nn.Linear(enc_in + covariate_size, d_model)#CustomEmbedding(self.enc_in, self.d_model, self.covariate_size, self.seq_num)\n",
        "\n",
        "        self.conv_layers = eval(self.CSCM)(self.d_model, self.window_size, self.d_bottleneck)\n",
        "\n",
        "        self.fc = nn.Linear(self.g_size, self.input_size)\n",
        "\n",
        "        self.test = nn.Linear(enc_in + covariate_size, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: (batch, input_size, enc_in + covariate_size)\n",
        "        output: (batch, input_size, d_model)\n",
        "        \"\"\"\n",
        "\n",
        "        seq_enc = self.enc_embedding(x)\n",
        "\n",
        "        # Repeat the mask for all the batch\n",
        "        mask = self.mask.repeat(len(seq_enc), 1, 1)\n",
        "\n",
        "        seq_enc = self.conv_layers(seq_enc)\n",
        "\n",
        "        for i in range(len(self.layers)):\n",
        "            seq_enc, _ = self.layers[i](seq_enc, mask)\n",
        "\n",
        "        seq_enc = einops.rearrange(seq_enc, 'b g d -> b d g')\n",
        "\n",
        "        seq_enc = self.fc(seq_enc)\n",
        "\n",
        "        seq_enc = einops.rearrange(seq_enc, 'b d l -> b l d')\n",
        "\n",
        "        return seq_enc"
      ],
      "metadata": {
        "id": "LiOdyvP8syOH"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PatchTST"
      ],
      "metadata": {
        "id": "QD52xt7xGsd9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility functions\n",
        "\n",
        "def create_patches(xb, patch_len, stride):\n",
        "    \"\"\"\n",
        "    xb -> [B x L x M] // [B x L x M x T]\n",
        "    output -> [B x N x M x P] // [B x N x M x T x P], N\n",
        "    \"\"\"\n",
        "    _, num_var, _, _ = xb.shape\n",
        "    # compute number of patches\n",
        "    patch_num = (max(patch_len, num_var)-patch_len) // stride + 2\n",
        "\n",
        "    # we repeat the last variable of the sequence to have equal patches\n",
        "    tail = torch.repeat_interleave(xb[:,-1:,...], stride, dim = 1)\n",
        "    xb = torch.concatenate((xb, tail), axis = 1)\n",
        "\n",
        "    # create patches\n",
        "    xb = xb.unfold(dimension=1, size=patch_len, step=stride)\n",
        "\n",
        "    assert patch_num == xb.shape[1], f\"wrong number of computed patches, expected {patch_num} but computed {xb.shape[1]}\"\n",
        "\n",
        "    return xb, patch_num\n",
        "\n",
        "\"\"\"\n",
        "ref: https://kazemnejad.com/blog/transformer_architecture_positional_encoding/\n",
        "\"\"\"\n",
        "\n",
        "def positional_encoding(batch_size, max_len, d_model):\n",
        "    \"\"\"\n",
        "    output\n",
        "    \"\"\"\n",
        "    pe = torch.zeros(batch_size, max_len, d_model)\n",
        "    # create a positional array\n",
        "    position = torch.arange(0, max_len).unsqueeze(1)\n",
        "    # div term for half of positions\n",
        "    div_term = torch.pow(10000.0, torch.arange(0, d_model, 2) / d_model)\n",
        "    # even positions\n",
        "    pe[:, :, 0::2] = torch.sin(position * div_term)\n",
        "    # odd positions\n",
        "    pe[:, :, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "    # if normalize:\n",
        "    #     pe = pe - pe.mean()\n",
        "    #     pe = pe / (pe.std() * 10)\n",
        "\n",
        "    return nn.parameter.Parameter(pe, requires_grad= False)"
      ],
      "metadata": {
        "id": "hbRO5NzrNy8b"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PatchTST\n",
        "\n",
        "class PatchTSTEncoder(nn.Module):\n",
        "    def __init__(self, num_channels, num_var, patch_len, stride, batch_size, time_dimension = 8, d_model = 128, n_layers = 3, n_heads = 16, dropout = 0.2):\n",
        "        super(PatchTSTEncoder, self).__init__()\n",
        "\n",
        "        self.log = setup_log(self, LOG_LEVEL)\n",
        "\n",
        "        self.num_channels = num_channels\n",
        "        self.patch_num = (max(patch_len, num_var)-patch_len) // stride + 2\n",
        "        self.patch_len = patch_len\n",
        "        self.stride = stride\n",
        "        self.batch_size = batch_size\n",
        "        self.d_model = d_model\n",
        "        self.n_layers = n_layers\n",
        "        self.n_heads = n_heads\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # instance normalization\n",
        "        \"\"\"\n",
        "        ref: https://wandb.ai/wandb_fc/Normalization-Series/reports/Instance-Normalization-in-PyTorch-With-Examples---VmlldzoxNDIyNTQx\n",
        "        \"\"\"\n",
        "        self.inst_norm = nn.InstanceNorm2d(num_channels)\n",
        "\n",
        "        # patch creation\n",
        "        self.create_patch = create_patches\n",
        "\n",
        "        # embedding\n",
        "        self.W_p = nn.Linear(patch_len * time_dimension, d_model, bias = False)\n",
        "\n",
        "        # positional encoding\n",
        "        self.W_pos = positional_encoding(batch_size * num_channels, self.patch_num, d_model)\n",
        "\n",
        "        # dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # encoder\n",
        "        self.encoders = nn.ModuleList([VanillaTransformerEncoder(d_model) for _ in range(n_layers)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x -> [B x L x M] // [(B x M) x MAX_TRAIN_LENGTH x TIME_DIM]\n",
        "        output -> [(B M) x N x D]\n",
        "        \"\"\"\n",
        "        b_m, _, _ = x\n",
        "        assert b_m / self.num_channel == self.batch_size, f\"invalid fisrt dimension {b_m / self.num_channel} != {self.batch_size}\"\n",
        "        # [(B M) x MAX_TRAIN_LENGTH x TIME_DIM] -> [B x M x MAX_TRAIN_LENGTH x TIME_DIM]\n",
        "        x = einops.rearrange(x, '(b m) l t -> b m l t', m=self.num_channels)\n",
        "        # we need to reshape dimensione before apply instance normalization\n",
        "        x = einops.rearrange(self.inst_norm(x), 'b m l t -> b l m t')\n",
        "\n",
        "        # create patches\n",
        "        x, patch_num = self.create_patch(x, self.patch_len, self.stride)\n",
        "\n",
        "        # x: [B x N x M x T x P]\n",
        "\n",
        "        assert self.patch_num == patch_num, f\"wrong number for patch_num {self.patch_num} != {patch_num}\"\n",
        "\n",
        "        # reshape the tensor from [B x N x M x T x P] -> [(B M) x N x (P T)]\n",
        "        x = einops.rearrange(x, 'b n m t p -> (b m) n (p t)')\n",
        "        # now it can be provided to our transformer implementation\n",
        "\n",
        "        # project into transformer latent space\n",
        "        x = self.W_p(x) + self.W_pos\n",
        "\n",
        "        for layer in self.encoders:\n",
        "            x = layer(x)\n",
        "\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "bhiJYUZcGvr_"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cost"
      ],
      "metadata": {
        "id": "DTlGPLvHrJ5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules.loss import TripletMarginWithDistanceLoss\n",
        "#Cost\n",
        "\n",
        "class Cost(nn.Module):\n",
        "    def __init__(self, input_size, d_model = 256, d_s = 128, d_t = 128, n_layers = 3, n_heads = 6, dropout = 0.2):\n",
        "        super(Cost, self).__init__()\n",
        "\n",
        "        self.log = setup_log(self, LOG_LEVEL)\n",
        "\n",
        "        # Dropout for seasonal representation output\n",
        "        self.seasonal_drop = nn.Dropout(0.1)\n",
        "\n",
        "        # Trend Feature Disentangler\n",
        "        self.tfd = TrendFeatureDisentangler(d_model, d_t, input_size)\n",
        "\n",
        "        # Seasonal Feature Disentangler\n",
        "        self.sfd = SeasonalFeatureDisentangler(d_model, d_s, input_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: (batch, input_size, d_model)\n",
        "        outputs: (batch, input_size, d_t), (batch, input_size, d_s)\n",
        "        \"\"\"\n",
        "        out_tfd = self.tfd(x)\n",
        "\n",
        "        out_svd = self.sfd(x)\n",
        "\n",
        "        out_svd = self.seasonal_drop(out_svd)\n",
        "\n",
        "        return out_tfd, out_svd\n",
        "\n",
        "\n",
        "\n",
        "# Causal Convolution (dilated)\n",
        "\n",
        "class CausalConv1d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, dilation=1):\n",
        "        super(CausalConv1d, self).__init__()\n",
        "\n",
        "        self.log = setup_log(self, LOG_LEVEL)\n",
        "\n",
        "        self.kernel_size = kernel_size\n",
        "        pad = (kernel_size - 1) * dilation\n",
        "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, padding=pad, dilation=dilation)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        input: (batch, input_size, in_channels)\n",
        "        output: (batch, conv_out, out_channels)\n",
        "        \"\"\"\n",
        "        # we need to reshape before applying the convolution\n",
        "        x = einops.rearrange(x, 'b l i_c -> b i_c l')\n",
        "        x = self.conv(x)\n",
        "\n",
        "        # we need to remove the trailing padding zeros (except for the fist layer) from the values\n",
        "        if self.kernel_size > 1:\n",
        "            x = x[...,0:-(self.kernel_size-1)]\n",
        "\n",
        "        # rearrange to the original shape\n",
        "        x = einops.rearrange(x, 'b o_c l -> b l o_c')\n",
        "\n",
        "        return x\n",
        "\n",
        "# TFD\n",
        "\n",
        "class TrendFeatureDisentangler(nn.Module):\n",
        "    def __init__(self, d_model, d_t, input_size):\n",
        "        super(TrendFeatureDisentangler, self).__init__()\n",
        "\n",
        "        self.log = setup_log(self, LOG_LEVEL)\n",
        "        self.d_model = d_model\n",
        "        self.d_t = d_t\n",
        "        self.input_size = input_size\n",
        "\n",
        "        # https://discuss.pytorch.org/t/causal-convolution/3456/3\n",
        "        # https://arxiv.org/pdf/1609.03499v2.pdf\n",
        "\n",
        "        # floor(log(N/2)) autoregressive expert\n",
        "        self.conv_num = math.floor(math.log2(input_size / 2)) + 1\n",
        "        self.kernel = [2**i for i in range(self.conv_num)]\n",
        "        self.convolutions = nn.ModuleList([CausalConv1d(d_model, d_t, k) for k in self.kernel])\n",
        "\n",
        "    def avg_pooling(self, input):\n",
        "        \"\"\"\n",
        "        input: (list, batch, input_size, d_t)\n",
        "        \"\"\"\n",
        "        return einops.reduce(input, 'list b l d_t -> b l d_t', 'mean')\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: (batch, input_size, d_model)\n",
        "        output: (batch, input_size, d_t)\n",
        "        \"\"\"\n",
        "        batch_size, input_size, d_model = x.shape\n",
        "\n",
        "        assert input_size == self.input_size and d_model == self.d_model, \"wrong input dimensions\"\n",
        "\n",
        "        # create the result tensor\n",
        "        trend = torch.zeros(self.conv_num, batch_size, input_size, self.d_t, device = x.device)\n",
        "\n",
        "        for i, conv in enumerate(self.convolutions):\n",
        "            out = conv(x)\n",
        "            trend[i,...] = out\n",
        "\n",
        "        # apply the average pooling operation\n",
        "        trend = self.avg_pooling(trend)\n",
        "\n",
        "        return trend\n",
        "\n",
        "# SVD\n",
        "\n",
        "class SeasonalFeatureDisentangler(nn.Module):\n",
        "    def __init__(self, d_model, d_s, input_size, dropout = 0.1):\n",
        "        super(SeasonalFeatureDisentangler, self).__init__()\n",
        "\n",
        "        self.log = setup_log(self, LOG_LEVEL)\n",
        "\n",
        "        self.input_size = input_size\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # number of frequencies after dft\n",
        "        self.f = input_size // 2 + 1\n",
        "\n",
        "        # discrete fast fourier transform, rfft output contains only the positive frequencies below the Nyquist frequency\n",
        "        self.dft = torch.fft.rfft\n",
        "\n",
        "        # Learnable Fourier Layer\n",
        "        self.fl = FourierLayer(self.f, d_model, d_s, input_size)\n",
        "\n",
        "        # inverse of discrete fast fourier transform\n",
        "        self.idft = torch.fft.irfft\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: (batch, input_size, d_model)\n",
        "        output: (batch, input_size, d_s)\n",
        "        \"\"\"\n",
        "        # we apply dft along the temporal dimension\n",
        "        x = self.dft(x, dim = 1)\n",
        "\n",
        "        assert self.f == x.shape[1], \"wrong dimension of dft\"\n",
        "\n",
        "        # apply fourier layer\n",
        "        x = self.fl(x)\n",
        "\n",
        "        # compute the inverse of dft to come back to time domain\n",
        "        x = self.idft(x, n = self.input_size, dim = 1) # pass also the legth in order to avoid odd-length problems\n",
        "\n",
        "        return self.dropout(x)\n",
        "\n",
        "class FourierLayer(nn.Module):\n",
        "    def __init__(self, f, d_model, d_s, input_size):\n",
        "        super(FourierLayer, self).__init__()\n",
        "\n",
        "        self.log = setup_log(self, LOG_LEVEL)\n",
        "\n",
        "        self.f = f\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.A = nn.Parameter(torch.empty((f, d_model, d_s), dtype=torch.cfloat))\n",
        "        self.B = nn.Parameter(torch.empty((f, d_s), dtype=torch.cfloat))\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))\n",
        "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.A)\n",
        "        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
        "        nn.init.uniform_(self.B, -bound, bound)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: (batch, f, d_model)\n",
        "        out: (batch, f, d_s)\n",
        "        \"\"\"\n",
        "        batch_size, f, _ = x.shape\n",
        "\n",
        "        assert f == self.f, \"wrong dimensions of x\"\n",
        "\n",
        "        out = einops.einsum(self.A, x, 'f d d_s, b f d -> b f d_s') + self.B\n",
        "\n",
        "        return out\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-U9fYJUKrN4S"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CoSpy Encoder"
      ],
      "metadata": {
        "id": "omgfLK9mFicv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %debug\n",
        "class CoSpyEncoder(nn.Module):\n",
        "    def __init__(self, input_size,\n",
        "                 d_model = 256, d_s = 128, d_t = 128,\n",
        "                 dropout = 0.2, device = 'cpu'):\n",
        "        super(CoSpyEncoder, self).__init__()\n",
        "        # self.save_hyperparameters()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.device = device\n",
        "        self.log = setup_log(self, LOG_LEVEL)\n",
        "\n",
        "        # Pyraformer layer (backbone encoder)\n",
        "        self.py = PyraformerEncoder(d_model, device = device)\n",
        "\n",
        "\n",
        "        # CoST layer (disentangler)\n",
        "        self.cost = Cost(self.input_size)\n",
        "\n",
        "    def encode(self, data_shape, loader, batch_size = 256, sliding_length=1, padding=200):\n",
        "\n",
        "        encoding_window = None\n",
        "        slicing = None\n",
        "\n",
        "        n_samples, ts_l, _ = data_shape\n",
        "\n",
        "        org_training = self.training\n",
        "        self.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = []\n",
        "            for batch in tqdm(loader, desc=\"data encoding\"):\n",
        "                x = batch[0]\n",
        "                reprs = []\n",
        "                # x = x.to(device)\n",
        "                if n_samples < batch_size:\n",
        "                    calc_buffer = []\n",
        "                    calc_buffer_l = 0\n",
        "                # self.log.debug(type(x))\n",
        "                # self.log.debug(f\"shape of batch: {x.shape}\")\n",
        "\n",
        "                # if self.batch_size != batch_size:\n",
        "                #     self.log.debug(\"different batch size return same batch\")\n",
        "                #     return batch\n",
        "\n",
        "                for i in tqdm(range(0, ts_l, sliding_length),\n",
        "                              desc = \"sequence encoding\"):\n",
        "                    l = i - padding # sliding_padding=200\n",
        "                    r = i + sliding_length\n",
        "                    # self.log.debug(x.device)\n",
        "                    x_sliding = pad_nan(\n",
        "                        x[:, max(l, 0) : min(r, ts_l)],\n",
        "                        left=-l if l<0 else 0,\n",
        "                        right=r-ts_l if r>ts_l else 0,\n",
        "                        dim=1\n",
        "                    )\n",
        "                    if n_samples < batch_size:\n",
        "                        if calc_buffer_l + n_samples > batch_size:\n",
        "                            out = self._eval_with_pooling(\n",
        "                                torch.cat(calc_buffer, dim=0)\n",
        "                            )\n",
        "                            reprs += torch.split(out, n_samples)\n",
        "                            calc_buffer = []\n",
        "                            calc_buffer_l = 0\n",
        "                        calc_buffer.append(x_sliding)\n",
        "                        calc_buffer_l += n_samples\n",
        "                    else:\n",
        "                        reprs.append(self._eval_with_pooling(x_sliding))\n",
        "\n",
        "                if n_samples < batch_size:\n",
        "                    if calc_buffer_l > 0:\n",
        "                        out = self._eval_with_pooling(\n",
        "                            torch.cat(calc_buffer, dim=0)\n",
        "                        )\n",
        "                        reprs += torch.split(out, n_samples)\n",
        "                        calc_buffer = []\n",
        "                        calc_buffer_l = 0\n",
        "\n",
        "                out = torch.cat(reprs, dim=1)\n",
        "                output.append(out)\n",
        "\n",
        "            output = torch.cat(output, dim=0)\n",
        "\n",
        "        self.train(org_training)\n",
        "        return output.numpy()\n",
        "\n",
        "    def _eval_with_pooling(self, x):\n",
        "        out_t, out_s = self(x.to(self.device, non_blocking=True))\n",
        "        out = torch.cat([out_t[:, -1], out_s[:, -1]], dim=-1)\n",
        "        return einops.rearrange(out.cpu(), 'b d -> b () d')\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: (batch, input_size, enc_in + covariate_size)\n",
        "        outputs: (batch, input_size, d_t), (batch, input_size, d_s)\n",
        "        \"\"\"\n",
        "\n",
        "        nan_mask = ~x.isnan().any(axis=-1)\n",
        "        x[~nan_mask] = 0\n",
        "\n",
        "        x = self.py(x)\n",
        "\n",
        "        trend, season = self.cost(x)\n",
        "\n",
        "        return trend, season"
      ],
      "metadata": {
        "id": "uSr7enmSFptR"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CoSpy Model"
      ],
      "metadata": {
        "id": "niFGAd70x41j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Constrastive model similar to MoCo\n",
        "\"\"\"\n",
        "https://arxiv.org/pdf/1911.05722.pdf\n",
        "https://github.com/facebookresearch/moco/blob/main/moco/builder.py\n",
        "\"\"\"\n",
        "\n",
        "class CoSpyModel(pl.LightningModule):\n",
        "    def __init__(self, max_train_length, comp_dimension = 128, alpha = 5e-4, K = 65536, m = 0.999, T = 0.07,\n",
        "                 lr = 1e-3, om = 0.9, wd = 1e-4, epochs = 10, device = 'cpu'):\n",
        "        super(CoSpyModel, self).__init__()\n",
        "\n",
        "        self.cum_loss = 0\n",
        "        self.n_epoch_iters = 0\n",
        "        self.epochs = epochs\n",
        "\n",
        "        self.input_size = max_train_length\n",
        "        self.max_train_length = max_train_length\n",
        "\n",
        "        self.K = K\n",
        "        self.m = m\n",
        "        self.T = T\n",
        "\n",
        "        self.alpha = alpha\n",
        "\n",
        "        self.lr = lr\n",
        "        self.om = om\n",
        "        self.wd = wd\n",
        "\n",
        "        self.encoder_q = CoSpyEncoder(self.input_size, device = device)\n",
        "        self.encoder_k = copy.deepcopy(self.encoder_q)\n",
        "\n",
        "        # projections head for queries and keyes\n",
        "        self.head_q = nn.Sequential(\n",
        "            nn.Linear(comp_dimension, comp_dimension),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(comp_dimension, comp_dimension)\n",
        "        )\n",
        "        self.head_k = nn.Sequential(\n",
        "            nn.Linear(comp_dimension, comp_dimension),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(comp_dimension, comp_dimension)\n",
        "        )\n",
        "\n",
        "        # initialize the parameters of the keyes encoder and projection head\n",
        "        for param_q, param_k in zip(self.encoder_q.parameters(), self.encoder_k.parameters()):\n",
        "            param_k.data.copy_(param_q.data)\n",
        "            param_k.requires_grad = False # the keyes encoder will be updated by the momentum update\n",
        "\n",
        "        for param_q, param_k in zip(self.head_q.parameters(), self.head_k.parameters()):\n",
        "            param_k.data.copy_(param_q.data)\n",
        "            param_k.requires_grad = False # the head_k will be updated by the momentum update\n",
        "\n",
        "        # register a dictionary buffer as a queue (decouped from the minibatch size)\n",
        "        # https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_buffer\n",
        "        self.register_buffer('queue', F.normalize(torch.randn(comp_dimension, K), dim=0))\n",
        "        self.register_buffer('queue_ptr', torch.zeros(1, dtype=torch.long))\n",
        "\n",
        "        self.save_hyperparameters(ignore=['encoder_q', 'encoder_k'])\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # set parameters of SGD\n",
        "        optimizer = torch.optim.SGD([p for p in self.parameters() if p.requires_grad],\n",
        "                                    lr = self.lr, momentum = self.om, weight_decay = self.wd)\n",
        "        # cosine annelling is a wrapper for SGD\n",
        "        # cosine_anneling = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer = optimizer, T_max = 100)\n",
        "        return optimizer\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _momentum_update_key_encoder(self):\n",
        "        \"\"\"\n",
        "        Momentum update for key encoder\n",
        "        \"\"\"\n",
        "        for param_q, param_k in zip(self.encoder_q.parameters(), self.encoder_k.parameters()):\n",
        "            param_k.data = param_k.data * self.m + param_q.data * (1 - self.m)\n",
        "\n",
        "        for param_q, param_k in zip(self.head_q.parameters(), self.head_k.parameters()):\n",
        "            param_k.data = param_k.data * self.m + param_q.data * (1 - self.m)\n",
        "\n",
        "    def compute_loss(self, q, k, k_negs):\n",
        "        # compute logits\n",
        "        # positive logits: Bx1 (one timestamp as postive)\n",
        "        l_pos = einops.einsum(q, k, 'b c,b c->b').unsqueeze(-1)\n",
        "        # negative logits: BxK\n",
        "        l_neg = einops.einsum(q, k_negs, 'b c,c k->b k')\n",
        "\n",
        "        # logits: Bx(1+K)\n",
        "        logits = torch.cat([l_pos, l_neg], dim=1)\n",
        "\n",
        "        # apply temperature\n",
        "        logits /= self.T\n",
        "\n",
        "        # labels: positive key indicators - first dim of each batch (it will be considered the positive sample)\n",
        "        # so we can consider this as a classification problem and use the CE\n",
        "        labels = torch.zeros(logits.shape[0], dtype=torch.long, device = logits.device)\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def get_polar(self, x):\n",
        "        mod = x.abs()\n",
        "        phase = x.angle()\n",
        "\n",
        "        return (mod, phase)\n",
        "\n",
        "    def instance_contrastive_loss(self, z1, z2):\n",
        "        B = z1.shape[0]\n",
        "        z = torch.cat([z1, z2], dim=0)  # 2B x F x d_s\n",
        "        z = einops.rearrange(z, 'b f d_s -> f b d_s')  # F x 2B x d_s\n",
        "        sim = einops.einsum(z, z, 'f b_1 d_s, f b_2 d_s -> f b_1 b_2')  # F x 2B x 2B\n",
        "        logits = torch.tril(sim, diagonal=-1)[:, :, :-1]  # F x 2B x (2B-1)\n",
        "        logits += torch.triu(sim, diagonal=1)[:, :, 1:]\n",
        "        logits = -F.log_softmax(logits, dim=-1)\n",
        "        # log.debug(logits)\n",
        "\n",
        "        i = torch.arange(B)\n",
        "        loss = (logits[:, i, B + i - 1].mean() + logits[:, B + i, i].mean()) / 2\n",
        "        return loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _dequeue_and_enqueue(self, keys):\n",
        "        batch_size = keys.shape[0]\n",
        "\n",
        "        ptr = int(self.queue_ptr)\n",
        "        assert self.K % batch_size == 0, \"K must be a multiple of batch_size\"\n",
        "\n",
        "        # replace keys at ptr (dequeue and enqueue)\n",
        "        self.queue[:, ptr:ptr + batch_size] = keys.T\n",
        "\n",
        "        ptr = (ptr + batch_size) % self.K\n",
        "        self.queue_ptr[0] = ptr\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x_q, x_k = batch\n",
        "        if self.max_train_length is not None and x_q.size(1) > self.max_train_length:\n",
        "            window_offset = np.random.randint(x_q.size(1) - self.max_train_length + 1)\n",
        "            x_q = x_q[:, window_offset : window_offset + self.max_train_length]\n",
        "            x_k = x_k[:, window_offset : window_offset + self.max_train_length]\n",
        "\n",
        "        loss = self.forward(x_q, x_k)\n",
        "\n",
        "        self.cum_loss += loss\n",
        "        self.n_epoch_iters += 1\n",
        "\n",
        "        # logs metrics for each training_step,\n",
        "        # and the average across the epoch, to the progress bar and logger\n",
        "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        wandb.log({\"train\": {\"loss\":loss}})\n",
        "        return loss\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "        # self.log(self.log(\"cum_loss\", self.cum_loss, on_step=True, on_epoch=True, prog_bar=True, logger = True))\n",
        "        wandb.log({\"train\": {\"epoch\": self.current_epoch ,\"cum_loss_epoch\": self.cum_loss / self.n_epoch_iters}})\n",
        "        # adjust learning rate\n",
        "        optimizer = self.optimizers().optimizer\n",
        "        self._adjust_learning_rate(self.lr, optimizer, self.current_epoch, self.epochs)\n",
        "\n",
        "\n",
        "\n",
        "    def _adjust_learning_rate(self, lr, optimizer, epoch, epochs):\n",
        "        \"\"\"Decay the learning rate based on schedule\"\"\"\n",
        "        lr *= 0.5 * (1. + math.cos(math.pi * epoch / epochs))\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "\n",
        "    # def validation_step(self, batch, batch_idx):\n",
        "    #     x_q, x_k = batch\n",
        "    #     loss = self.forward(x_q, x_k)\n",
        "\n",
        "    #     # logs metrics for each training_step,\n",
        "    #     # and the average across the epoch, to the progress bar and logger\n",
        "    #     self.log(\"val_loss\", loss)\n",
        "    #     return loss\n",
        "\n",
        "    # def test_step(self, batch, batch_idx):\n",
        "    #     x_q, x_k = batch\n",
        "    #     loss = self.forward(x_q, x_k)\n",
        "\n",
        "    #     # logs metrics for each training_step,\n",
        "    #     # and the average across the epoch, to the progress bar and logger\n",
        "    #     self.log(\"test_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "    #     return loss\n",
        "\n",
        "    def forward(self, x_q, x_k):\n",
        "        \"\"\"\n",
        "        x_q, x_k: (batch, input_size, enc_in + covariate_size)\n",
        "        \"\"\"\n",
        "        # select a random timestamp\n",
        "        rand_idx = np.random.randint(0, self.input_size)\n",
        "\n",
        "        # trend and seasonal queries\n",
        "        q_t, q_s = self.encoder_q(x_q)\n",
        "\n",
        "        if q_t is not None:\n",
        "            q_t = F.normalize(self.head_q(q_t[:, rand_idx]), dim=-1)\n",
        "\n",
        "        # compute key features\n",
        "        with torch.no_grad():  # no gradient update for keys (momentum update will be used)\n",
        "            self._momentum_update_key_encoder()  # update key encoder using momentum\n",
        "            k_t, k_s = self.encoder_k(x_k)\n",
        "            if k_t is not None:\n",
        "                k_t = F.normalize(self.head_k(k_t[:, rand_idx]), dim=-1)\n",
        "\n",
        "        loss = 0\n",
        "\n",
        "        loss += self.compute_loss(q_t, k_t, self.queue.clone().detach())\n",
        "        self._dequeue_and_enqueue(k_t)\n",
        "\n",
        "        q_s = F.normalize(q_s, dim=-1)\n",
        "        _, k_s = self.encoder_q(x_k)\n",
        "        k_s = F.normalize(k_s, dim=-1)\n",
        "\n",
        "        # the frequency and phase lost must be computed in the frequency domain\n",
        "        q_s_freq = torch.fft.rfft(q_s, dim=1)\n",
        "        k_s_freq = torch.fft.rfft(k_s, dim=1)\n",
        "        q_s_amp, q_s_phase = self.get_polar(q_s_freq)\n",
        "        k_s_amp, k_s_phase = self.get_polar(k_s_freq)\n",
        "\n",
        "        seasonal_loss = self.instance_contrastive_loss(q_s_amp, k_s_amp) + \\\n",
        "                        self.instance_contrastive_loss(q_s_phase,k_s_phase)\n",
        "        loss += (self.alpha * (seasonal_loss/2))\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "xSBWEMomx4KG"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "Oqxoxjh9C0rC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 1\n",
        "# initialize dataset\n",
        "# datamodule = ElectricityDataModule(datasets_path[ELECTRICITY] + datasets_processed_name[ELECTRICITY], BATCH_SIZE, L)\n",
        "datamodule = CustomDataModule(DATASET, datasets_path[ELECTRICITY] + datasets_processed_name[DATASET],\n",
        "                       BATCH_SIZE, L, encode_batch_size=ENCODE_BATCH_SIZE, pred_lens = datasets_pred_lens[DATASET], univariate=UNIVARIATE)\n",
        "\n",
        "# set seed if deterministic\n",
        "if DETERMINISTIC:\n",
        "    seed_everything(seed)\n",
        "# initialize model, or load an extisting one\n",
        "model = CoSpyModel(L, epochs = EPOCHS, device = DEVICE)\n",
        "if LOAD_MODEL:\n",
        "    log.info(\"loading model...\")\n",
        "    model = CoSpyModel.load_from_checkpoint(CHECKPOINT_FOLDER + \"/last.ckpt\")\n",
        "\n",
        "if MEMORY_PROFILING:\n",
        "    profiler = PyTorchProfiler()"
      ],
      "metadata": {
        "id": "AvftO5iOK9ZR",
        "outputId": "278938cb-ef92-4e1c-f88a-0791dade1c3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightning_fabric.utilities.seed:Global seed set to 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "DYNOokNm3LCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train function\n",
        "def train(batch_size, datamodule, model, model_name, max_epochs = 500, checkpoint_every_n_epochs = 5,\n",
        "          check_val_every_n_epoch = 5, resume_training = True, load_model = False,\n",
        "          enable_checkpoint = True, monitor_metric = \"val_loss\", checkpoint_dir = None,\n",
        "          log_flag = False, logs_dir = None,\n",
        "          early_stopping = True, deterministic = False, profiler = None):\n",
        "\n",
        "    # check monitor metric\n",
        "    assert monitor_metric in [\"train_loss\", \"vall_loss\"], \"metric to monitor is invalid\"\n",
        "\n",
        "    # initialize callbacks array\n",
        "    callbacks = [TQDMProgressBar(refresh_rate=20)]\n",
        "\n",
        "    # add checkpoints to callbacks\n",
        "    checkpoint_callback = None\n",
        "    if enable_checkpoint and checkpoint_dir is not None:\n",
        "        checkpoint_callback = ModelCheckpoint(dirpath=checkpoint_dir,  monitor = monitor_metric, filename=model_name + '-mnist-{epoch:02d}-{' + monitor_metric + ':.2f}',\n",
        "                                         save_last =True, every_n_epochs = checkpoint_every_n_epochs, save_on_train_epoch_end = True)\n",
        "        callbacks.append(checkpoint_callback)\n",
        "\n",
        "    # add early stopping to the callbacks\n",
        "    if early_stopping:\n",
        "        callbacks.append(EarlyStopping(monitor=\"val_loss\", min_delta = 0.1, patience = 3, mode=\"min\", check_on_train_epoch_end = False))\n",
        "\n",
        "    # define the logger object\n",
        "    logger = None\n",
        "    if log_flag:\n",
        "        # logger = TensorBoardLogger(logs_dir, name=model_name)\n",
        "        wandb_logger = WandbLogger(name = model_name, log_model = 'all')\n",
        "\n",
        "    # create the Trainer\n",
        "    trainer = pl.Trainer(enable_checkpointing=enable_checkpoint, devices=1, accelerator=\"auto\",\n",
        "                         max_epochs=max_epochs, logger=logger, callbacks=callbacks,  ## remove max_step\n",
        "                         check_val_every_n_epoch = check_val_every_n_epoch, ## remove\n",
        "                         deterministic = deterministic, profiler = profiler)\n",
        "\n",
        "    ckpt_path = None\n",
        "    if resume_training:\n",
        "        ckpt_path = checkpoint_dir + \"/last.ckpt\"\n",
        "    trainer.fit(ckpt_path = ckpt_path, model=model, datamodule=datamodule)\n",
        "    if checkpoint_callback is not None:\n",
        "        log.info(checkpoint_callback.best_model_path)\n",
        "\n",
        "    return trainer"
      ],
      "metadata": {
        "id": "q-936er_-snn"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if TRAIN:\n",
        "    trainer = train(BATCH_SIZE, datamodule, model, CoSpy, max_epochs = EPOCHS, check_val_every_n_epoch = None, load_model = LOAD_MODEL,\n",
        "        resume_training = RESUME_TRAINING, monitor_metric = \"train_loss\", checkpoint_dir = CHECKPOINT_FOLDER,\n",
        "        logs_dir = LOGS_FOLDER, early_stopping = False, deterministic = DETERMINISTIC,)"
      ],
      "metadata": {
        "id": "r3xeKsgm2792",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503,
          "referenced_widgets": [
            "3668f36def784138adeef5ec51b6a78e",
            "706de14f79ba492e8634d244762a7b20",
            "f44759d148f846108986f740858b32ae",
            "b006b8c4c91941f5be8eb5fa9a9277de",
            "32ba2130e1384ab9bef808d02dea7da0",
            "a2b78466c75c48c8abcc4effe579c8ba",
            "a0bf7ecfe2eb465bab68ecce50a054f9",
            "cb58d691ad464eba9c2f25b2ce1c052e",
            "6f5d5648047242609268c50b13582b9f",
            "8e3ded6670cf425795d2f66131ba7d3f",
            "78020c19767f4d7e95336e974a63635a"
          ]
        },
        "outputId": "35cdf850-796f-4f8e-a844-fd536a413933"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "<ipython-input-43-799d51a844d7>:71: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series. To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)\n",
            "  dt.weekofyear.to_numpy(),\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /content/drive/.shortcut-targets-by-id/1k77hCdwMON0_Vh3PFhnWybd1oBUUCWE9/Tesi/code/checkpoints/CoSpy exists and is not empty.\n",
            "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name      | Type         | Params\n",
            "-------------------------------------------\n",
            "0 | encoder_q | CoSpyEncoder | 12.0 M\n",
            "1 | encoder_k | CoSpyEncoder | 12.0 M\n",
            "2 | head_q    | Sequential   | 33.0 K\n",
            "3 | head_k    | Sequential   | 33.0 K\n",
            "-------------------------------------------\n",
            "12.0 M    Trainable params\n",
            "12.0 M    Non-trainable params\n",
            "24.1 M    Total params\n",
            "96.302    Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "  rank_zero_warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3668f36def784138adeef5ec51b6a78e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=600` reached.\n",
            "INFO:APP:/content/drive/.shortcut-targets-by-id/1k77hCdwMON0_Vh3PFhnWybd1oBUUCWE9/Tesi/code/checkpoints/CoSpy/CoSpy-mnist-epoch=599-train_loss=2.46.ckpt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Encoding"
      ],
      "metadata": {
        "id": "6lzsSVC6kmJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_encoding(datamodule):\n",
        "    datamodule.setup(\"encoding\")\n",
        "    loader = datamodule.encode_dataloader()\n",
        "    return loader, datamodule.data.shape\n",
        "\n",
        "def encode(model, data_shape, loader, batch_size, device, save_path):\n",
        "    model.to(device)\n",
        "    model = model.encoder_q\n",
        "    res = model.encode(data_shape, loader, batch_size = batch_size, padding = PADDING)\n",
        "    file_name = f\"encoding_{time.time()}.pkl\"\n",
        "    pkl_save(f'{save_path}/{file_name}', res)\n",
        "    pkl_save(f'{save_path}/last.pkl', res)\n",
        "    log.info(f\"encoding {file_name} saved\")\n",
        "\n",
        "    return res"
      ],
      "metadata": {
        "id": "BIhjMuNq2nOY"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoding_loader, data_shape = prepare_encoding(datamodule)\n",
        "# if LOAD_ENCODE:\n",
        "#     repr = pkl_load(ENCODING_FOLDER + \"/last.pkl\")\n",
        "# else:\n",
        "#     repr = encode(model, data_shape, encoding_loader, ENCODE_BATCH_SIZE, DEVICE, ENCODING_FOLDER)"
      ],
      "metadata": {
        "id": "s32ambAiMImm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dc323b2-e62f-4825-913a-303a4dbd6e4f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-4eff98b662b0>:94: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series. To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)\n",
            "  dt.weekofyear.to_numpy(),\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Forecasting Evaluation"
      ],
      "metadata": {
        "id": "sLMxIOWrOckk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper functions\n",
        "def generate_pred_samples(features, data, pred_len, drop=0):\n",
        "    n = data.shape[1]\n",
        "    features = features[:, :-pred_len]\n",
        "    labels = np.stack([ data[:, i:1+n+i-pred_len] for i in range(pred_len)], axis=2)[:, 1:]\n",
        "    features = features[:, drop:]\n",
        "    labels = labels[:, drop:]\n",
        "    return features.reshape(-1, features.shape[-1]), labels.reshape(-1, labels.shape[2]*labels.shape[3])\n",
        "\n",
        "def fit_ridge(train_features, train_y, valid_features, valid_y, MAX_SAMPLES=100000):\n",
        "    # If the training set is too large, subsample MAX_SAMPLES examples\n",
        "    if train_features.shape[0] > MAX_SAMPLES:\n",
        "        split = train_test_split(\n",
        "            train_features, train_y,\n",
        "            train_size=MAX_SAMPLES, random_state=0\n",
        "        )\n",
        "        train_features = split[0]\n",
        "        train_y = split[2]\n",
        "    if valid_features.shape[0] > MAX_SAMPLES:\n",
        "        split = train_test_split(\n",
        "            valid_features, valid_y,\n",
        "            train_size=MAX_SAMPLES, random_state=0\n",
        "        )\n",
        "        valid_features = split[0]\n",
        "        valid_y = split[2]\n",
        "    alphas = [0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 50, 100, 200, 500, 1000]\n",
        "    valid_results = []\n",
        "    for alpha in alphas:\n",
        "        lr = Ridge(alpha=alpha).fit(train_features, train_y)\n",
        "        valid_pred = lr.predict(valid_features)\n",
        "        score = np.sqrt(((valid_pred - valid_y) ** 2).mean()) + np.abs(valid_pred - valid_y).mean()\n",
        "        valid_results.append(score)\n",
        "    best_alpha = alphas[np.argmin(valid_results)]\n",
        "\n",
        "    lr = Ridge(alpha=best_alpha)\n",
        "    lr.fit(train_features, train_y)\n",
        "    return lr\n",
        "\n",
        "def cal_metrics(pred, target):\n",
        "    return {\n",
        "        'MSE': ((pred - target) ** 2).mean(),\n",
        "        'MAE': np.abs(pred - target).mean()\n",
        "    }"
      ],
      "metadata": {
        "id": "aw_fXa2wRZxr"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_forecasting(repr, data, train_slice, valid_slice, test_slice, n_covariate_cols, scaler, padding, pred_lens):\n",
        "    train_repr = repr[:, train_slice]\n",
        "    valid_repr = repr[:, valid_slice]\n",
        "    test_repr = repr[:, test_slice]\n",
        "\n",
        "    train_data = data[:, train_slice, n_covariate_cols:]\n",
        "    valid_data = data[:, valid_slice, n_covariate_cols:]\n",
        "    test_data = data[:, test_slice, n_covariate_cols:]\n",
        "\n",
        "    ours_result = {}\n",
        "    out_log = {}\n",
        "    for pred_len in tqdm(pred_lens, desc=\"forecasting evaluation\"):\n",
        "        train_features, train_labels = generate_pred_samples(train_repr, train_data, pred_len, drop=padding)\n",
        "        valid_features, valid_labels = generate_pred_samples(valid_repr, valid_data, pred_len)\n",
        "        test_features, test_labels = generate_pred_samples(test_repr, test_data, pred_len)\n",
        "\n",
        "        lr = fit_ridge(train_features, train_labels, valid_features, valid_labels)\n",
        "\n",
        "        test_pred = lr.predict(test_features)\n",
        "\n",
        "        ori_shape = test_data.shape[0], -1, pred_len, test_data.shape[2]\n",
        "        test_pred = test_pred.reshape(ori_shape)\n",
        "        test_labels = test_labels.reshape(ori_shape)\n",
        "\n",
        "        test_shape = test_pred.shape\n",
        "        test_shape_swap = (test_shape[3], test_shape[1], test_shape[2], test_shape[0])\n",
        "        if test_data.shape[0] > 1:\n",
        "            test_pred_inv = scaler.inverse_transform(test_pred.swapaxes(0, 3)\n",
        "                .reshape(-1, test_shape[0])).reshape(test_shape_swap).swapaxes(0, 3)\n",
        "            test_labels_inv = scaler.inverse_transform(test_labels.swapaxes(0, 3)\n",
        "                .reshape(-1, test_shape[0])).reshape(test_shape_swap).swapaxes(0, 3)\n",
        "        else:\n",
        "            test_pred_inv = scaler.inverse_transform(test_pred.reshape(-1, test_shape[3])).reshape(test_shape)\n",
        "            test_labels_inv = scaler.inverse_transform(test_labels.reshape(-1, test_shape[3])).reshape(test_shape)\n",
        "\n",
        "        # out_log[pred_len] = {\n",
        "        #     # 'norm': test_pred,\n",
        "        #     # 'raw': test_pred_inv\n",
        "        #     # 'norm_gt': test_labels,\n",
        "        #     # 'raw_gt': test_labels_inv\n",
        "        # }\n",
        "        ours_result[pred_len] = {\n",
        "            'norm': cal_metrics(test_pred, test_labels),\n",
        "            'raw': cal_metrics(test_pred_inv, test_labels_inv)\n",
        "        }\n",
        "        print(ours_result)\n",
        "\n",
        "    eval_res = {\n",
        "        'ours': ours_result\n",
        "    }\n",
        "    return out_log, eval_res"
      ],
      "metadata": {
        "id": "0aJRiW2lOhoh"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_slice = datamodule.train_slice\n",
        "valid_slice = datamodule.valid_slice\n",
        "test_slice = datamodule.test_slice\n",
        "data = datamodule.data\n",
        "n_covariate_cols = datamodule.n_covariate_cols\n",
        "scaler = datamodule.scaler\n",
        "padding = PADDING\n",
        "pred_lens = datamodule.pred_lens\n",
        "repr=data # REMOVE\n",
        "\n",
        "out, eval_res = eval_forecasting(repr, data, train_slice, valid_slice, test_slice,\n",
        "                                 n_covariate_cols, scaler, padding, pred_lens)\n",
        "\n",
        "wandb.log({\"eval_forecasting\": eval_res})\n",
        "pkl_save(FORECASTING_RESULT + \"/out.pkl\", out)\n",
        "pkl_save(FORECASTING_RESULT + \"/eval_res.pkl\", eval_res)"
      ],
      "metadata": {
        "id": "NRE-qJPXLXOH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "46779a05a75d41eda354ab0eb2e41405",
            "4e33b47b545949b1811cc06bc8ef6f2e",
            "fddf10833e674ea5b9a04f00798d4fe3",
            "c2f4bcb9d4b048ffba565c31733d12e3",
            "0a3f68cf791043fca953625ff0da05f2",
            "0a1348be21144324981a5099468c3a07",
            "162f253284a943b5bdd5cbac1b92463f",
            "b27b85c871954687911110abc6efe35f",
            "51a4164135624cbc8508188a1e4d38e0",
            "6d34cb4842764799b52f86bc64799f3d",
            "86f1c1fc799b49f7ae687bce3a336029"
          ]
        },
        "outputId": "41dff7f8-e44b-46f2-e7d0-8ace6db50124"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "forecasting evaluation:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "46779a05a75d41eda354ab0eb2e41405"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{24: {'norm': {'MSE': 0.7685303299388603, 'MAE': 0.670691597312765}, 'raw': {'MSE': 56591644.51395549, 'MAE': 779.5539658037903}}}\n",
            "{24: {'norm': {'MSE': 0.7685303299388603, 'MAE': 0.670691597312765}, 'raw': {'MSE': 56591644.51395549, 'MAE': 779.5539658037903}}, 48: {'norm': {'MSE': 0.7853621396690261, 'MAE': 0.6807687248857937}, 'raw': {'MSE': 57189912.692027554, 'MAE': 789.0118766276106}}}\n"
          ]
        }
      ]
    }
  ]
}