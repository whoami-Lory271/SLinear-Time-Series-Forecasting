{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/whoami-Lory271/thesis-project/blob/main/CoST_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training CoST model"
      ],
      "metadata": {
        "id": "Gh2L5z66W2X6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "1LWptcrfWeZG",
        "outputId": "4047ac3a-66ee-415c-fe4e-73f73f39f7d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CoST'...\n",
            "remote: Enumerating objects: 103, done.\u001b[K\n",
            "remote: Counting objects: 100% (103/103), done.\u001b[K\n",
            "remote: Compressing objects: 100% (61/61), done.\u001b[K\n",
            "remote: Total 103 (delta 49), reused 93 (delta 39), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (103/103), 371.25 KiB | 4.64 MiB/s, done.\n",
            "Resolving deltas: 100% (49/49), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/whoami-Lory271/CoST"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "5EIFz57FW_Wh",
        "outputId": "48637980-fbfb-4e91-f4ca-f19cb33a3db9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp \"/content/drive/MyDrive/Tesi/code/datasets/electricity/LD2011_2014.txt\" \"/content/CoST/datasets/\"\n",
        "!cp \"/content/drive/MyDrive/Tesi/code/datasets/ETT/ETTh1.csv\" \"/content/CoST/datasets/\"\n",
        "!cp \"/content/drive/MyDrive/Tesi/code/datasets/ETT/ETTm2.csv\" \"/content/CoST/datasets/\""
      ],
      "metadata": {
        "id": "0hTJksSZXPcX"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/CoST/requirements.txt\n",
        "!pip install einops==0.6.1 --quiet"
      ],
      "metadata": {
        "id": "x5qeqzGyXkx9",
        "outputId": "e9f23f10-7942-4c3e-c991-6bb6504c1134",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scipy==1.6.1 (from -r /content/CoST/requirements.txt (line 1))\n",
            "  Downloading scipy-1.6.1.tar.gz (27.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.3/27.3 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: Ignored the following versions that require a different python version: 1.6.2 Requires-Python >=3.7,<3.10; 1.6.3 Requires-Python >=3.7,<3.10; 1.7.0 Requires-Python >=3.7,<3.10; 1.7.1 Requires-Python >=3.7,<3.10\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.9.0 (from versions: 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 2.0.0, 2.0.1)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.9.0\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !cd /content/CoST/datasets; python electricity.py"
      ],
      "metadata": {
        "id": "A_zO0vylX_fF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/CoST; sh scripts/ETT_CoST.sh"
      ],
      "metadata": {
        "id": "JDtXGWSiYvBz",
        "outputId": "6308df78-b62a-4569-fdb3-51c008a1a045",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: ETTm2\n",
            "Arguments: Namespace(dataset='ETTm2', run_name='forecast_univar', archive='forecast_csv_univar', gpu=0, batch_size=16, lr=0.001, repr_dims=320, max_train_length=201, iters=None, epochs=None, save_every=None, seed=1, max_threads=8, eval=True, kernels=[1, 2, 4, 8, 16, 32, 64, 128], alpha=0.0005)\n",
            "/content/CoST/datautils.py:30: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series. To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)\n",
            "  dt.weekofyear.to_numpy(),\n",
            "Epoch #0: loss=2.9102361124008893\n",
            "Epoch #1: loss=4.063485956192016\n",
            "Epoch #2: loss=3.8800783157348633\n",
            "Epoch #3: loss=3.674055504798889\n",
            "Epoch #4: loss=3.617241883277893\n",
            "Epoch #5: loss=3.3328460693359374\n",
            "Epoch #6: loss=3.253129315376282\n",
            "Epoch #7: loss=3.2059966802597044\n",
            "Epoch #8: loss=3.1808583974838256\n",
            "Epoch #9: loss=3.138414406776428\n",
            "Epoch #10: loss=3.057609295845032\n",
            "Epoch #11: loss=3.0182302474975584\n",
            "Epoch #12: loss=2.818213176727295\n",
            "Epoch #13: loss=2.8705147981643675\n",
            "Epoch #14: loss=2.7210950374603273\n",
            "Epoch #15: loss=2.6638561964035032\n",
            "Epoch #16: loss=2.554247283935547\n",
            "Epoch #17: loss=2.6481862306594848\n",
            "Epoch #18: loss=2.483468461036682\n",
            "Epoch #19: loss=2.476785659790039\n",
            "Epoch #20: loss=2.587933969497681\n",
            "Epoch #21: loss=2.540912914276123\n",
            "Epoch #22: loss=2.4829546451568603\n",
            "Epoch #23: loss=2.225401318073273\n",
            "Epoch #24: loss=2.266400229930878\n",
            "Epoch #25: loss=2.199349319934845\n",
            "Epoch #26: loss=2.0248846650123595\n",
            "Epoch #27: loss=2.213402032852173\n",
            "Epoch #28: loss=2.0297090649604796\n",
            "Epoch #29: loss=2.003298544883728\n",
            "Epoch #30: loss=2.1221770524978636\n",
            "Epoch #31: loss=1.9324001908302306\n",
            "Epoch #32: loss=1.9545912981033324\n",
            "Epoch #33: loss=1.8078390121459962\n",
            "Epoch #34: loss=2.0097808957099916\n",
            "Epoch #35: loss=2.053979587554932\n",
            "Epoch #36: loss=1.7608006358146668\n",
            "Epoch #37: loss=1.8630815148353577\n",
            "Epoch #38: loss=1.7657219171524048\n",
            "Epoch #39: loss=1.7232870936393738\n",
            "Epoch #40: loss=1.9202025890350343\n",
            "Epoch #41: loss=1.6324706077575684\n",
            "Epoch #42: loss=1.7102654814720153\n",
            "Epoch #43: loss=1.5797030627727509\n",
            "Epoch #44: loss=1.7577405452728272\n",
            "Epoch #45: loss=1.495105803012848\n",
            "Epoch #46: loss=1.5737191677093505\n",
            "Epoch #47: loss=1.594092059135437\n",
            "Epoch #48: loss=1.6187940955162048\n",
            "Epoch #49: loss=1.2935345470905304\n",
            "Epoch #50: loss=1.5142089188098908\n",
            "Epoch #51: loss=1.5497856855392456\n",
            "Epoch #52: loss=1.4789761662483216\n",
            "Epoch #53: loss=1.6169312119483947\n",
            "Epoch #54: loss=1.2657702147960663\n",
            "Epoch #55: loss=1.333910495042801\n",
            "Epoch #56: loss=1.3555180251598358\n",
            "Epoch #57: loss=1.2158105075359344\n",
            "Epoch #58: loss=1.312803363800049\n",
            "Epoch #59: loss=1.310907679796219\n",
            "\n",
            "Training time: 0:01:09.923503\n",
            "\n",
            "Evaluation result: {'ours': {24: {'norm': {'MSE': 0.024075529832911842, 'MAE': 0.10722940731648936}, 'raw': {'MSE': 3.2288578727283763, 'MAE': 1.2417965359264769}}, 48: {'norm': {'MSE': 0.04836649198939297, 'MAE': 0.1572784887850081}, 'raw': {'MSE': 6.486608172908123, 'MAE': 1.8214022375907082}}, 96: {'norm': {'MSE': 0.07613546424912511, 'MAE': 0.19860298762336415}, 'raw': {'MSE': 10.210807214659264, 'MAE': 2.2999707679831345}}, 288: {'norm': {'MSE': 0.1351703617845207, 'MAE': 0.273612060161202}, 'raw': {'MSE': 18.128194537324973, 'MAE': 3.1686317894330904}}, 672: {'norm': {'MSE': 0.16301724071838045, 'MAE': 0.3133098655658106}, 'raw': {'MSE': 21.86284191029359, 'MAE': 3.6283619911951415}}}, 'encoder_infer_time': 31.064072608947754, 'lr_train_time': {24: 2.6356618404388428, 48: 2.841533660888672, 96: 4.741184949874878, 288: 4.89881157875061, 672: 10.221952199935913}, 'lr_infer_time': {24: 0.01650834083557129, 48: 0.016716957092285156, 96: 0.02492499351501465, 288: 0.05728578567504883, 672: 0.11385607719421387}}\n",
            "Finished.\n",
            "Dataset: ETTm2\n",
            "Arguments: Namespace(dataset='ETTm2', run_name='forecast_univar', archive='forecast_csv_univar', gpu=0, batch_size=16, lr=0.001, repr_dims=320, max_train_length=201, iters=None, epochs=None, save_every=None, seed=2, max_threads=8, eval=True, kernels=[1, 2, 4, 8, 16, 32, 64, 128], alpha=0.0005)\n",
            "/content/CoST/datautils.py:30: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series. To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)\n",
            "  dt.weekofyear.to_numpy(),\n",
            "Epoch #0: loss=2.9639530457556247\n",
            "Epoch #1: loss=4.116832900047302\n",
            "Epoch #2: loss=4.113442420959473\n",
            "Epoch #3: loss=3.7438601732254027\n",
            "Epoch #4: loss=3.6585047006607057\n",
            "Epoch #5: loss=3.559669041633606\n",
            "Epoch #6: loss=3.3667325019836425\n",
            "Epoch #7: loss=3.1701318502426146\n",
            "Epoch #8: loss=3.3521780014038085\n",
            "Epoch #9: loss=3.250494432449341\n",
            "Epoch #10: loss=3.0902371644973754\n",
            "Epoch #11: loss=3.0445865392684937\n",
            "Epoch #12: loss=2.9885262727737425\n",
            "Epoch #13: loss=3.0903438568115233\n",
            "Epoch #14: loss=2.9349520444869994\n",
            "Epoch #15: loss=2.746946382522583\n",
            "Epoch #16: loss=2.686579203605652\n",
            "Epoch #17: loss=2.851506233215332\n",
            "Epoch #18: loss=2.7050024032592774\n",
            "Epoch #19: loss=2.623872470855713\n",
            "Epoch #20: loss=2.71161003112793\n",
            "Epoch #21: loss=2.583148646354675\n",
            "Epoch #22: loss=2.4284547328948975\n",
            "Epoch #23: loss=2.413419508934021\n",
            "Epoch #24: loss=2.415213942527771\n",
            "Epoch #25: loss=2.737904763221741\n",
            "Epoch #26: loss=2.2918611645698546\n",
            "Epoch #27: loss=2.2059920191764832\n",
            "Epoch #28: loss=2.1797574400901794\n",
            "Epoch #29: loss=2.161019134521484\n",
            "Epoch #30: loss=2.2388900399208067\n",
            "Epoch #31: loss=2.247953975200653\n",
            "Epoch #32: loss=1.9014496803283691\n",
            "Epoch #33: loss=2.127410387992859\n",
            "Epoch #34: loss=2.021738255023956\n",
            "Epoch #35: loss=2.0408560037612915\n",
            "Epoch #36: loss=1.7759090662002563\n",
            "Epoch #37: loss=1.8737609028816222\n",
            "Epoch #38: loss=1.8992688059806824\n",
            "Epoch #39: loss=1.9055426716804504\n",
            "Epoch #40: loss=1.7861783266067506\n",
            "Epoch #41: loss=1.8644887208938599\n",
            "Epoch #42: loss=1.729187512397766\n",
            "Epoch #43: loss=1.7134696125984192\n",
            "Epoch #44: loss=1.6127302050590515\n",
            "Epoch #45: loss=1.5875723838806153\n",
            "Epoch #46: loss=1.5982539653778076\n",
            "Epoch #47: loss=1.501452225446701\n",
            "Epoch #48: loss=1.5180928111076355\n",
            "Epoch #49: loss=1.7839191555976868\n",
            "Epoch #50: loss=1.4401614904403686\n",
            "Epoch #51: loss=1.4757687211036683\n",
            "Epoch #52: loss=1.5355674505233765\n",
            "Epoch #53: loss=1.4914286971092223\n",
            "Epoch #54: loss=1.6767624378204347\n",
            "Epoch #55: loss=1.340598714351654\n",
            "Epoch #56: loss=1.2872415959835053\n",
            "Epoch #57: loss=1.3838965892791748\n",
            "Epoch #58: loss=1.3840994775295257\n",
            "Epoch #59: loss=1.610065495967865\n",
            "\n",
            "Training time: 0:01:12.917871\n",
            "\n",
            "Evaluation result: {'ours': {24: {'norm': {'MSE': 0.024818729891753412, 'MAE': 0.1099123119544816}, 'raw': {'MSE': 3.3285311815250656, 'MAE': 1.2728665769232594}}, 48: {'norm': {'MSE': 0.048935630030718234, 'MAE': 0.1590223115595738}, 'raw': {'MSE': 6.562937384746503, 'MAE': 1.8415970105708}}, 96: {'norm': {'MSE': 0.06948582102136999, 'MAE': 0.1947259064089637}, 'raw': {'MSE': 9.318999108455357, 'MAE': 2.255071275743737}}, 288: {'norm': {'MSE': 0.12457962602918114, 'MAE': 0.26924756688315893}, 'raw': {'MSE': 16.70783201611579, 'MAE': 3.1180877013583412}}, 672: {'norm': {'MSE': 0.16069566638472987, 'MAE': 0.31380046976319426}, 'raw': {'MSE': 21.55148704877446, 'MAE': 3.6340435547873273}}}, 'encoder_infer_time': 33.1649854183197, 'lr_train_time': {24: 2.384448766708374, 48: 2.630645990371704, 96: 4.725295066833496, 288: 4.90911340713501, 672: 9.765694379806519}, 'lr_infer_time': {24: 0.012100696563720703, 48: 0.01361846923828125, 96: 0.02316117286682129, 288: 0.0725409984588623, 672: 0.12627315521240234}}\n",
            "Finished.\n",
            "Dataset: ETTm2\n",
            "Arguments: Namespace(dataset='ETTm2', run_name='forecast_univar', archive='forecast_csv_univar', gpu=0, batch_size=16, lr=0.001, repr_dims=320, max_train_length=201, iters=None, epochs=None, save_every=None, seed=3, max_threads=8, eval=True, kernels=[1, 2, 4, 8, 16, 32, 64, 128], alpha=0.0005)\n",
            "/content/CoST/datautils.py:30: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series. To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)\n",
            "  dt.weekofyear.to_numpy(),\n",
            "Epoch #0: loss=2.8901608519256117\n",
            "Epoch #1: loss=4.093533515930176\n",
            "Epoch #2: loss=3.9526406049728395\n",
            "Epoch #3: loss=3.7480780363082884\n",
            "Epoch #4: loss=3.621424126625061\n",
            "Epoch #5: loss=3.5468194484710693\n",
            "Epoch #6: loss=3.3592732429504393\n",
            "Epoch #7: loss=3.266695261001587\n",
            "Epoch #8: loss=3.2024461030960083\n",
            "Epoch #9: loss=3.099034571647644\n",
            "Epoch #10: loss=3.2109861612319945\n",
            "Epoch #11: loss=2.927768921852112\n",
            "Epoch #12: loss=3.0119545459747314\n",
            "Epoch #13: loss=2.7390517711639406\n",
            "Epoch #14: loss=2.8317046403884887\n",
            "Epoch #15: loss=2.76105420589447\n",
            "Epoch #16: loss=2.842312288284302\n",
            "Epoch #17: loss=2.615411329269409\n",
            "Epoch #18: loss=2.521128010749817\n",
            "Epoch #19: loss=2.6824902534484862\n",
            "Epoch #20: loss=2.4875797510147093\n",
            "Epoch #21: loss=2.6554604530334474\n",
            "Epoch #22: loss=2.4436015725135802\n",
            "Epoch #23: loss=2.322640299797058\n",
            "Epoch #24: loss=2.3334697484970093\n",
            "Epoch #25: loss=2.481075572967529\n",
            "Epoch #26: loss=2.2815824270248415\n",
            "Epoch #27: loss=2.1128825545310974\n",
            "Epoch #28: loss=2.0002431154251097\n",
            "Epoch #29: loss=2.1015318632125854\n",
            "Epoch #30: loss=2.099328100681305\n",
            "Epoch #31: loss=2.0470573544502257\n",
            "Epoch #32: loss=2.109517455101013\n",
            "Epoch #33: loss=2.020608425140381\n",
            "Epoch #34: loss=1.8557398796081543\n",
            "Epoch #35: loss=1.8660183191299438\n",
            "Epoch #36: loss=1.7147109389305115\n",
            "Epoch #37: loss=1.813414466381073\n",
            "Epoch #38: loss=1.7646415829658508\n",
            "Epoch #39: loss=1.6247238159179687\n",
            "Epoch #40: loss=1.964290404319763\n",
            "Epoch #41: loss=1.6568044185638429\n",
            "Epoch #42: loss=1.6317258059978486\n",
            "Epoch #43: loss=1.6883037447929383\n",
            "Epoch #44: loss=1.4679208993911743\n",
            "Epoch #45: loss=1.6682429432868957\n",
            "Epoch #46: loss=1.6299810767173768\n",
            "Epoch #47: loss=1.5398693799972534\n",
            "Epoch #48: loss=1.4790233612060546\n",
            "Epoch #49: loss=1.6148893356323242\n",
            "Epoch #50: loss=1.4599203586578369\n",
            "Epoch #51: loss=1.2757254898548127\n",
            "Epoch #52: loss=1.5100595831871033\n",
            "Epoch #53: loss=1.2367000460624695\n",
            "Epoch #54: loss=1.3818267405033111\n",
            "Epoch #55: loss=1.3399620234966279\n",
            "Epoch #56: loss=1.3003956854343415\n",
            "Epoch #57: loss=1.1986802041530609\n",
            "Epoch #58: loss=1.4399232745170594\n",
            "Epoch #59: loss=1.3576110422611236\n",
            "\n",
            "Training time: 0:01:14.836325\n",
            "\n",
            "Evaluation result: {'ours': {24: {'norm': {'MSE': 0.02675983269707379, 'MAE': 0.11182467209154352}, 'raw': {'MSE': 3.588859613628495, 'MAE': 1.2950131349413099}}, 48: {'norm': {'MSE': 0.04988402267536598, 'MAE': 0.16056070472854564}, 'raw': {'MSE': 6.690129819098502, 'MAE': 1.8594127523399495}}, 96: {'norm': {'MSE': 0.07241033522630817, 'MAE': 0.19800520288518556}, 'raw': {'MSE': 9.71121648358084, 'MAE': 2.2930479779208928}}, 288: {'norm': {'MSE': 0.1504949039064089, 'MAE': 0.2835114818912086}, 'raw': {'MSE': 20.183425267468643, 'MAE': 3.283274480575941}}, 672: {'norm': {'MSE': 0.23813992628119984, 'MAE': 0.3456420208274246}, 'raw': {'MSE': 31.93782142212408, 'MAE': 4.002792468075257}}}, 'encoder_infer_time': 32.79826045036316, 'lr_train_time': {24: 2.427600622177124, 48: 2.5877013206481934, 96: 4.785088777542114, 288: 4.693118572235107, 672: 9.65637493133545}, 'lr_infer_time': {24: 0.01201176643371582, 48: 0.018207550048828125, 96: 0.025863170623779297, 288: 0.08069181442260742, 672: 0.1236259937286377}}\n",
            "Finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !cd /content/CoST; python -u train.py ETTh1 forecast_univar --alpha 0.0005 --kernels 1 2 4 8 16 32 64 128 --max-train-length 201 --batch-size 16 --archive forecast_csv_univar --repr-dims 320 --max-threads 8 --seed 1 --eval"
      ],
      "metadata": {
        "id": "cHGjomTozYI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/CoST/result.txt')"
      ],
      "metadata": {
        "id": "UjSz8onKYY9D",
        "outputId": "a5c25556-f3c7-4c04-e231-2f997ddfdc65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_eec44ccc-7178-4428-bc50-660b98dc7214\", \"result.txt\", 733)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}