{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/whoami-Lory271/thesis-project/blob/main/CoST_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training CoST model"
      ],
      "metadata": {
        "id": "Gh2L5z66W2X6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1LWptcrfWeZG",
        "outputId": "d83b5695-b1e6-4593-8cac-0818a5747574",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CoST'...\n",
            "remote: Enumerating objects: 74, done.\u001b[K\n",
            "remote: Counting objects: 100% (74/74), done.\u001b[K\n",
            "remote: Compressing objects: 100% (41/41), done.\u001b[K\n",
            "remote: Total 74 (delta 30), reused 74 (delta 30), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (74/74), 368.15 KiB | 4.97 MiB/s, done.\n",
            "Resolving deltas: 100% (30/30), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/whoami-Lory271/CoST"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "5EIFz57FW_Wh",
        "outputId": "4111d82a-b92e-465a-b75f-66c6f691ae02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp \"/content/drive/MyDrive/Tesi/code/datasets/electricity/LD2011_2014.txt\" \"/content/CoST/datasets/\""
      ],
      "metadata": {
        "id": "0hTJksSZXPcX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/CoST/requirements.txt\n",
        "!pip install einops==0.6.1 --quiet"
      ],
      "metadata": {
        "id": "x5qeqzGyXkx9",
        "outputId": "eb3d759a-afe1-4c6f-84fa-ec235844309e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scipy==1.6.1 (from -r /content/CoST/requirements.txt (line 1))\n",
            "  Downloading scipy-1.6.1.tar.gz (27.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.3/27.3 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: Ignored the following versions that require a different python version: 1.6.2 Requires-Python >=3.7,<3.10; 1.6.3 Requires-Python >=3.7,<3.10; 1.7.0 Requires-Python >=3.7,<3.10; 1.7.1 Requires-Python >=3.7,<3.10\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.9.0 (from versions: 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 2.0.0, 2.0.1)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.9.0\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/CoST/datasets; python electricity.py"
      ],
      "metadata": {
        "id": "A_zO0vylX_fF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !cd /content/CoST; sh scripts/Electricity_CoST.sh"
      ],
      "metadata": {
        "id": "JDtXGWSiYvBz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/CoST; python -u train.py electricity forecast_univar --alpha 0.0005 --kernels 1 2 4 8 16 32 64 128 --max-train-length 201 --batch-size 16 --iters 1800 --archive forecast_csv_univar --repr-dims 320 --max-threads 8 --seed 1 --eval"
      ],
      "metadata": {
        "id": "cHGjomTozYI6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fddda2c2-d795-46af-a888-a7696fefdbf9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: electricity\n",
            "Arguments: Namespace(dataset='electricity', run_name='forecast_univar', archive='forecast_csv_univar', gpu=0, batch_size=16, lr=0.001, repr_dims=320, max_train_length=201, iters=1800, epochs=None, save_every=None, seed=1, max_threads=8, eval=True, kernels=[1, 2, 4, 8, 16, 32, 64, 128], alpha=0.0005)\n",
            "/content/CoST/datautils.py:30: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series. To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)\n",
            "  dt.weekofyear.to_numpy(),\n",
            "Epoch #0: loss=2.006839037872851\n",
            "Epoch #1: loss=3.4614131450653076\n",
            "Epoch #2: loss=3.961059868335724\n",
            "Epoch #3: loss=4.095044732093811\n",
            "Epoch #4: loss=4.236361265182495\n",
            "Epoch #5: loss=4.284492373466492\n",
            "Epoch #6: loss=3.9876115322113037\n",
            "Epoch #7: loss=3.797514319419861\n",
            "Epoch #8: loss=3.6704928278923035\n",
            "Epoch #9: loss=3.847055196762085\n",
            "Epoch #10: loss=3.5264405012130737\n",
            "Epoch #11: loss=3.824763596057892\n",
            "Epoch #12: loss=3.408639132976532\n",
            "Epoch #13: loss=3.7135691046714783\n",
            "Epoch #14: loss=3.1873992681503296\n",
            "Epoch #15: loss=3.319207727909088\n",
            "Epoch #16: loss=3.257967710494995\n",
            "Epoch #17: loss=3.455393671989441\n",
            "Epoch #18: loss=3.4836224913597107\n",
            "Epoch #19: loss=3.218996822834015\n",
            "Epoch #20: loss=3.3102442622184753\n",
            "Epoch #21: loss=3.275110602378845\n",
            "Epoch #22: loss=3.2029294967651367\n",
            "Epoch #23: loss=3.3095678091049194\n",
            "Epoch #24: loss=3.135226309299469\n",
            "Epoch #25: loss=3.3568713068962097\n",
            "Epoch #26: loss=3.1099077463150024\n",
            "Epoch #27: loss=3.0333704948425293\n",
            "Epoch #28: loss=3.167929768562317\n",
            "Epoch #29: loss=3.1089696884155273\n",
            "Epoch #30: loss=3.228917956352234\n",
            "Epoch #31: loss=2.996453583240509\n",
            "Epoch #32: loss=3.1492190957069397\n",
            "Epoch #33: loss=2.9711097478866577\n",
            "Epoch #34: loss=3.11819851398468\n",
            "Epoch #35: loss=3.057362139225006\n",
            "Epoch #36: loss=2.8976298570632935\n",
            "Epoch #37: loss=2.9693397879600525\n",
            "Epoch #38: loss=2.866939067840576\n",
            "Epoch #39: loss=2.894542872905731\n",
            "Epoch #40: loss=2.58827668428421\n",
            "Epoch #41: loss=2.7962766885757446\n",
            "Epoch #42: loss=3.08659166097641\n",
            "Epoch #43: loss=2.981238007545471\n",
            "Epoch #44: loss=2.83911794424057\n",
            "Epoch #45: loss=2.894763171672821\n",
            "Epoch #46: loss=2.532478094100952\n",
            "Epoch #47: loss=2.661405920982361\n",
            "Epoch #48: loss=2.5397791266441345\n",
            "Epoch #49: loss=2.5351205468177795\n",
            "Epoch #50: loss=2.409230649471283\n",
            "Epoch #51: loss=2.5792373418807983\n",
            "Epoch #52: loss=2.8236252069473267\n",
            "Epoch #53: loss=2.378818690776825\n",
            "Epoch #54: loss=2.472578704357147\n",
            "Epoch #55: loss=2.944506049156189\n",
            "Epoch #56: loss=2.3728946447372437\n",
            "Epoch #57: loss=2.678763508796692\n",
            "Epoch #58: loss=2.369235396385193\n",
            "Epoch #59: loss=2.375994622707367\n",
            "Epoch #60: loss=2.5492918491363525\n",
            "Epoch #61: loss=2.3943883776664734\n",
            "Epoch #62: loss=2.4044060707092285\n",
            "Epoch #63: loss=2.5395495891571045\n",
            "Epoch #64: loss=2.2638409435749054\n",
            "Epoch #65: loss=2.4489715099334717\n",
            "Epoch #66: loss=2.3130030035972595\n",
            "Epoch #67: loss=2.3394015729427338\n",
            "Epoch #68: loss=2.1820636987686157\n",
            "Epoch #69: loss=2.230942666530609\n",
            "Epoch #70: loss=2.208815097808838\n",
            "Epoch #71: loss=2.235286593437195\n",
            "Epoch #72: loss=2.417755126953125\n",
            "Epoch #73: loss=2.052263081073761\n",
            "Epoch #74: loss=2.3038344383239746\n",
            "Epoch #75: loss=2.288465917110443\n",
            "Epoch #76: loss=2.2784807682037354\n",
            "Epoch #77: loss=1.944078803062439\n",
            "Epoch #78: loss=2.2326422035694122\n",
            "Epoch #79: loss=1.9555007219314575\n",
            "Epoch #80: loss=2.4218814969062805\n",
            "Epoch #81: loss=1.9806348085403442\n",
            "Epoch #82: loss=2.1086213886737823\n",
            "Epoch #83: loss=1.7156940400600433\n",
            "Epoch #84: loss=2.0044340193271637\n",
            "Epoch #85: loss=2.0080970525741577\n",
            "Epoch #86: loss=2.3230444192886353\n",
            "Epoch #87: loss=1.9683209359645844\n",
            "Epoch #88: loss=1.9357424676418304\n",
            "Epoch #89: loss=2.2384234368801117\n",
            "Epoch #90: loss=1.7695533633232117\n",
            "Epoch #91: loss=1.8651978969573975\n",
            "Epoch #92: loss=2.1496622562408447\n",
            "Epoch #93: loss=2.0274373590946198\n",
            "Epoch #94: loss=1.977207213640213\n",
            "Epoch #95: loss=1.85756516456604\n",
            "Epoch #96: loss=1.5621845126152039\n",
            "Epoch #97: loss=1.8520298302173615\n",
            "Epoch #98: loss=1.9518375098705292\n",
            "Epoch #99: loss=2.1168925762176514\n",
            "Epoch #100: loss=2.0466378927230835\n",
            "Epoch #101: loss=2.0051311552524567\n",
            "Epoch #102: loss=1.8158713579177856\n",
            "Epoch #103: loss=1.7162895798683167\n",
            "Epoch #104: loss=1.62071293592453\n",
            "Epoch #105: loss=1.6435080170631409\n",
            "Epoch #106: loss=1.7406167685985565\n",
            "Epoch #107: loss=1.7415221631526947\n",
            "Epoch #108: loss=1.835714429616928\n",
            "Epoch #109: loss=1.8827430605888367\n",
            "Epoch #110: loss=1.638942927122116\n",
            "Epoch #111: loss=1.8971678912639618\n",
            "Epoch #112: loss=1.8202311396598816\n",
            "Epoch #113: loss=1.4937915354967117\n",
            "Epoch #114: loss=1.4922015070915222\n",
            "Epoch #115: loss=1.7221563756465912\n",
            "Epoch #116: loss=1.7925346791744232\n",
            "Epoch #117: loss=1.6515690088272095\n",
            "Epoch #118: loss=1.6766452491283417\n",
            "Epoch #119: loss=1.5671364963054657\n",
            "Epoch #120: loss=1.4955928325653076\n",
            "Epoch #121: loss=1.6669611036777496\n",
            "Epoch #122: loss=1.648602932691574\n",
            "Epoch #123: loss=1.3169849216938019\n",
            "Epoch #124: loss=1.6176629066467285\n",
            "Epoch #125: loss=1.520932137966156\n",
            "Epoch #126: loss=1.6177943050861359\n",
            "Epoch #127: loss=1.4881731867790222\n",
            "Epoch #128: loss=1.638430893421173\n",
            "Epoch #129: loss=1.5982431471347809\n",
            "Epoch #130: loss=1.3749159276485443\n",
            "Epoch #131: loss=1.6333844065666199\n",
            "Epoch #132: loss=1.6022356450557709\n",
            "Epoch #133: loss=1.6277830600738525\n",
            "Epoch #134: loss=1.6999294459819794\n",
            "Epoch #135: loss=1.0415105521678925\n",
            "Epoch #136: loss=1.7199808657169342\n",
            "Epoch #137: loss=1.16320239007473\n",
            "Epoch #138: loss=1.4238701462745667\n",
            "Epoch #139: loss=1.4455462396144867\n",
            "Epoch #140: loss=1.3708468973636627\n",
            "Epoch #141: loss=1.4910732507705688\n",
            "Epoch #142: loss=1.3016858994960785\n",
            "Epoch #143: loss=1.4223488867282867\n",
            "Epoch #144: loss=1.2775141298770905\n",
            "Epoch #145: loss=1.2132505029439926\n",
            "Epoch #146: loss=1.3997913897037506\n",
            "Epoch #147: loss=1.2143552005290985\n",
            "Epoch #148: loss=1.2518369555473328\n",
            "Epoch #149: loss=1.3103022873401642\n",
            "Epoch #150: loss=1.3583519458770752\n",
            "Epoch #151: loss=1.2741521298885345\n",
            "Epoch #152: loss=1.2605337500572205\n",
            "Epoch #153: loss=1.3648313581943512\n",
            "Epoch #154: loss=1.4021280854940414\n",
            "Epoch #155: loss=1.0318579524755478\n",
            "Epoch #156: loss=1.245402842760086\n",
            "Epoch #157: loss=1.2830548286437988\n",
            "Epoch #158: loss=1.3812930285930634\n",
            "Epoch #159: loss=1.0049301385879517\n",
            "Epoch #160: loss=1.4212015867233276\n",
            "Epoch #161: loss=1.0761371105909348\n",
            "Epoch #162: loss=1.2961082756519318\n",
            "Epoch #163: loss=1.0623819828033447\n",
            "Epoch #164: loss=1.3162447065114975\n",
            "Epoch #165: loss=1.311134159564972\n",
            "Epoch #166: loss=1.2605186998844147\n",
            "Epoch #167: loss=1.1845595240592957\n",
            "Epoch #168: loss=1.1158549189567566\n",
            "Epoch #169: loss=1.4607419073581696\n",
            "Epoch #170: loss=1.5047197043895721\n",
            "Epoch #171: loss=1.0886665135622025\n",
            "Epoch #172: loss=1.5117415189743042\n",
            "Epoch #173: loss=0.9255877733230591\n",
            "Epoch #174: loss=1.1923762112855911\n",
            "Epoch #175: loss=1.054494097828865\n",
            "Epoch #176: loss=1.2486907094717026\n",
            "Epoch #177: loss=1.1745470762252808\n",
            "Epoch #178: loss=0.9992992132902145\n",
            "Epoch #179: loss=1.2291176468133926\n",
            "Epoch #180: loss=0.8034795075654984\n",
            "Epoch #181: loss=1.1774813532829285\n",
            "Epoch #182: loss=1.0721963346004486\n",
            "Epoch #183: loss=1.002673402428627\n",
            "Epoch #184: loss=1.0574027448892593\n",
            "Epoch #185: loss=1.0642361342906952\n",
            "Epoch #186: loss=0.9145318120718002\n",
            "Epoch #187: loss=1.0335000455379486\n",
            "Epoch #188: loss=1.1518851220607758\n",
            "Epoch #189: loss=0.861868754029274\n",
            "Epoch #190: loss=1.1884692758321762\n",
            "Epoch #191: loss=1.030331015586853\n",
            "Epoch #192: loss=1.0610820651054382\n",
            "Epoch #193: loss=0.9789975136518478\n",
            "Epoch #194: loss=1.0348247289657593\n",
            "Epoch #195: loss=0.9764692932367325\n",
            "Epoch #196: loss=1.1370901316404343\n",
            "Epoch #197: loss=0.9915188252925873\n",
            "Epoch #198: loss=0.9640871733427048\n",
            "Epoch #199: loss=1.0124561339616776\n",
            "Epoch #200: loss=1.1041197702288628\n",
            "Epoch #201: loss=1.0620157718658447\n",
            "Epoch #202: loss=0.9613841325044632\n",
            "Epoch #203: loss=1.0868576765060425\n",
            "Epoch #204: loss=0.9007234126329422\n",
            "Epoch #205: loss=1.3363091945648193\n",
            "Epoch #206: loss=1.0016616135835648\n",
            "Epoch #207: loss=1.040257915854454\n",
            "Epoch #208: loss=0.7254406213760376\n",
            "Epoch #209: loss=0.836182564496994\n",
            "Epoch #210: loss=0.7855479195713997\n",
            "Epoch #211: loss=1.0907495319843292\n",
            "Epoch #212: loss=0.8409635424613953\n",
            "Epoch #213: loss=0.8925792276859283\n",
            "Epoch #214: loss=0.792338028550148\n",
            "Epoch #215: loss=0.9408809840679169\n",
            "Epoch #216: loss=1.1581347286701202\n",
            "Epoch #217: loss=0.9659565985202789\n",
            "Epoch #218: loss=0.9692357033491135\n",
            "Epoch #219: loss=0.8651832342147827\n",
            "Epoch #220: loss=0.9697416871786118\n",
            "Epoch #221: loss=0.8695073425769806\n",
            "Epoch #222: loss=1.323758453130722\n",
            "Epoch #223: loss=0.9484671056270599\n",
            "Epoch #224: loss=0.6240261793136597\n",
            "Epoch #225: loss=1.0958140194416046\n",
            "Epoch #226: loss=0.8062844276428223\n",
            "Epoch #227: loss=1.0286781787872314\n",
            "Epoch #228: loss=0.7503639757633209\n",
            "Epoch #229: loss=0.8079261779785156\n",
            "Epoch #230: loss=1.300459772348404\n",
            "Epoch #231: loss=1.1279569119215012\n",
            "Epoch #232: loss=0.5349542200565338\n",
            "Epoch #233: loss=0.9684330821037292\n",
            "Epoch #234: loss=0.8695669174194336\n",
            "Epoch #235: loss=0.9557178020477295\n",
            "Epoch #236: loss=1.054728128015995\n",
            "Epoch #237: loss=1.1381670534610748\n",
            "Epoch #238: loss=1.038236677646637\n",
            "Epoch #239: loss=0.8218826055526733\n",
            "Epoch #240: loss=0.9559267312288284\n",
            "Epoch #241: loss=0.6584378480911255\n",
            "Epoch #242: loss=0.6396147459745407\n",
            "Epoch #243: loss=0.70545893907547\n",
            "Epoch #244: loss=0.8933955729007721\n",
            "Epoch #245: loss=1.125276729464531\n",
            "Epoch #246: loss=0.8800203651189804\n",
            "Epoch #247: loss=0.7876964658498764\n",
            "Epoch #248: loss=0.7492918968200684\n",
            "Epoch #249: loss=0.9246007949113846\n",
            "Epoch #250: loss=0.8640652671456337\n",
            "Epoch #251: loss=0.8465541154146194\n",
            "Epoch #252: loss=1.1496601551771164\n",
            "Epoch #253: loss=0.7611022442579269\n",
            "Epoch #254: loss=0.9592965841293335\n",
            "Epoch #255: loss=0.9637089222669601\n",
            "Epoch #256: loss=0.947872519493103\n",
            "Epoch #257: loss=0.6526200175285339\n",
            "Epoch #258: loss=0.7080151364207268\n",
            "Epoch #259: loss=0.8380126059055328\n",
            "Epoch #260: loss=0.9023453593254089\n",
            "Epoch #261: loss=0.701688751578331\n",
            "Epoch #262: loss=0.6902355924248695\n",
            "Epoch #263: loss=1.018676295876503\n",
            "Epoch #264: loss=0.7186322957277298\n",
            "Epoch #265: loss=0.8820765465497971\n",
            "Epoch #266: loss=1.0555630177259445\n",
            "Epoch #267: loss=0.6838452368974686\n",
            "Epoch #268: loss=0.6014894396066666\n",
            "Epoch #269: loss=1.0353967100381851\n",
            "Epoch #270: loss=0.8682364970445633\n",
            "Epoch #271: loss=0.745138555765152\n",
            "Epoch #272: loss=0.9539504647254944\n",
            "Epoch #273: loss=0.6269312500953674\n",
            "Epoch #274: loss=0.850230947136879\n",
            "Epoch #275: loss=0.74555354565382\n",
            "Epoch #276: loss=0.4660596176981926\n",
            "Epoch #277: loss=0.70402592420578\n",
            "Epoch #278: loss=0.7191861644387245\n",
            "Epoch #279: loss=0.6772842854261398\n",
            "Epoch #280: loss=0.9396797567605972\n",
            "Epoch #281: loss=0.6924506723880768\n",
            "Epoch #282: loss=0.6223688870668411\n",
            "Epoch #283: loss=0.6985576674342155\n",
            "Epoch #284: loss=0.6688302904367447\n",
            "Epoch #285: loss=0.7859370186924934\n",
            "Epoch #286: loss=0.6324080526828766\n",
            "Epoch #287: loss=0.5741366147994995\n",
            "Epoch #288: loss=0.7982024997472763\n",
            "Epoch #289: loss=0.7994926199316978\n",
            "Epoch #290: loss=0.6186079680919647\n",
            "Epoch #291: loss=0.7705612331628799\n",
            "Epoch #292: loss=0.8070279061794281\n",
            "Epoch #293: loss=0.6741963922977448\n",
            "Epoch #294: loss=0.7028651237487793\n",
            "Epoch #295: loss=0.6047516018152237\n",
            "Epoch #296: loss=0.7787826806306839\n",
            "Epoch #297: loss=1.1116887480020523\n",
            "Epoch #298: loss=0.8187948763370514\n",
            "Epoch #299: loss=0.4937725104391575\n",
            "Epoch #300: loss=0.6536763086915016\n",
            "Epoch #301: loss=0.6912788897752762\n",
            "Epoch #302: loss=0.858527660369873\n",
            "Epoch #303: loss=0.5734478309750557\n",
            "Epoch #304: loss=0.9503039568662643\n",
            "Epoch #305: loss=0.7712540030479431\n",
            "Epoch #306: loss=0.7740030586719513\n",
            "Epoch #307: loss=0.8016606867313385\n",
            "Epoch #308: loss=0.8693011999130249\n",
            "Epoch #309: loss=0.9362980276346207\n",
            "Epoch #310: loss=0.5936995595693588\n",
            "Epoch #311: loss=0.6219579875469208\n",
            "Epoch #312: loss=0.6591794267296791\n",
            "Epoch #313: loss=0.5787736102938652\n",
            "Epoch #314: loss=0.5782975256443024\n",
            "Epoch #315: loss=0.6788584217429161\n",
            "Epoch #316: loss=0.6622671261429787\n",
            "Epoch #317: loss=0.5921951681375504\n",
            "Epoch #318: loss=0.7916715517640114\n",
            "Epoch #319: loss=0.6174298524856567\n",
            "Epoch #320: loss=0.9632489085197449\n",
            "Epoch #321: loss=0.7037236765027046\n",
            "Epoch #322: loss=0.5736501216888428\n",
            "Epoch #323: loss=0.7059985585510731\n",
            "Epoch #324: loss=0.6045278981328011\n",
            "Epoch #325: loss=0.48580628633499146\n",
            "Epoch #326: loss=0.754031628370285\n",
            "Epoch #327: loss=0.5206250548362732\n",
            "Epoch #328: loss=0.6989100873470306\n",
            "Epoch #329: loss=0.4765295311808586\n",
            "Epoch #330: loss=0.766331672668457\n",
            "Epoch #331: loss=0.6052225455641747\n",
            "Epoch #332: loss=0.8564611077308655\n",
            "Epoch #333: loss=0.4986101910471916\n",
            "Epoch #334: loss=0.5559818893671036\n",
            "Epoch #335: loss=0.45328719168901443\n",
            "Epoch #336: loss=0.7609977573156357\n",
            "Epoch #337: loss=0.49396921694278717\n",
            "Epoch #338: loss=0.6671625524759293\n",
            "Epoch #339: loss=0.6116553694009781\n",
            "Epoch #340: loss=0.6526167616248131\n",
            "Epoch #341: loss=0.6457967907190323\n",
            "Epoch #342: loss=0.40503131598234177\n",
            "Epoch #343: loss=0.5838841125369072\n",
            "Epoch #344: loss=0.6223350763320923\n",
            "Epoch #345: loss=0.5220455005764961\n",
            "Epoch #346: loss=0.5158206969499588\n",
            "Epoch #347: loss=0.6963577046990395\n",
            "Epoch #348: loss=0.4765297621488571\n",
            "Epoch #349: loss=0.5060207694768906\n",
            "Epoch #350: loss=0.9151352941989899\n",
            "Epoch #351: loss=0.658705722540617\n",
            "Epoch #352: loss=0.6979959383606911\n",
            "Epoch #353: loss=0.593771580606699\n",
            "Epoch #354: loss=0.5618414729833603\n",
            "Epoch #355: loss=0.6839058697223663\n",
            "Epoch #356: loss=0.4114852771162987\n",
            "Epoch #357: loss=0.6260804608464241\n",
            "Epoch #358: loss=0.5171894580125809\n",
            "Epoch #359: loss=0.7529858201742172\n",
            "Epoch #360: loss=0.6234642639756203\n",
            "Epoch #361: loss=0.5785796716809273\n",
            "Epoch #362: loss=0.6140006557106972\n",
            "Epoch #363: loss=0.6254490315914154\n",
            "Epoch #364: loss=0.6093469336628914\n",
            "Epoch #365: loss=0.6545378565788269\n",
            "Epoch #366: loss=0.632802203297615\n",
            "Epoch #367: loss=0.6328683197498322\n",
            "Epoch #368: loss=0.8247810453176498\n",
            "Epoch #369: loss=0.7708588615059853\n",
            "Epoch #370: loss=0.7741004973649979\n",
            "Epoch #371: loss=0.5053938180208206\n",
            "Epoch #372: loss=0.4864778369665146\n",
            "Epoch #373: loss=0.4769347906112671\n",
            "Epoch #374: loss=0.7660103291273117\n",
            "Epoch #375: loss=0.9587554633617401\n",
            "Epoch #376: loss=0.7368835061788559\n",
            "Epoch #377: loss=0.5690882503986359\n",
            "Epoch #378: loss=0.7145222574472427\n",
            "Epoch #379: loss=0.34634869918227196\n",
            "Epoch #380: loss=0.5590553358197212\n",
            "Epoch #381: loss=0.6216514781117439\n",
            "Epoch #382: loss=0.6075607538223267\n",
            "Epoch #383: loss=0.6414252892136574\n",
            "Epoch #384: loss=0.559947345405817\n",
            "Epoch #385: loss=0.7894751727581024\n",
            "Epoch #386: loss=0.7697593718767166\n",
            "Epoch #387: loss=0.5571939796209335\n",
            "Epoch #388: loss=0.6873789131641388\n",
            "Epoch #389: loss=0.688379630446434\n",
            "Epoch #390: loss=0.3516288511455059\n",
            "Epoch #391: loss=0.8077990859746933\n",
            "Epoch #392: loss=0.5991950184106827\n",
            "Epoch #393: loss=0.410365529358387\n",
            "Epoch #394: loss=0.3902776688337326\n",
            "Epoch #395: loss=0.6539181023836136\n",
            "Epoch #396: loss=0.595929067581892\n",
            "Epoch #397: loss=0.49003590643405914\n",
            "Epoch #398: loss=0.6309395655989647\n",
            "Epoch #399: loss=0.4869874492287636\n",
            "Epoch #400: loss=0.48531368374824524\n",
            "Epoch #401: loss=0.5985518023371696\n",
            "Epoch #402: loss=0.6095182448625565\n",
            "Epoch #403: loss=0.6322233825922012\n",
            "Epoch #404: loss=1.036802925169468\n",
            "Epoch #405: loss=0.7053213939070702\n",
            "Epoch #406: loss=0.3997780941426754\n",
            "Epoch #407: loss=0.5788861885666847\n",
            "Epoch #408: loss=0.9869028478860855\n",
            "Epoch #409: loss=0.4989824406802654\n",
            "Epoch #410: loss=0.5254727527499199\n",
            "Epoch #411: loss=0.9906285405158997\n",
            "Epoch #412: loss=0.5763927921652794\n",
            "Epoch #413: loss=0.5307209342718124\n",
            "Epoch #414: loss=0.5492968149483204\n",
            "Epoch #415: loss=0.5008196383714676\n",
            "Epoch #416: loss=0.471352219581604\n",
            "Epoch #417: loss=0.49987006932497025\n",
            "Epoch #418: loss=0.461547888815403\n",
            "Epoch #419: loss=0.35369422286748886\n",
            "Epoch #420: loss=0.5438884273171425\n",
            "Epoch #421: loss=0.5450945682823658\n",
            "Epoch #422: loss=0.6406026482582092\n",
            "Epoch #423: loss=0.35992417484521866\n",
            "Epoch #424: loss=0.5096823051571846\n",
            "Epoch #425: loss=0.5302877463400364\n",
            "Epoch #426: loss=0.3980378061532974\n",
            "Epoch #427: loss=0.746063843369484\n",
            "Epoch #428: loss=0.6053728312253952\n",
            "Epoch #429: loss=0.610352136194706\n",
            "Epoch #430: loss=0.6467105932533741\n",
            "Epoch #431: loss=0.5969135835766792\n",
            "Epoch #432: loss=0.8163820058107376\n",
            "Epoch #433: loss=0.9321863651275635\n",
            "Epoch #434: loss=0.6266675293445587\n",
            "Epoch #435: loss=0.2928646579384804\n",
            "Epoch #436: loss=0.8088798075914383\n",
            "Epoch #437: loss=0.6182084232568741\n",
            "Epoch #438: loss=0.29620281606912613\n",
            "Epoch #439: loss=0.5680360719561577\n",
            "Epoch #440: loss=0.507138766348362\n",
            "Epoch #441: loss=0.6342252120375633\n",
            "Epoch #442: loss=0.4299807772040367\n",
            "Epoch #443: loss=0.6196112483739853\n",
            "Epoch #444: loss=0.4282172843813896\n",
            "Epoch #445: loss=0.4550603851675987\n",
            "Epoch #446: loss=0.5876971557736397\n",
            "Epoch #447: loss=0.7095058262348175\n",
            "Epoch #448: loss=0.5858416333794594\n",
            "Epoch #449: loss=0.5779576115310192\n",
            "\n",
            "Training time: 0:03:39.512061\n",
            "\n",
            "Evaluation result: {'ours': {24: {'norm': {'MSE': 0.24331453852596752, 'MAE': 0.26141338662594343}, 'raw': {'MSE': 142.67081763509069, 'MAE': 6.330111666183869}}, 48: {'norm': {'MSE': 0.29187958327821223, 'MAE': 0.2957730513314714}, 'raw': {'MSE': 171.14759805319528, 'MAE': 7.162129184596468}}, 168: {'norm': {'MSE': 0.4027810207405968, 'MAE': 0.37348209163316565}, 'raw': {'MSE': 236.17617835513406, 'MAE': 9.043849641864812}}, 336: {'norm': {'MSE': 0.5585014416058054, 'MAE': 0.4645183451242635}, 'raw': {'MSE': 327.484983302307, 'MAE': 11.248287836741543}}, 720: {'norm': {'MSE': 0.8975191666832346, 'MAE': 0.6410446660528124}, 'raw': {'MSE': 526.2726745597535, 'MAE': 15.52286354153137}}}, 'encoder_infer_time': 12.319658279418945, 'lr_train_time': {24: 1.8574070930480957, 48: 1.6480457782745361, 168: 1.9300110340118408, 336: 2.882091760635376, 720: 5.265837669372559}, 'lr_infer_time': {24: 0.006064653396606445, 48: 0.007431983947753906, 168: 0.019249916076660156, 336: 0.028790950775146484, 720: 0.08361172676086426}}\n",
            "Finished.\n"
          ]
        }
      ]
    }
  ]
}